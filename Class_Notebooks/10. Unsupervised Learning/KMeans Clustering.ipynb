{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"2yzyNpRmepb_"},"source":["\n","\n","#K-means (Lloyd's algorithm)\n","K-Means is one the most popular cluster techniques applied in Data Mining. It attempts to divide the $n$ rows of data into $k$ clusters, and is implemented in four steps:\n","\n",">* Partition objects into k nonempty subsets\n",">* Compute seed points as the centroids of the clusters of the current partition (the centroid is the center, i.e., mean point, of the cluster)\n",">* Assign each object to the cluster with the nearest seed point  \n",">* Go back to Step 2, stop when no more new assignment\n","\n","Kmeans can be relaively efficent (O(tkn), where t is the number of iterations and k, t << n), but is suspectible to outliers or noisy data, struggles with local optimums when you have large datasets and is only applicable to continous data. The global optimum may be found using techniques such as deterministic annealing and genetic algorithms.\n","\n","It also requires us to specify $k$ which is not always possible. Finally, the shape of the clusters is very important for kmeans. In reallity it will not do a good job finding clusters of non-convex shapes, and generally it requires that the clusters be of similar size and density.\n","\n","\n","The following code snipet is modelled on an example from [Towardsdatascience](https://towardsdatascience.com/machine-learning-algorithms-part-9-k-means-example-in-python-f2ad05ed5203). However, I have changed the dataset to the scikit learns Iris dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"QKnxniOzKvm0"},"source":["As usual we import the relevant libraries."]},{"cell_type":"code","metadata":{"id":"NoXwk_Xbejfp","executionInfo":{"status":"ok","timestamp":1763280232924,"user_tz":0,"elapsed":16257,"user":{"displayName":"Maryam Basereh","userId":"17916650843126787012"}}},"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from sklearn import datasets\n","from sklearn.cluster import KMeans\n","from mpl_toolkits.mplot3d import Axes3D"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Qh6-XrPe2Mb"},"source":["Next we import the Iris data from sklearn.datasets. I have printed out the feature names so I could label the 3-d plot. I have also printed out the correlation matrix and as in previous examples see that petal length and petal width are highly correlated, 96%. Sepal length is also highly correlated with these feature (81.7%)."]},{"cell_type":"code","metadata":{"id":"y_UVfN6EfxDF"},"source":["iris = datasets.load_iris()\n","X = iris.data  # we only take the first two features.\n","\n","y = iris.target\n","target_names=list(iris.target_names)\n","print(target_names)\n","df = pd.DataFrame({'Xsl':X[:,0],'Xsw':X[:,1],'Xpl':X[:,3],'Xpw':X[:,3],'y':y})\n","print(df)\n","corr = df.corr()\n","corr.style.background_gradient(cmap='coolwarm')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2I9cL8HismYL"},"source":["The next thing we will do is create a plot showing the various flower types and how there sepal length and sepal widths relate to each other."]},{"cell_type":"code","metadata":{"id":"69g8BPzue2xE"},"source":["#X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","groups=(\"1\",\"2\",\"3\")\n","fig, ax = plt.subplots()\n","\n","setosa=plt.scatter(df.loc[df['y']==0,'Xsl'].values, df.loc[df['y']==0,'Xsw'].values,alpha=0.8, c='blue', edgecolors='none', s=30)\n","versicolor=plt.scatter(df.loc[df['y']==1,'Xsl'].values, df.loc[df['y']==1,'Xsw'].values,alpha=0.8, c='red', edgecolors='none', s=30)\n","virginica=plt.scatter(df.loc[df['y']==2,'Xsl'].values, df.loc[df['y']==2,'Xsw'].values,alpha=0.8, c='yellow', edgecolors='none', s=30)\n","\n","plt.legend((setosa, versicolor, virginica),\n","           ('setosa', 'versicolor', 'virginica'),\n","           scatterpoints=1,\n","           loc='lower left',\n","           ncol=3,\n","           fontsize=8)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hJbaK1AttPPX"},"source":["We can see that setosa is easy to differentiate with these 2 variables, however the other are quite \"close\" to each other."]},{"cell_type":"code","metadata":{"id":"Lk7x_sZfgMFb"},"source":["fig = plt.figure(1, figsize=(8, 6))\n","ax = Axes3D(fig, elev=-150, azim=110)\n","\n","ax.scatter(X[:, 0], X[:, 1], X[:, 3], c=y,\n","           cmap=plt.cm.Set1, edgecolor='k', s=40)\n","ax.set_title(\"First three Features\")\n","ax.set_xlabel(\"sepal length (cm)\")\n","ax.xaxis.set_ticklabels([])\n","ax.set_ylabel(\"sepal width (cm)\")\n","ax.yaxis.set_ticklabels([])\n","ax.set_zlabel(\"petal length (cm)\")\n","ax.zaxis.set_ticklabels([])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3dDbTBegLG22"},"source":["The 3-d plot doesn't show us much more even though we can see petal length might help a little."]},{"cell_type":"markdown","metadata":{"id":"C8Urx58Te9EL"},"source":["The following piece of code is something you should probably do if you are trying to determine the value of k. In our case we know it is 3 but look how the graph below. The \"elbow\" of the curve tell us the optimum number of clusters is either 3. The point where the curve flattens out. To get the values used in the graph, we train multiple models using a different number of clusters and storing the value of the intertia_ property (within cluster sum of squares (WCSS)). As the number of clusters increases we expect the WCSS to reduce at a slower rate. The \"elbow\" really tells us the value of k where the WCSS is not reducing substantially."]},{"cell_type":"code","metadata":{"id":"Bwug7NNefFQL"},"source":["wcss = []\n","for i in range(1, 11):\n","    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n","    kmeans.fit(X)\n","    wcss.append(kmeans.inertia_)\n","plt.plot(range(1, 11), wcss)\n","plt.title('Elbow Method')\n","plt.xlabel('Number of clusters')\n","plt.ylabel('WCSS')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jX73n9_fHVj"},"source":["Now compare the clusters from the actual y values with the Kmeans predicted y values in the 2 graphs below. The cluster numbers will be different. Kmeans is just telling that certain rows belong to specific clusters."]},{"cell_type":"code","metadata":{"id":"XL408M6zfJk8"},"source":["kmeans = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\n","pred_y = kmeans.fit_predict(X)\n","print(pred_y)\n","print(y)\n","pred_y = np.where(pred_y == 1,3 , pred_y)\n","pred_y = np.where(pred_y == 2,4 , pred_y)\n","pred_y = np.where(pred_y == 0,5 , pred_y)\n","pred_y=pred_y-3\n","plt.scatter(X[:,0], X[:,1],s=100, c=pred_y)\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n","plt.title('Predicted Categories using kmeans')\n","plt.show()\n","plt.title('actual Categories using kmeans')\n","plt.scatter(X[:,0], X[:,1],s=100, c=y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6RnyegXqvuj-"},"source":["Finally, we have printed out the confusion matrix.\n","Play with the above code and see if you can get it to work on another dataset. As usual leave your thought on the comments board."]},{"cell_type":"code","metadata":{"id":"SegJWfzPdDeB"},"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(y, pred_y)"],"execution_count":null,"outputs":[]}]}