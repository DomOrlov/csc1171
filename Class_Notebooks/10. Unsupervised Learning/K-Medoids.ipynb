{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mFjr_j7XrGE8"},"source":["#**K-Mediods Clustering**\n","In the previous step we outlined how the kmeans algorithm is the most commonly used clustering algorithm. We also outlined how it has a number of drawbacks related to outliers and the size of dataset for example. K-mediods is a clustering algorithm that uses partitoning to to create clusters. Unlike the K-means algorithm which attempts to minmize the total squared error the K-mediods choose datapoints as centers. K-medoids is also a partitioning technique of clustering that clusters the data set of n objects into k clusters with k known a priori. Its more robust to outliers than the K-means algorithm an uses the mediod as oppossed to the average of a cluster to determine centrality.  \n","\n","The most common realisation of k-medoid clustering is the Partitioning Around Medoids (PAM) algorithm and is as follows:\n","\n",">* Initialize: randomly select k of the n data points as the medoids\n",">* Assignment step: Associate each data point to the closest medoid.\n",">* Update step: For each medoid m and each data point o associated to m swap m and o and compute the total cost of the configuration (that is, the average dissimilarity of o to all the data points associated to m). Select the medoid o with the lowest cost of the configuration.\n","Repeat alternating steps 2 and 3 until there is no change in the assignments.\n","\n","PAM works well for small datasets and deals with outliers well but has many of the other problems that K-means has. There are other alternatives such as [CLARA](https://codedocs.xyz/annoviko/pyclustering/namespacepyclustering.html) (Clustering Large Applications) algorithm which select random observations from the dataset and performs Partitioning Around Medoids (PAM) algorithm on them.\n","\n","\n","The following code implements the K-mediods algorithm. As usual play with code and try it out on a dataset of your chosing. Put your thoughts on the comments board.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9cMkdg1EWf4O"},"source":["You will have to run the following code as you will have to install sklearn_extra."]},{"cell_type":"code","metadata":{"id":"xpsnisMKUSkO"},"source":["!pip install 'numpy<2'\n","!pip install https://github.com/scikit-learn-contrib/scikit-learn-extra/archive/master.zip\n","# After running this cell, please restart the Colab runtime (Runtime -> Restart runtime) to apply the changes."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hdH0po0FRZ-m"},"source":["As usual we import the relevant python libraries."]},{"cell_type":"code","metadata":{"id":"tNgSM06WRZGI","executionInfo":{"status":"ok","timestamp":1763281017812,"user_tz":0,"elapsed":1769,"user":{"displayName":"Maryam Basereh","userId":"17916650843126787012"}}},"source":["import numpy as np\n","import pandas as pd\n","from matplotlib import pyplot as plt\n","from sklearn import datasets\n","from sklearn.cluster import KMeans\n","from sklearn_extra.cluster import KMedoids\n","from mpl_toolkits.mplot3d import Axes3D"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gYJDofJIUIgS"},"source":["Next we import the Iris data from sklearn.datasets. I have printed out the feature names so I could label the 3-d plot. I have also printed out the correlation matrix and as in previous examples see that petal length and petal width are highly correlated, 96%. Sepal length is also highly correlated with these feature (81.7%)."]},{"cell_type":"code","metadata":{"id":"R7c_5qfuRQ7U"},"source":["iris = datasets.load_iris()\n","X = iris.data  # we only take the first two features.\n","\n","y = iris.target\n","target_names=list(iris.target_names)\n","print(target_names)\n","df = pd.DataFrame({'Xsl':X[:,0],'Xsw':X[:,1],'Xpl':X[:,3],'Xpw':X[:,3],'y':y})\n","print(df)\n","corr = df.corr()\n","corr.style.background_gradient(cmap='coolwarm')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hi6nDTGlW9T3"},"source":["The next thing we will do is create a plot showing the various flower types and how there sepal length and sepal widths relate to each other."]},{"cell_type":"code","metadata":{"id":"3vkTFwMPrFdO"},"source":["#X, y = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n","groups=(\"1\",\"2\",\"3\")\n","fig, ax = plt.subplots()\n","\n","setosa=plt.scatter(df.loc[df['y']==0,'Xsl'].values, df.loc[df['y']==0,'Xsw'].values,alpha=0.8, c='blue', edgecolors='none', s=30)\n","versicolor=plt.scatter(df.loc[df['y']==1,'Xsl'].values, df.loc[df['y']==1,'Xsw'].values,alpha=0.8, c='red', edgecolors='none', s=30)\n","virginica=plt.scatter(df.loc[df['y']==2,'Xsl'].values, df.loc[df['y']==2,'Xsw'].values,alpha=0.8, c='yellow', edgecolors='none', s=30)\n","\n","plt.legend((setosa, versicolor, virginica),\n","           ('setosa', 'versicolor', 'virginica'),\n","           scatterpoints=1,\n","           loc='lower left',\n","           ncol=3,\n","           fontsize=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ynruAHfmjTBi"},"source":["We will now implement the K-Mediods and can see that it out performs the kmeans algorithm from the previous step. Have a look at the code below and note that there is an option to specify the metric that you are going to use for the distance measure. The default option is the euclidean distance."]},{"cell_type":"code","metadata":{"id":"Cr-y-qnoXHYP"},"source":["#KMedoids\n","kmedoids = KMedoids(n_clusters=3, random_state=0,).fit(X)\n","pred_y = kmedoids.predict(X)\n","print(pred_y)\n","pred_y = np.where(pred_y == 2,3 , pred_y)\n","pred_y = np.where(pred_y == 1,4 , pred_y)\n","pred_y = np.where(pred_y == 0,5 , pred_y)\n","pred_y=pred_y-3\n","print(pred_y)\n","print(y)\n","plt.scatter(X[:,0], X[:,1],s=100, c=pred_y)\n","plt.scatter(kmedoids.cluster_centers_[:, 0], kmedoids.cluster_centers_[:, 1], s=300, c='red')\n","plt.title('Predicted Categories using kmedoids')\n","plt.show()\n","plt.title('actual Categories using kmedoids')\n","plt.scatter(X[:,0], X[:,1],s=100, c=y)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q_n69u3teCDr"},"source":["Finally, we have printed out the confusion matrix.\n","Play with the above code and see if you can get it to work on another dataset. As usual leave your thought on the comments board."]},{"cell_type":"code","metadata":{"id":"yMA19jFjcdTS"},"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(y, pred_y)"],"execution_count":null,"outputs":[]}]}