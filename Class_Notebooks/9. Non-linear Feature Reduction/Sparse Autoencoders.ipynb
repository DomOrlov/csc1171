{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"981jOfQFzUJd"},"source":["#**Sparse AutoEncoders**\n","\n","In the previous step, the input layer was constrainedby by the size of the hidden layer (128). In such a situation, what typically happens is that the hidden layer is learning an approximation of PCA (principal component analysis). But another way to constrain the representations to be compact is to add a sparsity contraint on the activity of the hidden representations, so fewer units would \"fire\" at a given time. In Keras, this can be done by adding an activity_regularizer to our Dense layer. The code below outlines how this works. In particular, look at the line in the code that specifies the regularization:\n","\n","> \"activity_regularizer=regularizers.l1(10e-5)\"\n","\n","Remember L1 regularization constraints the cost function with and absolute value of the magnitude of the weights\n","\n",">$\\lambda \\sum_{j=1}^p\\lvert \\beta_j \\rvert$ where $p$ is the number of weights\n","\n","L1 regularization shrinks the less important featureâ€™s coefficient to zero thus, removing some feature altogether.\n","\n","In the following example we use the  [\"binary_crossentropy\"](https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a) loss or cost function. This term effectively tries to maximise the Log-loss function and is very similar to the entropy calculations we did when attempting to discretize a continous variable. In this approach we are trying to match pixels rather than match the intensity of them which the \"mean_squared_error\" does.\n","\n","\n","Now when you print how out the hidden feature set you will notice that a number of them are zero. This means we have introduced sparsity into the autoencoder.\n","\n","Have a go at changing the loss function and changing the optimizers in the following code. See how your results change.\n","\n","Share your thoughts on the comment box.\n","\n"]},{"cell_type":"code","metadata":{"id":"zl4wP-9q2bXS"},"source":["import tensorflow as tf\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","from keras.utils import plot_model\n","from keras import regularizers\n","import numpy as np\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","\n","img_rows, img_cols = 28, 28\n","\n","x_train = x_train.astype('float32') / 255.\n","x_test = x_test.astype('float32') / 255.\n","x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n","x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n","print(x_train.shape)\n","print(x_test.shape)\n","\n","#,activity_regularizer=regularizers.l1(10e-5)\n","\n","model = Sequential()\n","model.add(Dense(32,activation='relu',activity_regularizer=regularizers.l1(10e-5),input_dim=784))\n","model.add(Dense(784,activation='sigmoid'))\n","#loss_choice='mean_squared_error'\n","loss_choice='binary_crossentropy'\n","#model.compile(optimizer='adadelta', loss=loss_choice,metrics = ['accuracy'])\n","model.compile(loss=loss_choice,\n","              optimizer='adam',\n","              metrics = ['accuracy'])\n","\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jHb93KD6pK6"},"source":["#print(x_train.shape)\n","\n","history=model.fit(x_train,x_train,verbose=1,epochs=50,batch_size=256,shuffle=True,)\n","model.save('auto_en.h5')\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSJV1Gvicjfy"},"source":["We have now trianed our algorithm and I want to find the new decompressed variables. I have used an alternative piece of code to that shown previously. In it we basically create a new neural network model by mimicing the original neural network up to the hidden layer. We then insert the weights from the original model. We then use the new_model predictions to get outputs."]},{"cell_type":"code","source":["model.layers[0].get_weights()[0][1]"],"metadata":{"id":"IgmVwBSWwIth"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"44rRxBGwb7FM"},"source":["new_model = Sequential()\n","new_model.add(Dense(32,activation='relu',activity_regularizer=regularizers.l1(10e-5),input_dim=784))\n","new_model.set_weights(model.layers[0].get_weights())\n","new_model.compile(optimizer='adam', loss='categorical_crossentropy')\n","output = new_model.predict(x_train)\n","print(output[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2dmiUBC8atb"},"source":["predicted_image = model.predict(x_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AqrwIgbb8mpX"},"source":["# use Matplotlib (don't ask)\n","import matplotlib.pyplot as plt\n","\n","n = 10  # how many digits we will display\n","plt.figure(figsize=(20, 4))\n","for i in range(n):\n","    # display original\n","    ax = plt.subplot(2, n, i + 1)\n","    plt.imshow(x_test[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","\n","    # display reconstruction\n","    ax = plt.subplot(2, n, i + 1 + n)\n","    plt.imshow(predicted_image[i].reshape(28, 28))\n","    plt.gray()\n","    ax.get_xaxis().set_visible(False)\n","    ax.get_yaxis().set_visible(False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ScuJ8fa7-xb9"},"source":["import matplotlib.pyplot as plt\n","\n","\n","\n","\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]}]}