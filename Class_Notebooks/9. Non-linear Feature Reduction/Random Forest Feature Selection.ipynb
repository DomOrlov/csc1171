{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KiVFHJ65t54N"},"source":["#**Random Forest Feature Selection**\n","\n","In this step we are going to outline how the random forest machine learning approach can help in determining the importance of a feature.\n","Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting, and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision. In scikit learn random forest library the relative rank (i.e. depth) of a feature used as a decision node in a tree can be used to assess the relative importance of that feature with respect to the predictability of the target variable. Features used at the top of the tree contribute to the final prediction decision of a larger fraction of the input samples. The expected fraction of the samples they contribute to can thus be used as an estimate of the relative importance of the features, [scikt learn](https://scikit-learn.org/stable/modules/ensemble.html#forest).\n","\n","Random forests are also incredibly robust and generally are very easy to implement. In every project I give, students always use Random forests for predictions. Lately, I have also found them using them for dimensionality reduction. However, there is an issue with them and Sebastian Raschka explains it really nicely below:\n","\n","\n","\"The random forest technique comes with an important gotcha that is worth mentioning. For instance, if two or more features are highly correlated, one feature may be ranked very highly while the information of the other feature(s) may not be fully captured. On the other hand, we don't need to be concerned about this problem if we are merely interested in the predictive performance of a model rather than the interpretation of feature importances.\" - Python Machine Learning by Sebastian Raschka.\n","\n","This \"gotcha\" is really important to understand and in a way negates the use of random forests for feature importance and thus the use of them for feature reduction and engineering.\n","\n","Have a look at the code below and play about with the standardisation methods from scikit learn. The description of how the Ranodmforest Classifier works can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html?highlight=randomforest#sklearn.ensemble.RandomForestClassifier).\n"]},{"cell_type":"markdown","metadata":{"id":"UsIyHV_L6_7r"},"source":["Aa  test example we are going to use the Scikit learns wine dataset. We can import this directly. In this example the $y$ variable is the wine class of which there are 3. The $X$ variables are characteristics that are normally used to characterize wine."]},{"cell_type":"code","metadata":{"id":"CJY29dmAvFrn"},"source":["import pandas as pd\n","from sklearn.datasets import load_wine\n","import numpy as np\n","data = load_wine()\n","\n","X=data.data\n","print(len(X[1,:]))\n","y=data.target\n","df_X=pd.DataFrame(X,columns=data.feature_names)\n","print(df_X.corr())\n","\n","import seaborn as sns\n","%matplotlib inline\n","\n","\n","# calculate the correlation matrix\n","corr = df_X.corr()\n","\n","# plot the heatmap\n","sns.heatmap(corr,\n","        xticklabels=corr.columns,\n","        yticklabels=corr.columns)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KGThW5y-FMNr"},"source":["For demonstration purposes we split the dataset into training and test. However, if you are doing this on  a real dataset it maybe worth while using a bootstrapping approach."]},{"cell_type":"code","metadata":{"id":"jYQ3k4X9xZmG"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = \\\n","        train_test_split(X, y, test_size=0.3, random_state=0)\n","print(X_train[1,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JodBuW91Fj0j"},"source":["The next 2 pieces of code are not really necessary but demonstrate how random forests don't need normalised or standarised data."]},{"cell_type":"code","metadata":{"id":"AqH_4o5NyNo-"},"source":["from sklearn.preprocessing import MinMaxScaler\n","mms = MinMaxScaler()\n","X_train = mms.fit_transform(X_train)\n","X_test = mms.transform(X_test)\n","print(X_train[1,:])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kkx7EV88yUs7"},"source":["from sklearn.preprocessing import StandardScaler\n","stdsc = StandardScaler()\n","X_train = stdsc.fit_transform(X_train)\n","X_test = stdsc.transform(X_test)\n","print(X_train[1,:])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWR8gjw_Fw_C"},"source":["The next code fragement imports random forest classifer from scikit learn. The n_estimators refers to the number of random trees we are going to examine and the n_jobs=-1 specifies that all available processors should be used. I have also added in a variable here called no_f. This variable allows us to set the number of features in our training set."]},{"cell_type":"code","metadata":{"id":"6WCMcVqTy_86","executionInfo":{"status":"ok","timestamp":1763135289720,"user_tz":0,"elapsed":1597,"user":{"displayName":"Maryam Basereh","userId":"17916650843126787012"}}},"source":["from sklearn.ensemble import RandomForestClassifier"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gKmyCVCFzHVy"},"source":["no_f=13\n","feat_labels = data.feature_names[0:no_f]\n","forest = RandomForestClassifier(n_estimators=10000, random_state=0, n_jobs=-1)\n","forest.fit(X_train[:,0:no_f], y_train)\n","importances = forest.feature_importances_\n","indices = np.argsort(importances)[::-1]\n","for f in range(X_train[:,0:no_f].shape[1]):\n","    print(\"%2d) %-*s %f\" % (f + 1, 30, feat_labels[f],importances[indices[f]]))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JtsRCZ6m0h6a"},"source":["Now we are going to plot the attribute importance."]},{"cell_type":"code","metadata":{"id":"d9cViQll15zi"},"source":["import matplotlib.pyplot as plt\n","\n","plt.title('Feature Importances')\n","plt.bar(range(X_train.shape[1]), importances[indices], color='green', align='center')\n","plt.xticks(range(X_train.shape[1]),feat_labels, rotation=90)\n","plt.xlim([-1, X_train.shape[1]])\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOjO_vWbK4f4"},"source":["Adjust the variable that go into the $X_train$ dataset. See if the variable importance is affected by the correlation between the features. Put your comments on the comments board."]}]}