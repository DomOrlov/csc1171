{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMUHSI36dZ7C"
   },
   "source": [
    "# **Statistical Tests**\n",
    "\n",
    "**1 Introduction**\n",
    "\n",
    "The purpose of a Statistical test is to check a hypothesis or a prediction about a population parameter using an estimate of that parameter calculated from a sample of the population.\n",
    "These tests have many applications in business. For instance\n",
    ">\tHas a marketing campaign increased a company’s sales figures?\n",
    "\n",
    ">\tHas a marketing campaign increased a company’s market share?\n",
    "In many business examples tests are used to examine differences between groups.\n",
    "\n",
    ">Do males and females spend the same amount on magazines?\n",
    "\n",
    ">Are urban or rural consumers more likely to buy a product?\n",
    "\n",
    ">Do consumers in France, Germany, Ireland and the UK have different views on what is important when buying a car?\n",
    "\n",
    "**Why do we need statistical tests?**\n",
    "\n",
    "Consider the results of a survey that show that a sample of males spent an average of €20 a week on magazines and that a sample of females spent an average of €21 a week on magazines.\n",
    "\n",
    "Does this prove that females spend more on magazines on average?\n",
    "\n",
    "Not necessarily because we know that both results are only sample estimates. It is not possible to directly prove or disprove an hypothesis using sample estimates alone, because these statistics will have inherent sample errors.\n",
    "\n",
    "So we need some technique or techniques to help us decide whether a sample parameter provides enough evidence to prove a hypothesis about the population parameter. These techniques are called statistical tests or hypothesis tests.\n",
    "\n",
    "These tests are designed to check if the sample results observed are statistically significant.  Put simply, in the example above we would like to know if the difference between males and females was due to random chance or due to a significant difference in expenditure between them.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvXtdIiRdtoL"
   },
   "source": [
    "# **2 Steps involved in a Statistical Test**\n",
    "\n",
    "**(i) State the Null and Alternative Hypotheses**\n",
    "\n",
    "The first step is to outline the objectives of the test. The Null and Alternative Hypothesis are opposing statements about the population parameter being tested. In general the researcher is trying to prove the Alternative Hypothesis by showing that the Null Hypothesis is false.\n",
    "For instance, consider HR Research Ltd. who are examining pay rates in two different sectors, A and B, to test if there is a differences between them using a sample of employees from both sectors. In this case the Null Hypothesis would state that the Average Salaries in both sectors are equal, while the Alternative would state that the Average Salaries are not equal.\n",
    "\n",
    "Mathematically this can be written as\n",
    "\n",
    "Null\n",
    " \t$H_0$: $\\mu_{a} $  = $\\mu_{b} $  or  $\\mu_{a}-\\mu_{b} $ = 0 (Average Salary is equal in both sectors)\n",
    "\n",
    "Alternative\n",
    "$H_1$: $\\mu_{a} <> \\mu_{b}$ or $\\mu_{a} - \\mu_{b} <> 0$ (Average Salary is not equal in both sectors)\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**ii)\tSet the precision level**\n",
    "\n",
    "At the initial stage the researcher will also set the level of precision for the test. This is the level of confidence the researcher wishes to achieve before rejecting the null hypothesis.\n",
    "The 95% confidence level is the accepted norm in Social Science and Business Research although occasionally some researchers will use other levels (99%, 90% etc.). Note we can never be 100% confident about a sample result.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**(iii)\tGather sample data and calculate sample statistics**\n",
    "\n",
    "To test the hypothesis a sample of data is selected from the population and the relevant sample statistics (mean, standard deviation, proportion etc.) are calculated.\n",
    "In the HR Research example, they would need to calculate the relevant descriptive statistics for the problem. In this case they would need the sample means, sample standard deviations and sample sizes.\n",
    "\n",
    "$\\bar{X}_a =$ €$30500$\t$\\bar{X}_b\t  =$ €$30,000$\n",
    "\n",
    "$S_A   =$ €$9,400$\t\t$S_B   =$ €$8,900\t\t$\n",
    "\n",
    "$ n_A= 100$\t\t $n_B = 100$\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "The t-statistic can be calculated using the following formula:\n",
    "![probabiltiy_example](https://www.computing.dcu.ie/~amccarren/mcm_images/2_sample_t_test.png)\n",
    "\n",
    "\n",
    "\n",
    "with df=$n_x+n_y-2$\n",
    "\n",
    "The simple python code for the above example is shown below. Vary $N_a$ and $N_b$ and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1705424832998,
     "user": {
      "displayName": "Andrew Mccarren",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "IiXqfi8u3clA",
    "outputId": "67f82ab4-6a4e-48ec-926a-ea2ea802952d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Calculate the Standard Deviation\n",
    "#Calculate the variance to get the standard deviation\n",
    "\n",
    "#For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\n",
    "var_a = 9200**2\n",
    "var_b = 8900**2\n",
    "\n",
    "#std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "\n",
    "Na=100\n",
    "Nb=100\n",
    "\n",
    "## Calculate the t-statistics\n",
    "t = (30500 - 30200)/(np.sqrt((var_a/Na)+(var_b/Nb)))\n",
    "\n",
    "\n",
    "\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = Na+Nb - 2\n",
    "\n",
    "#p-value after comparison with the t\n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmHq-np25sPX"
   },
   "source": [
    "You will notice as $N_a$ and $N_b$ increase we get a lower p value. This means as sample size increase we have a greater condfidence that there is a difference between the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxgRprwT3G_p"
   },
   "source": [
    "The following code calculates the p value manually and using a scipy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1643025304029,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "YnVnHaCAleDb",
    "outputId": "888b965d-6421-46e8-8ee6-3c4b9ee49f85"
   },
   "outputs": [],
   "source": [
    "# Import the packages\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "## Define 2 random distributions\n",
    "#Sample Size\n",
    "N = 10\n",
    "#Gaussian distributed data with mean = 2 and var = 1\n",
    "a = np.random.randn(N) + 2\n",
    "#Gaussian distributed data with with mean = 0 and var = 1\n",
    "b = np.random.randn(N)\n",
    "\n",
    "\n",
    "## Calculate the Standard Deviation\n",
    "#Calculate the variance to get the standard deviation\n",
    "\n",
    "#For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\n",
    "var_a = a.var(ddof=1)\n",
    "var_b = b.var(ddof=1)\n",
    "\n",
    "#std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "s\n",
    "\n",
    "\n",
    "\n",
    "## Calculate the t-statistics\n",
    "t = (a.mean() - b.mean())/(s*np.sqrt(2/N))\n",
    "\n",
    "\n",
    "\n",
    "## Compare with the critical t-value\n",
    "#Degrees of freedom\n",
    "df = 2*N - 2\n",
    "\n",
    "#p-value after comparison with the t\n",
    "p = 1 - stats.t.cdf(t,df=df)\n",
    "\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))\n",
    "### You can see that after comparing the t statistic with the critical t value (computed internally) we get a good p value of 0.0005 and thus we reject the null hypothesis and thus it proves that the mean of the two distributions are different and statistically significant.\n",
    "\n",
    "\n",
    "## Cross Checking with the internal scipy function\n",
    "t2, p2 = stats.ttest_ind(a,b)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhZ-cFDHleiW"
   },
   "source": [
    "**(iv)\tChoose the appropriate Statistical Test**\n",
    "\n",
    "To find the appropriate test, we must look at the type of variable we are testing and what the test is actually trying to show.\n",
    "In the HR Research example, they are looking at a difference in salaries, so the test variable is Salary, which is a Ratio variable. They are trying to compare the salaries in two sectors, A and B, so the test is a measure of the difference between 2 groups. The appropriate test for this problem type is called an Independent Sample t-Test.\n",
    "We will look at a sample of some of the more widely used tests later in this section\n",
    "\n",
    "<br/>\n",
    "\n",
    "**(v)\tCalculate the Test Statistic**\n",
    "\n",
    "To test the Null Hypothesis, the difference between the hypothesised value of the parameter and the sample value of the parameter is calculated.\n",
    "For instance, The Null hypothesis for the HR Research problem stated that the difference between salaries in the two sectors is equal to zero.\n",
    "However they found in a survey that respondents in Sector A were earning on average €500 more than the respondents in Sector B.\n",
    "In order to test if this difference is statistically significant we calculate a test statistic (in this case a t-value) which is, put simply, a measure of the relative size of the difference taking into account the amount of variation in the data and the sample size.\n",
    "The t-value for the HR Research data is equal to .386\n",
    "At the end of this section, we will briefly look at how two of these test statistics are calculated, but in general this calculation is performed using appropriate statistical software.\n",
    "\n",
    "<br/>\n",
    "\n",
    "**(vi)\tInterpret the Results of the Test / Examine the P-value**\n",
    "\n",
    "Unfortunately there are a large number of test statistics and they are not always easy to interpret, without a detailed knowledge of how the particular test works. What for instance does a t-value equal to .386 tell us?\n",
    "Fortunately, due to developments in statistical software, there is a simpler way of interpreting the results of a statistical test, by using what is called the p-value of the test, which is common to all tests.\n",
    "\n",
    "The p-value measures the probability of calculating the observed sample value if the Null Hypothesis was in fact true.\n",
    "\n",
    "The decision to reject or “accept” the null hypothesis (to test if it is false) can be made by examining the p-value.\n",
    "Quite simply, if the p-value is less than .05 then there is less than 5% of a chance that the Null Hypothesis is true and conversely more than a 95% chance that is false, so it can be rejected with 95% confidence.\n",
    "However if it is above .05, then the Null Hypothesis cannot be rejected at this level.\n",
    "The p-value for the HR Research study was .7, so we cannot reject the null hypothesis, which put simply, indicates that there is no evidence to suggest that there are significant salary differences between Sectors A and B.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcK9XVuD6XQl"
   },
   "source": [
    "# **3 Summary of Statistical Tests for Differences**\n",
    "\n",
    "Here is a list of some of the more widely used tests in Business and where they should be applied.\n",
    "\n",
    "**Tests for Differences Between Two Independent Groups**\n",
    "\n",
    "Test Variable\t\tStatistical Test\n",
    "  \n",
    "Nominal Variable\t- \tChi-Square Test\n",
    "\n",
    "Ratio Variable \t- \tIndependent Sample T-Test/Z-Test*\n",
    "\n",
    "Ordinal Variable\t-\tMann-Whitney U Test\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Difference Between N>2 – Independent Groups**\n",
    "\n",
    "Test Variable\t\t\tStatistical Test\n",
    "\n",
    "Nominal Variable\t-\tChi-Square Test\n",
    "\n",
    "Ratio Variable\t\t- \tOne – Way ANOVA*\n",
    "\n",
    "Ordinal Variable\t-\tKruskal – Wallis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgKKRHZL8S48"
   },
   "source": [
    "# **4 Assumptions underlying statistical tests**\n",
    "\n",
    "Most statistical tests have a number of assumptions underlying them. It is important to check these assumptions when using a test. Violations in assumptions can lead to incorrect conclusions being made, when interpreting the test results.\n",
    "\n",
    "**Parametric Tests (Normality assumption)**\n",
    "\n",
    "*The T-Tests and One-way Anova are from a family of tests called parametric tests, which are based on the assumption, that the test variable is Normally distributed.\n",
    "This means that strictly speaking parametric tests can only be used for ratio/interval variables, that follow a Normal Distribution. This assumption is relaxed for sample statistics calculated from a large sample (n>50), but for smaller samples, the normality of the variable should be tested before using a parametric test.\n",
    "If they are not Normal the non-parametric equivalent (Mann-Whitney U Test or Kruskal Wallis) should be used instead.\n",
    "\n",
    "\n",
    "**Sample Size Requirements**\n",
    "\n",
    "Sample size is also an important factor. Clearly the bigger the sample size the more powerful the test and the more likely we are to identify statistically significant results. As well as looking at the overall sample size we need to look at the sample sizes in each group as well.\n",
    "\n",
    "- The strict minimum per group for a statistical test is 3, however the practical minimum will be higher for most tests.\n",
    "- The minimum sample size per group recommended for the parametric tests is 10.\n",
    "- There are similar requirements for non-parametric tests.\n",
    "- The Chi-Square assumes an expected minimum sample size of 5 in each cell. If this assumption is violated it can affect the validity of the test. PASW/SPSS indicates how many cells violate this assumption.\n",
    "\n",
    "Sometimes it is necessary to collapse groups or remove a group from an analysis, if the small size within the group is too small.\n",
    "Consider a class where there are 40 students; 15 from Ireland, 15 from the UK, 4 from Germany, 4 from France and 2 from Spain.\n",
    "We cannot conduct a One Way ANOVA on the five groups due to small sample sizes. However we may decide to either just compare the Irish and UK students or combine the French, German and Spanish students into a third group.\n",
    "\n",
    "\n",
    "**Independent Observations**\n",
    "\n",
    "The independence assumption is one of the most important and can have the most detrimental effects on the test results. All of the tests we examined here require independent measurements. In other words each data point must be independent of the others.\n",
    "\n",
    "For instance, if we have a set of monthly sales figures for a product, we cannot say that these are independent as clearly last month’s sales will have an impact on this month’s sales.\n",
    "\n",
    "This is only a sample of the issues involved in testing assumptions. It can be a complex issue as there will always be some violations of these assumptions, so we need to look at the size and nature of the violations. There are also differing views in different disciplines as to how these violations should be tackled.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KPRoMbz9MTc"
   },
   "source": [
    "## 5 Chi-Square Test  $\\chi{2}$\n",
    "\n",
    "The Chi-Square Test is used when examining the difference between two or more groups and the test variable is Nominal. Example 7.2 illustrates the use of this test. It can also be seen as a test that measures the relationship between two nominal variables.\n",
    "\n",
    "**Example 7.2**\n",
    "\n",
    "The following table shows the breakdown of students by gender and mode of transport to college.\n",
    "Tables:\n",
    "\n",
    ">Mode of Transport | \tMale | \tFemale\t | Total\n",
    ">--- | --- | --- | ---\n",
    ">Bus | 80\t| 220\t| 300\n",
    ">Walk| 50\t| 50\t| 10  \n",
    ">Bicycle | 40 |\t10 |\t50\n",
    ">Car |\t30 |\t20 |\t50\n",
    ">Total |\t200 |\t300 |\t500\n",
    "\n",
    "Does this data suggest that males and females use different modes of transport to get to college?\n",
    "The test variable in this example is mode of transport, which is a Nominal variable. We are examining the difference between two groups; males and females, so the appropriate test is the Chi-Square Test.\n",
    "\n",
    "This test is a little more complex than the Z-test and involves four steps.\n",
    "\n",
    "\n",
    "(i) **State the Null and Alternative Hypotheses**\n",
    "\n",
    "\n",
    "$H_0$: Males and Females use the same modes of transport to get to college\n",
    "\n",
    "$H_1$: Males and Females do not use the same modes of transport to get to college\n",
    "\n",
    "<br/>\n",
    "\n",
    "(ii) **Calculate the Expected values**\n",
    "\n",
    "The Expected values are the values we would expect to get form the survey if the Null Hypothesis was true.\n",
    "\n",
    "For instance if there was no difference between males and females, how many males would we expect to take the bus?\n",
    "300 students out of 500 take the bus, which is equal to 60%(.6) of the students, so if there was no difference between the genders, we would expect 60% of males and 60% of females to take the bus. So\n",
    "\n",
    "<br/>\n",
    "\n",
    "(ii) **Calculate the Expected values**\n",
    "\n",
    "The Expected values are the values we would expect to get form the survey if the Null Hypothesis was true.\n",
    "\n",
    "For instance if there was no difference between males and females, how many males would we expect to take the bus?\n",
    "300 students out of 500 take the bus, which is equal to 60%(.6) of the students, so if there was no difference between the genders, we would expect 60% of males and 60% of females to take the bus. So\n",
    "\n",
    "$E(Bus-Male) = 200$x$300/500=120$\n",
    "\n",
    "$E (Bus-Female) =  300$x$300/500=180$\n",
    "\n",
    "We can calculate the rest of the expected values using the same formula\n",
    "$E( A and B) =n_a$.$n_b/n$   where\n",
    "\n",
    "$n_a$ =Number in category A,\n",
    "$n_b$ =Number in category B.\n",
    "\n",
    "$n$ = Total number in sample\n",
    "\n",
    "<br/>\n",
    "\n",
    "**iii) Calulate the Chi-Square Value**\n",
    "\n",
    "The next step is to calculate the difference between the values observed(Oi) in our survey and the expected theoretical values(Ei) using the following formula\n",
    "\n",
    ">>>$\\chi{2}$=  $ \\frac{\\sum_{i=1}^n (O_i-E_i)}{E_i}$\n",
    "   \n",
    "   \n",
    "The Chi-Square provides a measure of the average relative squared differences between the observed and expected values.\n",
    "\n",
    "The simplest way to calculate the Chi-Square statistic is to set up the following table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "ok",
     "timestamp": 1705425275721,
     "user": {
      "displayName": "Andrew Mccarren",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "fA9WM3Q2D7M0",
    "outputId": "0846de11-dd94-43e2-a08e-87e99f2d2100"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#a1 = [80, 220]\n",
    "#a2 = [50, 50]\n",
    "#a3 = [40, 10]\n",
    "#a4 = [30,20]\n",
    "\n",
    "a1 = [20, 13]\n",
    "a2 = [48, 96]\n",
    "#a3 = [40, 10]\n",
    "#a4 = [30,20]\n",
    "\n",
    "#transport = np.array([a1, a2, a3, a4])\n",
    "transport = np.array([a1, a2])\n",
    "\n",
    "chi2_stat, p_val, dof, ex = stats.chi2_contingency(transport)\n",
    "print(\"===Chi2 Stat===\")\n",
    "print(chi2_stat)\n",
    "print(\"\\n\")\n",
    "print(\"===Degrees of Freedom===\")\n",
    "print(dof)\n",
    "print(\"\\n\")\n",
    "print(\"===P-Value===\")\n",
    "print(p_val)\n",
    "print(\"\\n\")\n",
    "print(\"===Contingency Table===\")\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o7Nz3s-eFL9z"
   },
   "source": [
    "So lets interpret these results. The Chi2-stat is basically the $\\chi{2}$ statistic in the above formula.The degrees of freedom is calculated by the following formula:\n",
    "\n",
    "$(Rows-1)$.$(Columns-1)$$=(4-1).(2-1)=3$\n",
    "\n",
    "The p-value is really small and is <0.05.This tells us that the difference between the males and females is well beyond what we would expect to get due to random chance. We can therefore reject the null hypothesis and accept the alternative, that there is a significant difference between males and females.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6POS8l_TUAp"
   },
   "source": [
    "# **6 Analysis of Variance (ANOVA)**\n",
    "\n",
    "**What is ANOVA (ANalysis Of VAriance)?**\n",
    "\n",
    "An ANOVA is generally used when we are comparing more than 2 groups for an outcome variable. In section 2 we compared the means from 2 samples taken from 2 distinct groups. We used a t-test to determine if would accept or reject the Null hypothesis.\n",
    "\n",
    "Null Hypothesis was\n",
    " \t$H_0$: $\\mu_{a} $  = $\\mu_{b} $  or  $\\mu_{a}-\\mu_{b} $ = 0 (Average Salary is equal in both sectors)\n",
    "\n",
    "Alternative Hypothesis was\n",
    "$H_1$: $\\mu_{a} <> \\mu_{b}$ or $\\mu_{a} - \\mu_{b} <> 0$ (Average Salary is not equal in both sectors)\n",
    "\n",
    "<br/>\n",
    "\n",
    "With an ANOVA we are extending this principal at its simplest level to more than 2 groups. So now our questions would be is the mean of each group the same and our Null hypothesis would be :\n",
    "\n",
    "$H_0$: $\\mu_{a} $  = $\\mu_{b} = \\mu_{c} =\\mu_{d}$\n",
    "\n",
    "and the alternative implies that at least one meand is different from other groups:\n",
    "\n",
    "$H_0$: $\\mu_{a} $  <> $\\mu_{b} <> \\mu_{c} <>\\mu_{d}$\n",
    "\n",
    "This is a one factor analysis and it can be extended to multi faxtor studies with covariate variables, but more on this when we talk about Generalised Linear Models(GLM)\n",
    "\n",
    "ANOVA Assumptions\n",
    "\n",
    "*   Residuals (experimental error) are normally distributed (Shapiro Wilks Test)\n",
    "*   Homogeneity of variances (variances are equal between treatment groups) (Levene or Bartlett Test)\n",
    "*   Observations are sampled independently from each other\n",
    "\n",
    "Lets look at a dataset which gives medical data for 4 different groups of patients.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "groups mean differences inferred by analyzing variances\n",
    "Main types: One-way (one factor) and two-way (two factors) ANOVA (factor is an independent variable)\n",
    "Note: In ANOVA, group, factors, and independent variables are similar terms.\n",
    "\n",
    "\n",
    "Check sample sizes: equal number of observation in each group\n",
    "Calculate Mean Square for each group (MS) (SS of group/level-1); level-1 is a degree of freedom (df) for a group\n",
    "Calculate Mean Square error (MSE) (SS error/df of residuals)\n",
    "Calculate F-value (MS of group/MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "executionInfo": {
     "elapsed": 644,
     "status": "ok",
     "timestamp": 1643025524894,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "Tl5BLt48Xw4Z",
    "outputId": "a8612415-5291-4022-cf30-9fb695dad05f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# load data file\n",
    "\n",
    "d = pd.read_csv(\"https://www.reneshbedre.com/assets/posts/anova/onewayanova.txt\", sep=\"\\t\")\n",
    "print(d.describe())\n",
    "# generate a boxplot to see the data distribution by treatments. Using boxplot, we can easily detect the differences\n",
    "# between different treatments\n",
    "d.boxplot(column=['A', 'B', 'C', 'D'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2dSwTAVX9Pu"
   },
   "source": [
    "You can see from the boxplot that groups A and C are quite similar. However, group D does seem to be quite a bit away from A,B ad C, but is this statistically significant. ANOVA basically works by grouping the between group variation with the within group variation. If the within group variation is really small and the between group variation is really large then we can intuitively understand that there is a difference between the groups. When we compare these 2 statistics and they will follow a F distribution as long as the assumptions outlined earlier are met.\n",
    "\n",
    "Lets complete the ANOVA analysis below:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "executionInfo": {
     "elapsed": 1205,
     "status": "ok",
     "timestamp": 1643025548193,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "cOObGr35aJBp",
    "outputId": "446fc0a0-1784-40be-8e97-a5f4974fb15e"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import scipy.stats as stats\n",
    "# stats f_oneway functions takes the groups as input and returns F and P-value\n",
    "fvalue, pvalue = stats.f_oneway(d['A'], d['B'], d['C'], d['D'])\n",
    "print(fvalue, pvalue)\n",
    "# 17.492810457516338 2.639241146210922e-05\n",
    "\n",
    "# get ANOVA table as R like output\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "# reshape the d dataframe suitable for statsmodels package\n",
    "d_melt = pd.melt(d.reset_index(), id_vars=['index'], value_vars=['A', 'B', 'C', 'D'])\n",
    "# replace column names\n",
    "d_melt.columns = ['index', 'treatments', 'value']\n",
    "# Ordinary Least Squares (OLS) model\n",
    "model = ols('value ~ C(treatments)', data=d_melt).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "anova_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX1dH6jFsX7s"
   },
   "source": [
    "From the analysis above we can see that the $F_{3,16}$ ratio is 16.429 with a P-value of 0.000026. This tells us that there is sufficent evidence to reject the null hypothesis and that we can accept the alternative hypothesis that at least one of groups has a different mean. The following code does a pairwise comparison between each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 263,
     "status": "ok",
     "timestamp": 1643025620248,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "7OIE4YHdtizD",
    "outputId": "9a1c1222-2e39-49de-db6e-1a2b7b6a9c4a"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "# perform multiple pairwise comparison (Tukey HSD)\n",
    "m_comp = pairwise_tukeyhsd(endog=d_melt['value'], groups=d_melt['treatments'], alpha=0.05)\n",
    "print(m_comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPDcy4yjt6kG"
   },
   "source": [
    "The Tukey HSD takes allows us to complete what is known as a post hoc tests. These test will do all the pairwise comparisons between all the groups. There are 6 comparisons and in only one case do we accept (A vs C) the null hytpothesis.\n",
    "\n",
    "The next test we will complete here is to check the Homogeneity of variances. This is important as one of the assumptions we made was the variance within each groups was relatively constant across groups. the levnee test is used and the code to demostrate it is shown below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1643025623462,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "j5zDKg3vvR9d",
    "outputId": "9c18919b-125d-477a-f3e4-2b7f1bff4881"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "w, pvalue = stats.levene(d['A'], d['B'], d['C'], d['D'])\n",
    "print(w, pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9q_2rFjvcZj"
   },
   "source": [
    "The Levene statistic is 1.9219 and the P-value is 0.166. This implies that we cannot reject the null hypothesis for this test which asks are the variances constant across the groups.\n",
    "\n",
    "The final test we will conduct is the Sharpiro Wilks test. The null hypothesis for this test is that the errors for the model in the OLS function are from a normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1643025626034,
     "user": {
      "displayName": "Andrew McCarren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "16186536572019350587"
     },
     "user_tz": 0
    },
    "id": "W-pXbmRcweaS",
    "outputId": "168182b4-99b1-4902-d96e-789fcb130894"
   },
   "outputs": [],
   "source": [
    "# load packages\n",
    "import scipy.stats as stats\n",
    "w, pvalue = stats.shapiro(model.resid)\n",
    "print(w, pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5yYfMiPwgKU"
   },
   "source": [
    "Again the p-value is 0.7229 which is non-significant and implies we should accept the null hypothesis and the errors come from a normal distribution."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
