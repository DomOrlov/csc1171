{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P4cbs_0HvW0z"
   },
   "source": [
    "# **Linear Regression**\n",
    "\n",
    "In this class we will introduce the concept of regression models, with a particular focus on linear and logistic regression. This class is meant as a recap for those of you who may have struggled with it in previous modules or are new to the topic. We use a simple python examples to show how one would do basic regression analyses.\n",
    "\n",
    "We will get into much more detail about these when we talk about Generalised linear models in MOOC 5.\n",
    "\n",
    "**What is Linear Regression?**\n",
    "\n",
    "In many business research situations the path to decision making lies in understanding the relationship between two or more variables.\n",
    "\n",
    "For example, can the price of airline stock be predicted using a variable like the cost of oil? or how strong a relationship is there between a companyâ€™s sales figures and their advertising budget?\n",
    "\n",
    "Regression Analysis is the process of constructing a mathematical model or function that can be used to predict one variable using another variable (simple linear regression) or variables (multiple linear regression). The basic idea is to create a line of best fit in the data.\n",
    "\n",
    "\n",
    "By linear we are really saying we have a function such as:\n",
    "\n",
    ">$y_{i}=a+bx_{i} + e_{i}$ $ where $ $i=1 $ $to$  $n$\n",
    "\n",
    ">$y_{i}$ is known as the dependent variable and is generally the variable we are trying to predict and $i$ is the row number in the dataset.\n",
    "\n",
    ">$x_{i}$ is the independent variable and is the one we are using as a predictor variable.\n",
    "\n",
    ">$a$ is the intercept and is the expected mean when $x_{i}$ is zero.\n",
    ">$b$ is the slope or the rate of change in $y$ for a unit increase in $x$.\n",
    "\n",
    ">$e_{i}$ is **independently normally distributed with a mean of 0 and a constant variance**.\n",
    "\n",
    "Linear Regression can be considered has one of the most basic forms of machine learning or artifical intelligence. It uses a method called Min Squared Error to come up with the most appropriate values of $a$ and $b$. It does however assume that the error term is independently normaly distributed with a constant variance.\n",
    "\n",
    "From a data mining position we will use it in a number of ways and they are as follows:\n",
    "\n",
    "\n",
    "*   Predict missing values\n",
    "*   Identify important features in a dataset\n",
    "*   Understand if there are hidden features\n",
    "\n",
    "\n",
    "\n",
    "The wonderful thing about linear regression is that we can easily understand the impact independent variables have on the outcome or dependent variable. The draw back is that things can go very wrong if we break the assumtpions such as the normality of the error or the consistency of the error variance. These are important issues and should not be overlooked.\n",
    "\n",
    "It is also worth framing in your mind the idea of $y_{i}=a+bx_{i} + e_{i}$ as a basis for all prediction problems. I appreciate we cannot predict all variables with one input or independent variable or the assumption that the functinal structure is linear, but when we try to solve a problem it is always good to try and ask yourself what are the output/dependent and input/independent variables.\n",
    "\n",
    "\n",
    "\n",
    "**Example**\n",
    "\n",
    "Fedbus have been using radio advertisements to advertise their weekend excursions for the past 8 months. The managing director has asked the sales team to assess the effect of these ads on the number of bookings.\n",
    "\n",
    "The example data for this example can be found [here](https://drive.google.com/file/d/1UVZozqcZdOX8vnM7t1DZurs2yJJpEcE4/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GuFiAcyIhix"
   },
   "source": [
    "Now we are going to use several python libraries to help us with our analysis. They are all imported below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1757872586624,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "JcxiqsfAPj5g"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pandas.testing as tm\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "from statsmodels.compat import lzip\n",
    "import statsmodels.formula.api as sm\n",
    "import statsmodels.stats.stattools as st\n",
    "import statsmodels.stats.api as sms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXbH759TZ9zq"
   },
   "source": [
    "**Note**\n",
    "If you are using google colabs you will need to connect to google drive using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1581,
     "status": "ok",
     "timestamp": 1757873212175,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "WSjz7ZrEX1by",
    "outputId": "16c5a704-ceaa-4f00-f1cd-4c6d37dde590"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "df3 = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Lecture2-CAC1171/Adcampaign.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z79NjCF2Ppjf"
   },
   "source": [
    "Print the data to check that it has imported properly. You should have 8 rows and 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1757873215080,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "qwz3wLQ_x0KB",
    "outputId": "0b164df4-f07d-4c8c-87d3-2033b43b7042"
   },
   "outputs": [],
   "source": [
    "print(df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYui53rOQnz8"
   },
   "source": [
    "If you have managed to import the data then you will now be able to start building the regression model. Initially, we are going to use \"Sklearn\" which is a python machine learning library. The first thing one usually does when completing a linear regression is display the relationship between the independent and the dependent variables. What are we looking for when we display a graph?\n",
    "\n",
    "Generally we are looking for a shape that \"closely\" resembles a line. We can handle \"curved\" shapes but you must be able to apply a linear model to them. If we look at the following model:\n",
    "\n",
    ">$y_{i}=a+bx_{i}^ 2 + e_{i}$ $ where $ $i=1 $ $to$  $n$\n",
    "\n",
    "This is not a linear in shape as it is a quadratic curve but the $b$ parameter is in a linear form and thus regression analysis can be applied. We will get back to this later. Use the following code to plot a scatter plot of the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 283,
     "status": "ok",
     "timestamp": 1757873484594,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "YygVqLSmwMH5",
    "outputId": "3236f565-98aa-4927-c611-e83b652d061f"
   },
   "outputs": [],
   "source": [
    "\n",
    "#!pip install matplotlib # install matplotlib using pip\n",
    "\n",
    "import matplotlib.pyplot\n",
    "#as plt # import the module into the session\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "df3.plot.scatter(x='No. of Adverts (X)', y='No. of bookings (Y)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W3llWvZMWoxK"
   },
   "source": [
    "You can see from the graph that a rough line could actually give a reasonable reflection of the sales process. Its not perfect but consider that we will always have an error in the model, so for this case a line would seem to be a reasonable solution. The error term is designed to \"adjust\" for the variation and one would expect it to vary acording to a random normal distribution. If you go to the following [link](https://drive.google.com/file/d/1yoGR7NLyzD0Zs9FOAyPtqZ1hJis_x3-P/view?usp=sharing) you will find a chapter on simple linear regression by Howard Seltman from CMU. It is worth while reading this as it will give you a flavour of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1757873606029,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "JOa2y9ydvh3H",
    "outputId": "cd692678-66bc-452c-a7e5-87148ff6c519"
   },
   "outputs": [],
   "source": [
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 297,
     "status": "ok",
     "timestamp": 1757873623439,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "1aARRzTP8TB2",
    "outputId": "5b75d563-b307-4d63-cede-a9fcbf70b088"
   },
   "outputs": [],
   "source": [
    "# df3.plot.scatter(x=\"No. of Adverts (X)\", y=\"No. of bookings (Y)\")\n",
    "sns.regplot(x=\"No. of Adverts (X)\", y=\"No. of bookings (Y)\", data=df3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtadvnQvb3_o"
   },
   "source": [
    "So in the code above we have completed a regression plot. You will see a shaded plot around the line. This corresponds to a 95% confidence interval for the regression. This will often be inside the actual points because we are predicting a confidence interval for the mean no of bookings for a particular no of adverts and not the overall no of bookings. This sounds strange but when we implement inferential statistics we always assume that the experiment can be repeated multiple times, and we measure the theoritcal range of the mean for these experiments. If you don't get this now don't worry, and for those who do, maybe you should read about the battle between frequentist and baysian statistics. There is a nice article [here](https://www.theregister.co.uk/2017/06/22/bayesian_vs_frequentist_ai/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gsXn7oT_etgK"
   },
   "source": [
    "So we have now examined the data and feel reasonably confident that a linear regression can be used to model bookings from the number of adverts. You may have read about using a training and test dataset as assessment tool for Machine Learning algorithms. We will get to this later but for now all we want to do is to see if we can model bookings from adverts.\n",
    "\n",
    "In order to do this we need to convert our pandas dataframe to a numpy array. You can see how to do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 55,
     "status": "ok",
     "timestamp": 1757873804759,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "Z31zhBm9JOkE",
    "outputId": "5d650360-8740-41f2-cb5c-d95633367e15"
   },
   "outputs": [],
   "source": [
    "x=df3['No. of Adverts (X)'].to_numpy().reshape((-1, 1))\n",
    "y=df3['No. of bookings (Y)'].to_numpy().reshape((-1, 1))\n",
    "\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XVKKx4mVaAu"
   },
   "source": [
    "Now we will create a model and implement the regressions analysis.\n",
    "From the graph output in the last sns.regplot we can see that the the model looks like a reasonable fit. However, can we get a metric/measure to tells how good a fit we have?\n",
    "\n",
    "Well the answer is Yes & No. There is a term called the coefficent of determination or $R^2$ and this is outlined below. You will note this is ~75%. This is just a single measure and can be misleading but it is a measure than can guide you if your model is going in the right direction. **However, be warned don't expect a high  $R^2$ to give you good predictions.** They don't go hand in hand. We will get to this in a latter session.\n",
    "\n",
    "You will also notice in the code below we print out the intercept ($a$) and the slope coefficent ($b$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1757859828756,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "FUIkOs_DIs0T",
    "outputId": "1f74a4c0-2f2a-4f21-efc6-d8c6d7a64aa2"
   },
   "outputs": [],
   "source": [
    "model =LinearRegression().fit(x,y)\n",
    "\n",
    "r_sq = model.score(x, y)\n",
    "y_pred=model.predict(x)\n",
    "df3['error']=y-y_pred\n",
    "\n",
    "print('coefficient of determination:', r_sq)\n",
    "print('intercept:', model.intercept_)\n",
    "print('coefficent:', model.coef_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIZzxX1uCYKT"
   },
   "source": [
    "Now we have got the $R^2$ and the $a$ and $b $ terms but what do they tell us and can we have confidence in the model?\n",
    "\n",
    "Remember when we do a linear regression we have a number of assumptions and we should check these. The first thing we should check is the independence of the error term. This is important and should be checked in any modelling process we use. If the errors are not independent then we are probably not optimising the model structure or in lay man's language we have left a term out or the variables are in an incorrect functional form.\n",
    "\n",
    "So lets plot the error term against the dependent variable and see what it looks like. The plot can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZx2sI_VfRsS"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1726486674820,
     "user": {
      "displayName": "Andrew Mccarren",
      "userId": "16186536572019350587"
     },
     "user_tz": -60
    },
    "id": "123KKKQRfSfx",
    "outputId": "f4b63991-1f4d-4a7c-887c-31d4a5335832"
   },
   "outputs": [],
   "source": [
    "df3['error']=y_pred-y\n",
    "#print(df3)\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "df3.plot.scatter(x='No. of bookings (Y)', y='error')\n",
    "\n",
    "\n",
    "#sns.regplot('No. of bookings (Y)', # Horizontal axis\n",
    "#           'error', # Vertical axis\n",
    "#           data=df3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5D3qZsWK6Zd"
   },
   "source": [
    "This plot is really interesting as the error terms seem to go from positive to negative as the number of bookings increase. This tells us that the errors may not really be independent and the model is a little suspect. We need to examine this further.\n",
    "\n",
    "One of the drawbacks with sklearn is that it doesn't really give enough detailed statistics on the model assumptions. It is really good at predition but when you make assumptions, such as in our case with a linear regression approach, then we need to get some hard facts about the error distribution and how far the calculated $a$ and $b$ terms are from zero.\n",
    "\n",
    "We are now going to use python's statsmodels library. In the section of code where we import the python libraries we had the following piece of code:\n",
    "\n",
    "> *statsmodels.formula.api as sm*\n",
    "\n",
    "This allows us to import the relevant code from statsmodels and complete a much more detailed linear regression analysis.\n",
    "\n",
    "You should note in the code below that we use a dataframe and you can specify the formula in the following line:\n",
    "\n",
    ">formula_str=\"Y~X\"\n",
    "\n",
    "this says Y is the dependent variable and X is an independent variable.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1757874627962,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "OmIU8Ta9e90S",
    "outputId": "5ac4f2fe-974c-4662-8bc7-22e800bb5a57"
   },
   "outputs": [],
   "source": [
    "\n",
    "df4=df3\n",
    "df4=df4.rename(columns={\"No. of bookings (Y)\":\"Y\",\"No. of Adverts (X)\":\"X\"})\n",
    "formula_str=\"Y~X\"\n",
    "print(df4)\n",
    "result=sm.ols(formula=formula_str,data=df4).fit()\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zN3zjckDVwOU"
   },
   "source": [
    "We have now got a much more extensive analysis to determine if our model works. We won't go into enormous detail here but lets focus on the error term and the paremater estimates.\n",
    "\n",
    "**Error Analysis**\n",
    "\n",
    "The error term has been analysed using a number of statistics in this analysis. The first is the 'Jarque-Bera' statistic. This gives a $Chi^2$ probability value of 0.44, which is not significant ie >0.05. This tells us that there is no evidence to suggest that error term does not follow a normal distribution. A note of warning, many practioneers prefer to use an alternative test such as the Shapiro Wilks test.\n",
    "\n",
    "The next one we look at is the Durbin Watson which has a value of 2.16. Generally, if this lies between 1.5 and 2.5 then we can accept the assumuption that there is no significant auto-correlation between the errors, thus, there is no evidence to suggest that the errors are not  independent.\n",
    "\n",
    "**Model Fit**\n",
    "\n",
    "You will note that the $R^2$ is 0.75. This value will range between 0 and 1. Where 0 reperesents a complete lack of fit and 1 being a perfect fit. Remember, it is not the holy grail of modelling as in theory you will get a perfect fit if you have as many parameters as the number of data points.\n",
    "\n",
    "Generally, we do a few quick checks. The first is to examine the $P>|t| $ for each paremeter. The p value for the intercept ($a$) and slope ($b$) are given as 0.235 and 0.005 respectively. These results tell us that the intercept ($a$) is not significantly different from zero but there is strong evidence that independent $b$ variable is, as the p value < 0.05. This tells us that we could use a model with no intercept. But before we do this we should examine the AIC(55.20)(Akaike Information Criterion) and BIC(55.36)(Bayesian Information Criterion). Both of these terms are very useful as they will penalise large models relative to the information they are attempting to model. If you add more independent variables and the quality of the results does not improve then both measures will increase. So if we remove the intercept term we should derive new AIC and BIC statistics. The results are shown in the code below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1757875538036,
     "user": {
      "displayName": "Maryam Basereh",
      "userId": "17916650843126787012"
     },
     "user_tz": -60
    },
    "id": "dQO89Zd9KulS",
    "outputId": "7022e67b-0b83-42ee-ccde-d3d5605c549a"
   },
   "outputs": [],
   "source": [
    "formula_str=\"Y~0+X\"\n",
    "print(df4)\n",
    "result=sm.ols(formula=formula_str,data=df4).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9vHMSeV6K7xg"
   },
   "source": [
    "You will have noticed that we have changed the following line:\n",
    ">formula_str=\"Y~0+X\"\n",
    "\n",
    "This tells statsmodel to drop the intercept.\n",
    "\n",
    "Now notice how the AIC (55.23) marginally increases and the BIC (55.31) decreases. This suggest that there is no real benifit to the intercept. This is really important to understand as these statistics help is with the selection of variables for our models.\n",
    "\n",
    "**However, this is not the case with the intercept unless we have a genuine belief that the Y variable will be zero when the X variable is Zero. If you don't know this then never drop the intercept.** Also excluding the intercept can create a non-zero mean for the error, which as we noted above is an assumption for regression.\n",
    "\n",
    "So by now you are probably a little confused. The following rules will help you with your model building:\n",
    "\n",
    "\n",
    "\n",
    "*   Graph the errors against the predicted and the actual dependent variable.\n",
    "*   Use a combination of AIC, BIC and p values to make a decision about reducing or increasing the number of independent variables.\n",
    "*   Use the $R^2$ (coefficent of determination) with caution.\n",
    "*   Examine the Durbin Watson statistic to establish if the the errors are independent.\n",
    "\n",
    "\n",
    "Most of you will have asked the following question:\n",
    "> Why not just do a correlation analysis?\n",
    "\n",
    "Well the answer to this question is quite long, but to be as succent as possible I would always say to students that this strategy has the following issues:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*   Correlation is not causation\n",
    "*   Correlation results can be be highly significant when the datasets are large but the actual correlation result can be quite low.\n",
    "*   When we examine datasets with more than 1 independent variable, correlation really doesn't tells us a lot, as independent variables can possess what is known as *Multi-Collinearity*. Multi-Collinearity will mislead investigators as to which variables are important in our analyses.\n",
    "\n",
    "We will dicuss this issue in more detail later as it is extremely important when attempting to assess which variables are going to have a potential influence on our analysis.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
