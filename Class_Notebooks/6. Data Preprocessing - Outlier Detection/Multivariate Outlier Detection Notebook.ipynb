{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7zRfnuRon4S"
   },
   "source": [
    "**Multivariate Outlier Detection**\n",
    "\n",
    "We are now going to look at multivariate outlier detection. What we mean by multivariate outlier detection is the use of more than one variable to identify an outlier. In univariate outlier detection we looked for individual outliers in a single variable. These univariate outliers may not show up if there are unusual combinations between 2 or more variables. This is where multivatriate outlier detiection comes in. The same principles as mentioned  in univariate outlier detection can be applied with multivariate outliers. In otherwords don't exclude particular rows of data unless you scientific evidence to support this.</br>\n",
    "</br>\n",
    "\n",
    "\n",
    "There are numerous approaches to multivariate outlier detection, some of which are outlined below:\n",
    "\n",
    "\n",
    "*   Bivariate Charts\n",
    "> * Based on bivariate Normal assumptions\n",
    "*   Depth Based control charts (nonparameteric)\n",
    "> * Map n-dimensional data to one dimension using for example Mahalanobis\n",
    "> * Build Control Charts for depth\n",
    "\n",
    "* Mutiscale process control wavelets\n",
    "> * Detects abnormalities at multiple scales as large wavelett coefficients\n",
    "> * Useful for data with hetroscedasticity\n",
    "> * Used in chemical process control\n",
    "\n",
    "In this section we will specifically focus on the depth based control charts as it is a reasonably reboust process and is quite quick to set-up. The Multiscale process are beyond this material at this stage.\n",
    "\n",
    "We will need the following three python libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pP_MxWLbPAH"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "housing.DESCR\n",
    "data = housing.data\n",
    "target = housing.target\n",
    "print(housing.feature_names)\n",
    "x = data\n",
    "y = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3T_f_grkk-jf"
   },
   "outputs": [],
   "source": [
    "!pip install pyspc\n",
    "!pip install matplotlib\n",
    "!pip install spm1d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4-0wNZy_U-f"
   },
   "source": [
    "Now lets load the housing housing data that we used in the univariate approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2uhJn5olauR"
   },
   "outputs": [],
   "source": [
    "#from sklearn.datasets import load_housing\n",
    "\n",
    "\n",
    "from sklearn.metrics import DistanceMetric as DM\n",
    "import pandas as pd\n",
    "housing = data\n",
    "\n",
    "\n",
    "x = data\n",
    "y = target\n",
    "#create the dataframe\n",
    "housing_df = pd.DataFrame(housing)\n",
    "#housing_df.columns = ['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT']\n",
    "#housing_df['Y']=pd.Series(y)\n",
    "housing_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVV2EqCfBTVB"
   },
   "source": [
    "We need to be able to calcualte multivariate distances to tells us which points are outliers. This is where were the library [*scipy.spatial*](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.mahalanobis.html)  comes in. This library calculates the Mahalanobis distance and then compares this to values for a hotellings t squared distribution to deterimine if they are outliers. Its like comparing a single univariate variable to a t-distribution. Have a look at this [link](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution) to understand this better. The code below is an example for the housing dataset of how to calculate the critical Hotellings T Squared value for the dataset. So any values whoes distance is greater than our critical value will be considered outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "humz-mNcx7nY"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "x=housing_df.values.tolist()\n",
    "#print(x))\n",
    "n=len(x)\n",
    "k=len(x[0])\n",
    "vals = f.ppf([0.99], n, k)*(n*k)/((n-k)+1)\n",
    "print(vals)\n",
    "rv = f(len(x), len(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4VoYS7VHPbh"
   },
   "source": [
    "The final piece of code tries to put all this together. We have found our crtical values by calcualting the Hotellings T-Squared from n rows with k columns. We then calcualte the Mahalanobis distance for each row from the overall multivariate mean $\\bar{X}$ (xbar). Use this distance with the covariance matrix to calculate individual Hotelling T Squared statistics and  then compare each one to the critical Hotellings T Squared values. You will notice that we have used the covariance matrix to helps us standarised the data using *pandas.cov*. There will be small differences between the chart above and the one below as we do not have the exact calculations from *pyspc* but by using the code below we can identify the outlier points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zRQw-mgAaQjn"
   },
   "outputs": [],
   "source": [
    "from  scipy.spatial import distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f\n",
    "import pandas as pd\n",
    "#print(housing_df.cov())\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "#print(housing)\n",
    "print(housing.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IZPE7l-0Xr0C"
   },
   "outputs": [],
   "source": [
    "from  scipy.spatial import distance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f\n",
    "#print(housing_df.cov())\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "print(housing)\n",
    "housing.DESCR\n",
    "data = housing.data\n",
    "target = housing.target\n",
    "print(housing.feature_names)\n",
    "x = data\n",
    "y = target\n",
    "\n",
    "inv_cov = pd.DataFrame(np.linalg.pinv(housing_df.cov().values), housing_df.cov().columns, housing_df.cov().index).values.tolist()\n",
    "x=housing_df.values.tolist()\n",
    "xbar=housing_df.mean().tolist()\n",
    "\n",
    "n=len(x)\n",
    "k=len(x[0])\n",
    "## Convert f value to hotellings critical value at 1% level\n",
    "##https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution\n",
    "\n",
    "Hotvalue = f.ppf([0.99], n, k)*(n*k)/((n-k)+1)\n",
    "print(Hotvalue)\n",
    "\n",
    "## calculating hoteling test statistic for each row in housing dataset\n",
    "\n",
    "for i in range(0,len(x)):\n",
    "\n",
    "  housing_df.loc[housing_df.index==i,'hotelling']=(n*k)*distance.mahalanobis(x[i], xbar, inv_cov)**2/(k*(n-k))\n",
    "  housing_df.loc[housing_df.index==i,'critical value']=Hotvalue\n",
    "\n",
    "x=np.array(housing_df.index.tolist())\n",
    "y1=np.array(housing_df['hotelling'])\n",
    "f = plt.figure()\n",
    "\n",
    "ax = f.add_subplot(111)\n",
    "\n",
    "plt.plot(x, y1)\n",
    "plt.axhline(y=housing_df['hotelling'].mean())\n",
    "plt.axhline(y=Hotvalue,color='r')\n",
    "#plt.axhline(y=housing_df['hotelling'].mean()+3*housing_df['hotelling'].std(),color='r')\n",
    "#plt.axhline(y=housing_df['hotelling'].mean()-3*housing_df['hotelling'].std(),color='r')\n",
    "\n",
    "plt.title('Multivariate Charts  ', fontsize=8)\n",
    "print(housing_df.loc[housing_df['hotelling']>housing_df['critical value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZH_8W7KJ6pZ"
   },
   "source": [
    "You should be able to use this depth based approach to help you identify multivariate outliers. It is worth looking at this [article](https://onlinelibrary.wiley.com/doi/full/10.1002/cem.2763) from Richard Brereton if you want more detail. The code I have developed above is based on this paper.</br></br>\n",
    "Finally, play with the data and test out for yourself if you can find or identify new outliers that you may create. Share your conclusions amongst the class.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
