{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzYUMbmtAQLb"
   },
   "source": [
    "Enrico part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KsBHn8NCEme"
   },
   "outputs": [],
   "source": [
    "!pip install pandas-datareader --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22dr98kW_CJq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from pandas_datareader.data import DataReader\n",
    "from scipy.stats import chi2\n",
    "from datetime import timedelta\n",
    "\n",
    "# read for market info file\n",
    "market_info = pd.read_csv(\"clean_company_listings.csv\")\n",
    "market_info.columns = market_info.columns.str.strip()\n",
    "market_currency_map = {\n",
    "    'NASDAQ': 'USD',\n",
    "    'NYSE': 'USD',\n",
    "    'London Stock Exchange': 'GBP',\n",
    "    'LSE': 'GBP',\n",
    "    'Tokyo Stock Exchange': 'JPY',\n",
    "    'TSE': 'JPY',\n",
    "    'Shanghai Stock Exchange': 'CNY',\n",
    "    'SSE': 'CNY',\n",
    "    'Hong Kong Stock Exchange': 'HKD',\n",
    "    'HKEX': 'HKD',\n",
    "    'Toronto Stock Exchange': 'CAD',\n",
    "    'TSX': 'CAD',\n",
    "    'Australian Securities Exchange': 'AUD',\n",
    "    'ASX': 'AUD',\n",
    "    'Euronext': 'EUR',\n",
    "    'Frankfurt Stock Exchange': 'EUR',\n",
    "    'FWB': 'EUR',}\n",
    "\n",
    "market_info['Currency'] = market_info['stock offering'].map(market_currency_map)\n",
    "market_info['Currency'] = market_info['Currency'].fillna('Unknown')\n",
    "print(market_info.head())\n",
    "\n",
    "# adjust to USD\n",
    "def get_exchange_rate(currency, start_date, end_date):\n",
    "    \"\"\"Get daily exchange rate to USD.\"\"\"\n",
    "    if currency.upper() == 'USD':\n",
    "        df = pd.DataFrame({\n",
    "            'Date': pd.date_range(start=start_date, end=end_date),\n",
    "            'Rate_to_USD': 1.0\n",
    "        })\n",
    "        return df\n",
    "    fx_map = {\n",
    "        'EUR': ('EURUSD=X', 1),\n",
    "        'GBP': ('GBPUSD=X', 1),\n",
    "        'AUD': ('AUDUSD=X', 1),\n",
    "        'NZD': ('NZDUSD=X', 1),\n",
    "        'CAD': ('USDCAD=X', -1),\n",
    "        'JPY': ('USDJPY=X', -1),\n",
    "        'CHF': ('USDCHF=X', -1),\n",
    "        'CNY': ('USDCNY=X', -1),\n",
    "        'HKD': ('USDHKD=X', -1),\n",
    "        'SEK': ('USDSEK=X', -1),\n",
    "        'NOK': ('USDNOK=X', -1),\n",
    "        'SGD': ('USDSGD=X', -1)}\n",
    "\n",
    "    cur = currency.upper()\n",
    "    if cur in fx_map:\n",
    "        symbol, invert = fx_map[cur]\n",
    "    else:\n",
    "        symbol, invert = (f\"{cur}USD=X\", 1)  # fallback\n",
    "\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)[['Close']].reset_index()\n",
    "    data = data.rename(columns={'Close': 'Rate_to_USD'})\n",
    "\n",
    "    if invert == -1:\n",
    "        data['Rate_to_USD'] = 1 / data['Rate_to_USD']\n",
    "\n",
    "    return data[['Date', 'Rate_to_USD']]\n",
    "\n",
    "\n",
    "def convert_to_usd(df, currency, exchange_rates):\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "    exchange_rates['Date'] = pd.to_datetime(exchange_rates['Date']).dt.tz_localize(None)\n",
    "    merged = df.merge(exchange_rates, on='Date', how='left')\n",
    "    merged['Value_USD'] = merged['Value'] * merged['Rate_to_USD']\n",
    "    return merged[['Date','Value_USD']]\n",
    "\n",
    "# adjust for inflation\n",
    "def get_inflation_index(start_date, end_date):\n",
    "    \"\"\"Fetch daily interpolated US CPI from FRED and clean NaNs.\"\"\"\n",
    "    try:\n",
    "        cpi = DataReader('CPIAUCNS', 'fred', start_date - pd.DateOffset(days=31), end_date)\n",
    "    except Exception as e:\n",
    "        print(\"FRED CPI fetch failed:\", e)\n",
    "        return pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date), 'CPI_USD': np.nan})\n",
    "\n",
    "    cpi = cpi.reset_index().rename(columns={'DATE': 'Date', 'CPIAUCNS': 'CPI_USD'})\n",
    "    cpi['Date'] = pd.to_datetime(cpi['Date'])\n",
    "\n",
    "    daily_index = pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date)})\n",
    "\n",
    "    daily_index = pd.merge_asof(daily_index.sort_values('Date'),\n",
    "                                cpi.sort_values('Date'),\n",
    "                                on='Date',\n",
    "                                direction='backward')\n",
    "\n",
    "    daily_index['CPI_USD'] = daily_index['CPI_USD'].ffill().bfill().interpolate(method='linear')\n",
    "\n",
    "    # Drop if still NaN\n",
    "    if daily_index['CPI_USD'].isna().all():\n",
    "        print(\"All CPI_USD values are NaN. Inflation adjustment skipped.\")\n",
    "    return daily_index\n",
    "\n",
    "\n",
    "def adjust_for_inflation(df, inflation_index):\n",
    "    if inflation_index['CPI_USD'].isna().all():\n",
    "        df['Value_Adj_USD'] = df['Value_USD']\n",
    "        return df[['Date','Value_Adj_USD']]\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "    inflation_index['Date'] = pd.to_datetime(inflation_index['Date']).dt.tz_localize(None)\n",
    "    merged = df.merge(inflation_index, on='Date', how='left')\n",
    "\n",
    "    # Normalize to today CPI\n",
    "    latest_cpi = inflation_index['CPI_USD'].iloc[-1]\n",
    "    merged['Value_Adj_USD'] = merged['Value_USD'] * (latest_cpi / merged['CPI_USD'])\n",
    "    return merged[['Date','Value_Adj_USD']]\n",
    "\n",
    "\n",
    "\n",
    "# chi^2 test and systemic dislocation\n",
    "def chi2_systemic_dislocation(df, bins=10):\n",
    "    from scipy.stats import norm\n",
    "\n",
    "    df['Return'] = df['Value_Adj_USD'].pct_change()\n",
    "    df['Dislocation'] = (df['Return'] - df['Return'].mean()) / df['Return'].std()\n",
    "\n",
    "    disloc = df['Dislocation'].dropna()\n",
    "\n",
    "    quantiles = np.linspace(0, 1, bins + 1)\n",
    "    bin_edges = norm.ppf(quantiles)\n",
    "\n",
    "    observed, _ = np.histogram(disloc, bins=bin_edges)\n",
    "    expected = np.ones_like(observed) * (len(disloc) / bins)\n",
    "\n",
    "    chi2_stat = ((observed - expected)**2 / expected).sum()\n",
    "    p_val = 1 - chi2.cdf(chi2_stat, df=bins - 1)\n",
    "\n",
    "    return df, chi2_stat, p_val\n",
    "\n",
    "# processing function\n",
    "def process_asset_file(asset_file):\n",
    "    df = pd.read_csv(asset_file)\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df['Value'] = df['Adj Close']\n",
    "\n",
    "    asset_name = asset_file.split('/')[-1].replace('.csv','')\n",
    "    currency = market_info.loc[market_info['symbol (acronym)'] == asset_name, 'Currency'].values[0]\n",
    "\n",
    "    start_date = df['Date'].min() - timedelta(days=1)\n",
    "    end_date = df['Date'].max() + timedelta(days=1)\n",
    "\n",
    "    exchange_rates = get_exchange_rate(currency, start_date, end_date)\n",
    "    inflation_index = get_inflation_index(start_date, end_date)\n",
    "\n",
    "    df_usd = convert_to_usd(df, currency, exchange_rates)\n",
    "    df_real = adjust_for_inflation(df_usd, inflation_index)\n",
    "\n",
    "    df_chi, chi2_stat, p_val = chi2_systemic_dislocation(df_real)\n",
    "    return df_chi, chi2_stat, p_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5EdrA7fAN41"
   },
   "source": [
    "Domo part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8QyFjqOATPh"
   },
   "outputs": [],
   "source": [
    "import os, re, pandas as pd, numpy as np\n",
    "from pandas.tseries.holiday import (\n",
    "    AbstractHolidayCalendar, Holiday, nearest_workday, USFederalHolidayCalendar,\n",
    "    GoodFriday, EasterMonday\n",
    ")\n",
    "from pandas.tseries.offsets import CustomBusinessDay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1RBUFyXATBo"
   },
   "outputs": [],
   "source": [
    "# builds: company name, symbol (acronym), date of listing, stock offering\n",
    "# writes: ~/csc1171/data/raw/amex_nyse_nasdaq_stock_histories/clean_company_listings.csv\n",
    "\n",
    "dataset_dir = os.path.expanduser(\"~/csc1171/data/raw/AMEX_NYSE_NASDAQ_stock_histories\")\n",
    "symbols_file = os.path.join(dataset_dir, \"all_symbols.txt\")\n",
    "big_file     = os.path.join(dataset_dir, \"fh_5yrs.csv\")\n",
    "output_file  = os.path.expanduser(\"~/csc1171/notebooks/Dominik/clean_company_listings.csv\")\n",
    "full_dir = os.path.join(dataset_dir, \"full_history\")\n",
    "\n",
    "\n",
    "with open(symbols_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    symbols = [ln.strip().upper() for ln in f if ln.strip() and not ln.startswith(\"#\")]\n",
    "symset = set(symbols)\n",
    "use_full = os.path.isdir(full_dir)\n",
    "\n",
    "# build lookup maps from nasdaq symbol directory files (if present)\n",
    "nasdaq_file = os.path.join(dataset_dir, \"nasdaqlisted.txt\")\n",
    "other_file  = os.path.join(dataset_dir, \"otherlisted.txt\")\n",
    "\n",
    "name_map, exch_map = {}, {}\n",
    "\n",
    "if os.path.isfile(nasdaq_file):\n",
    "    nl = pd.read_csv(nasdaq_file, sep=\"|\")\n",
    "    if \"Symbol\" in nl.columns and \"Security Name\" in nl.columns:\n",
    "        nl = nl[nl[\"Symbol\"] != \"File Creation Time\"]\n",
    "        for _, r in nl.iterrows():\n",
    "            s = str(r[\"Symbol\"]).upper()\n",
    "            name_map[s] = str(r[\"Security Name\"]).strip()\n",
    "            exch_map[s] = \"NASDAQ\"\n",
    "\n",
    "if os.path.isfile(other_file):\n",
    "    ol = pd.read_csv(other_file, sep=\"|\")\n",
    "    col = {c.lower(): c for c in ol.columns}\n",
    "    symcol  = col.get(\"act symbol\") or col.get(\"symbol\")\n",
    "    namecol = col.get(\"security name\") or col.get(\"name\")\n",
    "    excol   = col.get(\"exchange\")\n",
    "    if symcol and namecol and excol:\n",
    "        ol = ol[ol[symcol] != \"File Creation Time\"]\n",
    "        # optional: map single-letter exchange codes to full names\n",
    "        xmap = {\"A\": \"AMEX\", \"N\": \"NYSE\", \"P\": \"NYSE ARCA\", \"Q\": \"NASDAQ\", \"Z\": \"BATS\"}\n",
    "        for _, r in ol.iterrows():\n",
    "            s = str(r[symcol]).upper()\n",
    "            name_map.setdefault(s, str(r[namecol]).strip())\n",
    "            exch_raw = str(r[excol]).strip().upper()\n",
    "            exch_map.setdefault(s, xmap.get(exch_raw, exch_raw))\n",
    "\n",
    "\n",
    "def detect_cols(df):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    s = cols.get(\"symbol\") or cols.get(\"ticker\")\n",
    "    d = cols.get(\"date\") or cols.get(\"timestamp\")\n",
    "    e = cols.get(\"exchange\")\n",
    "    n = cols.get(\"name\") or cols.get(\"security name\") or cols.get(\"company name\")\n",
    "    return s, d, e, n\n",
    "\n",
    "min_date  = {}\n",
    "first_ex  = {}\n",
    "first_name = {}\n",
    "\n",
    "probe = pd.read_csv(big_file, nrows=5)\n",
    "scol, dcol, ecol, ncol = detect_cols(probe)\n",
    "\n",
    "if use_full:\n",
    "    # per-symbol mode: compute earliest date from full_history/<symbol>.csv\n",
    "    for s in symbols:\n",
    "        p = os.path.join(full_dir, f\"{s}.csv\")\n",
    "        if os.path.isfile(p):\n",
    "            hdr = pd.read_csv(p, nrows=1)\n",
    "            dcol = next((c for c in hdr.columns if c.lower() == \"date\"), None)\n",
    "            if dcol:\n",
    "                d = pd.read_csv(p, usecols=[dcol])[dcol]\n",
    "                d = pd.to_datetime(d, errors=\"coerce\").dropna()\n",
    "                if len(d):\n",
    "                    min_date[s] = d.min()\n",
    "\n",
    "    # note: full_history files do not include company name or exchange, so\n",
    "    # first_name and first_ex will remain blank unless you join another metadata file.\n",
    "else:\n",
    "    # fallback to big combined file fh_5yrs.csv (your original logic)\n",
    "    print(\"using fall back\")\n",
    "    probe = pd.read_csv(big_file, nrows=5)\n",
    "    scol, dcol, ecol, ncol = detect_cols(probe)\n",
    "\n",
    "    for chunk in pd.read_csv(big_file, usecols=[c for c in [scol, dcol, ecol, ncol] if c], chunksize=500000):\n",
    "        chunk[scol] = chunk[scol].astype(str).str.upper()\n",
    "        chunk = chunk[chunk[scol].isin(symset)]\n",
    "        if not len(chunk):\n",
    "            continue\n",
    "        dt = pd.to_datetime(chunk[dcol], errors=\"coerce\")\n",
    "        chunk = chunk.loc[dt.notna()].copy()\n",
    "        chunk[\"_dt\"] = dt[dt.notna()]\n",
    "        mins = chunk.groupby(scol)[\"_dt\"].min()\n",
    "        for s, d in mins.items():\n",
    "            if (s not in min_date) or (d < min_date[s]):\n",
    "                min_date[s] = d\n",
    "        if ecol:\n",
    "            for s, ex in chunk[[scol, ecol]].dropna().drop_duplicates(subset=[scol]).itertuples(index=False):\n",
    "                if s not in first_ex:\n",
    "                    first_ex[s] = str(ex).upper()\n",
    "        if ncol:\n",
    "            for s, nm in chunk[[scol, ncol]].dropna().drop_duplicates(subset=[scol]).itertuples(index=False):\n",
    "                if s not in first_name:\n",
    "                    first_name[s] = str(nm).strip()\n",
    "\n",
    "\n",
    "rows = []\n",
    "# for s in symbols:\n",
    "#     rows.append({\n",
    "#         \"company name\": first_name.get(s, name_map.get(s, \"\")),\n",
    "#         \"symbol (acronym)\": s,\n",
    "#         \"date of listing\": (min_date.get(s).date().isoformat() if s in min_date else \"\"),\n",
    "#         \"stock offering\": first_ex.get(s, exch_map.get(s, \"\")),\n",
    "#     })\n",
    "for s in symbols:\n",
    "    s_raw = s\n",
    "    alts = {s_raw, s_raw.replace('.', '-'), s_raw.replace('-', '.')}\n",
    "    rows.append({\n",
    "        \"company name\": (first_name.get(s) or next((name_map[a] for a in alts if a in name_map), \"\")),\n",
    "        \"symbol (acronym)\": s,\n",
    "        \"date of listing\": (min_date.get(s).date().isoformat() if s in min_date else \"\"),\n",
    "        \"stock offering\": (first_ex.get(s) or next((exch_map[a] for a in alts if a in exch_map), \"\")),\n",
    "    })\n",
    "\n",
    "pd.DataFrame(rows, columns=[\"company name\",\"symbol (acronym)\",\"date of listing\",\"stock offering\"]).to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xed2EYejAaMU"
   },
   "outputs": [],
   "source": [
    "# Fix names and dtypes\n",
    "\n",
    "# --- config you can tweak ---\n",
    "STANDARD_COLS = {\n",
    "    \"open\":\"open\",\"o\":\"open\",\n",
    "    \"high\":\"high\",\"h\":\"high\",\n",
    "    \"low\":\"low\",\"l\":\"low\",\n",
    "    \"close\":\"close\",\"c\":\"close\",\"last\":\"close\",\"adjclose\":\"adj_close\",\"adj_close\":\"adj_close\",\"adjusted_close\":\"adj_close\",\n",
    "    \"volume\":\"volume\",\"vol\":\"volume\",\"shares_traded\":\"volume\",\n",
    "    \"date\":\"date\",\"timestamp\":\"date\",\"time\":\"date\",\"trade_date\":\"date\",\n",
    "    \"symbol\":\"symbol\",\"ticker\":\"symbol\",\n",
    "    \"exchange\":\"exchange\",\"market\":\"exchange\",\"stock_offering\":\"exchange\",\n",
    "    \"currency\":\"currency\"\n",
    "}\n",
    "\n",
    "STANDARD_COLS.update({\n",
    "    \"adj_close\":\"adj_close\",      # already there, keeps idempotent\n",
    "    \"adj_close_\":\"adj_close\",     # some dumps trail underscores\n",
    "    \"adj__close\":\"adj_close\",     # double underscores after snake\n",
    "})\n",
    "\n",
    "PRICE_COLS = [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]\n",
    "INT_COLS   = [\"volume\"]\n",
    "CAT_COLS   = [\"exchange\",\"currency\"]\n",
    "REQ_DATE   = \"date\"\n",
    "REQ_SYMBOL = \"symbol\"\n",
    "\n",
    "def _snake(s:str)->str:\n",
    "    s = re.sub(r\"[^\\w]+\",\"_\", s.strip().lower())\n",
    "    s = re.sub(r\"_+\",\"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def normalize_columns(df:pd.DataFrame)->pd.DataFrame:\n",
    "    colmap = {}\n",
    "    for c in df.columns:\n",
    "        k = _snake(c)\n",
    "        k = STANDARD_COLS.get(k, k)\n",
    "        colmap[c] = k\n",
    "    df = df.rename(columns=colmap)\n",
    "    # collapse duplicates after renaming (keep first non-null)\n",
    "    if len(set(df.columns)) != len(df.columns):\n",
    "        agg = {}\n",
    "        for c in df.columns:\n",
    "            if c not in agg: agg[c] = lambda x: x.bfill().ffill().iloc[0] if hasattr(x,\"bfill\") else x\n",
    "        df = df.groupby(axis=1, level=0).first()\n",
    "    return df\n",
    "\n",
    "# def parse_date_col(df:pd.DataFrame)->pd.DataFrame:\n",
    "#     if REQ_DATE not in df.columns: return df\n",
    "#     # coerce strings like 'YYYY-MM-DD', 'MM/DD/YYYY', 'YYYY-MM-DD HH:MM:SS', etc.\n",
    "#     dt = pd.to_datetime(df[REQ_DATE], errors=\"coerce\", utc=True)\n",
    "#     # if parsed tz-naive, convert to UTC; if already tz-aware, keep UTC\n",
    "#     if dt.dtype == \"datetime64[ns, UTC]\":\n",
    "#         df[REQ_DATE] = dt\n",
    "#     else:\n",
    "#         df[REQ_DATE] = pd.to_datetime(df[REQ_DATE], errors=\"coerce\").dt.tz_localize(\"UTC\")\n",
    "#     return df\n",
    "\n",
    "def parse_date_col(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if REQ_DATE not in df.columns:\n",
    "        return df\n",
    "    dt = pd.to_datetime(df[REQ_DATE], errors=\"coerce\", utc=False)\n",
    "    # If tz-aware, convert to UTC; if naive, localize to UTC.\n",
    "    if getattr(dt.dt, \"tz\", None) is not None:\n",
    "        dt = dt.dt.tz_convert(\"UTC\")\n",
    "    else:\n",
    "        dt = dt.dt.tz_localize(\"UTC\")\n",
    "    df[REQ_DATE] = dt\n",
    "    return df\n",
    "\n",
    "\n",
    "def coerce_numeric(df:pd.DataFrame)->pd.DataFrame:\n",
    "    for c in PRICE_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = (df[c].astype(str).str.replace(\",\",\"\",regex=False)\n",
    "                              .str.replace(\" \",\"\",regex=False)\n",
    "                              .str.replace(\"$\",\"\",regex=False)\n",
    "                              .replace([\"\", \"nan\",\"None\"], np.nan)\n",
    "                              .astype(float))\n",
    "    for c in INT_COLS:\n",
    "        if c in df.columns:\n",
    "            ser = df[c].astype(str).str.replace(\",\",\"\",regex=False).str.replace(\" \",\"\",regex=False)\n",
    "            ser = ser.replace([\"\", \"nan\",\"None\"], np.nan)\n",
    "            # allow floats that should be ints (e.g., '1234.0')\n",
    "            ser = pd.to_numeric(ser, errors=\"coerce\")\n",
    "            if ser.notna().any() and (ser.dropna() % 1 == 0).all():\n",
    "                df[c] = ser.astype(\"Int64\")\n",
    "            else:\n",
    "                df[c] = ser.astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "def tidy_symbol(df:pd.DataFrame, symbol_hint:str|None=None)->pd.DataFrame:\n",
    "    if REQ_SYMBOL not in df.columns and symbol_hint:\n",
    "        df[REQ_SYMBOL] = symbol_hint\n",
    "    if REQ_SYMBOL in df.columns:\n",
    "        df[REQ_SYMBOL] = (df[REQ_SYMBOL].astype(str).str.upper().str.strip()\n",
    "                          .str.replace(\"\\u200b\",\"\",regex=False))\n",
    "        df = df[df[REQ_SYMBOL]!=\"\"]\n",
    "    return df\n",
    "\n",
    "def cat_types(df:pd.DataFrame)->pd.DataFrame:\n",
    "    for c in CAT_COLS:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "            df.loc[df[c].isin([\"\", \"nan\",\"None\"]), c] = np.nan\n",
    "            df[c] = df[c].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "def finalize_schema(df:pd.DataFrame)->pd.DataFrame:\n",
    "    # Ensure all price cols exist even if missing in source\n",
    "    for c in PRICE_COLS:\n",
    "        if c not in df.columns: df[c] = np.nan\n",
    "    for c in INT_COLS:\n",
    "        if c not in df.columns: df[c] = pd.Series(pd.array([pd.NA]*len(df), dtype=\"Int64\"))\n",
    "    # If adj_close was absent and we only have raw close, you can mirror it (optional)\n",
    "    if \"adj_close\" in df.columns and df[\"adj_close\"].isna().all() and \"close\" in df.columns:\n",
    "        df[\"adj_close\"] = df[\"close\"].astype(\"float64\")\n",
    "    # Required cols\n",
    "    if REQ_DATE in df.columns:\n",
    "        df = df[df[REQ_DATE].notna()]\n",
    "    if REQ_SYMBOL in df.columns:\n",
    "        df = df[df[REQ_SYMBOL].notna() & (df[REQ_SYMBOL]!=\"\")]\n",
    "    # Sort, drop dupes\n",
    "    keep_order = [x for x in [REQ_DATE,REQ_SYMBOL,\"exchange\",\"currency\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"] if x in df.columns]\n",
    "    others = [c for c in df.columns if c not in keep_order]\n",
    "    df = df[keep_order+others]\n",
    "    df = df.drop_duplicates(subset=[c for c in [REQ_DATE, REQ_SYMBOL] if c in df.columns]).sort_values(by=[c for c in [REQ_DATE, REQ_SYMBOL] if c in df.columns])\n",
    "    # Assert dtypes\n",
    "    for c in PRICE_COLS:\n",
    "        if c in df.columns: df[c] = df[c].astype(\"float64\")\n",
    "    if \"volume\" in df.columns: df[\"volume\"] = df[\"volume\"].astype(\"Int64\")\n",
    "    # Force exact end-state dtypes for your contract\n",
    "    if \"symbol\" in df.columns: df[\"symbol\"] = df[\"symbol\"].astype(\"object\")\n",
    "    if \"source\" in df.columns: df[\"source\"] = df[\"source\"].astype(\"object\")\n",
    "    if \"date\"   in df.columns and df[\"date\"].dtype.tz is None:\n",
    "        df[\"date\"] = df[\"date\"].dt.tz_localize(\"UTC\")\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# --- returns-matrix (EDHEC) support -----------------------------------------\n",
    "\n",
    "def looks_like_returns_matrix(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristic: has 'date' but no OHLC columns, and >3 other columns (strategies).\n",
    "    Good for HF.csv (EDHEC) style monthly returns panel.\n",
    "    \"\"\"\n",
    "    cols = set(df.columns.str.lower())\n",
    "    has_date = \"date\" in cols\n",
    "    has_ohlc = bool(cols & {\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"symbol\"})\n",
    "    many_wide_cols = (len(cols) >= 5)  # date + many strategy columns\n",
    "    return has_date and (not has_ohlc) and many_wide_cols\n",
    "\n",
    "def clean_edhec_returns(df_raw: pd.DataFrame, source: str = \"EDHEC\") -> pd.DataFrame:\n",
    "    # normalize header but keep strategy names (don’t map to STANDARD_COLS)\n",
    "    # df = df_raw.rename(columns={c: c.strip() for c in df_raw.columns})\n",
    "    df = df_raw.rename(columns={str(c): str(c).strip() for c in df_raw.columns})\n",
    "    # Parse date to UTC\n",
    "    df = df.rename(columns={\"date\":\"date\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\", utc=True)\n",
    "    df = df[df[\"date\"].notna()].copy()\n",
    "\n",
    "    # Wide -> long: one row per (date, strategy)\n",
    "    value_cols = [c for c in df.columns if c != \"date\"]\n",
    "    df = df.melt(id_vars=[\"date\"], value_vars=value_cols,\n",
    "                 var_name=\"symbol\", value_name=\"ret\")\n",
    "\n",
    "    # Tidy symbol names: dots -> underscores, upper-case for consistency\n",
    "    df[\"symbol\"] = (df[\"symbol\"].astype(str)\n",
    "                    .str.replace(\".\", \"_\", regex=False)\n",
    "                    .str.strip().str.upper())\n",
    "\n",
    "    # Coerce returns to float\n",
    "    df[\"ret\"] = pd.to_numeric(df[\"ret\"], errors=\"coerce\").astype(\"float64\")\n",
    "    df = df.dropna(subset=[\"symbol\",\"ret\"]).sort_values([\"symbol\",\"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # Attach source, enforce dtypes\n",
    "    df[\"source\"] = source\n",
    "    df[\"symbol\"] = df[\"symbol\"].astype(\"object\")\n",
    "    df[\"source\"] = df[\"source\"].astype(\"object\")\n",
    "    # Ensure tz-aware UTC (pd.to_datetime(utc=True) already does, but belt & braces)\n",
    "    if getattr(df[\"date\"].dt, \"tz\", None) is None:\n",
    "        df[\"date\"] = df[\"date\"].dt.tz_localize(\"UTC\")\n",
    "    return df[[\"date\",\"symbol\",\"ret\",\"source\"]]\n",
    "\n",
    "def clean_any(df: pd.DataFrame, source: str, symbol_hint: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Unified entry point:\n",
    "    - EDHEC-style returns matrix -> returns schema: [date, symbol, ret, source]\n",
    "    - Otherwise -> OHLCV schema via your clean_names_and_dtypes()\n",
    "    \"\"\"\n",
    "    # Quick peek without mutating user’s df\n",
    "    cols_lower = set(map(str.lower, df.columns))\n",
    "    if looks_like_returns_matrix(df.rename(columns=str.lower)):\n",
    "        return clean_edhec_returns(df, source=source)\n",
    "    # Otherwise: OHLCV path\n",
    "    return clean_names_and_dtypes(df, source=source, symbol_hint=symbol_hint)\n",
    "\n",
    "def clean_names_and_dtypes(df:pd.DataFrame, source:str, symbol_hint:str|None=None)->pd.DataFrame:\n",
    "    df = normalize_columns(df)\n",
    "    df = tidy_symbol(df, symbol_hint)\n",
    "    df = parse_date_col(df)\n",
    "    df = coerce_numeric(df)\n",
    "    df = cat_types(df)\n",
    "    df[\"source\"] = source\n",
    "    return finalize_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Hu8gkNgAhCD"
   },
   "outputs": [],
   "source": [
    "# Test names dtypes\n",
    "\n",
    "# # 1) Load\n",
    "# p = os.path.expanduser(\"~/csc1171/data/raw/SP500_ETF_FX_Crypto_Daily/AAPL.csv\")\n",
    "# df_raw = pd.read_csv(p)\n",
    "\n",
    "# # 2) Clean\n",
    "# df_clean = clean_names_and_dtypes(df_raw, source=\"Yahoo\", symbol_hint=\"AAPL\")\n",
    "\n",
    "# # 3) Inspect schema & a few rows\n",
    "# print(df_clean.dtypes)\n",
    "# print(df_clean.head(3))\n",
    "# print(df_clean[['date','symbol']].isna().sum())\n",
    "# print(df_clean[['open','high','low','close','adj_close','volume']].describe())\n",
    "\n",
    "# p = \"~/csc1171/data/raw/Global_Stock_Market_2008-2023/2008_Globla_Markets_Data.csv\"\n",
    "# df = pd.read_csv(os.path.expanduser(p))\n",
    "# df = clean_names_and_dtypes(df, source=\"Global\")\n",
    "# print(df.dtypes)  # should match your contract exactly\n",
    "\n",
    "# folder = os.path.expanduser(\"~/csc1171/data/raw/SP500_ETF_FX_Crypto_Daily\")\n",
    "# fn = \"JSM.csv\"\n",
    "# df = pd.read_csv(os.path.join(folder, fn))\n",
    "# df = clean_names_and_dtypes(df, source=\"Yahoo\", symbol_hint=os.path.splitext(fn)[0].upper())\n",
    "# print(df.dtypes)\n",
    "\n",
    "# p = \"~/csc1171/data/raw/AMEX_NYSE_NASDAQ_stock_histories/fh_5yrs.csv\"\n",
    "# df = pd.read_csv(os.path.expanduser(p))\n",
    "# df = clean_names_and_dtypes(df, source=\"FH5\")\n",
    "# print(df.dtypes)\n",
    "\n",
    "# import os, pandas as pd\n",
    "\n",
    "# # make wide prints readable\n",
    "# pd.set_option(\"display.width\", None)\n",
    "# pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# def preview(df: pd.DataFrame, name: str):\n",
    "#     print(f\"\\n=== {name} ===\")\n",
    "#     print(\"shape:\", df.shape)\n",
    "#     print(\"dtypes:\\n\", df.dtypes.to_string())\n",
    "#     print(\"\\nhead(10):\")\n",
    "#     print(df.head(10))\n",
    "\n",
    "# # Global (prices)\n",
    "# p = os.path.expanduser(\"~/csc1171/data/raw/Global_Stock_Market_2008-2023/2008_Globla_Markets_Data.csv\")\n",
    "# df = pd.read_csv(p)\n",
    "# df = clean_any(df, source=\"Global\")\n",
    "# preview(df, \"Global\")\n",
    "\n",
    "# # Yahoo single-ticker (prices)\n",
    "# folder = os.path.expanduser(\"~/csc1171/data/raw/SP500_ETF_FX_Crypto_Daily\")\n",
    "# fn = \"JSM.csv\"\n",
    "# df = pd.read_csv(os.path.join(folder, fn))\n",
    "# df = clean_any(df, source=\"Yahoo\", symbol_hint=os.path.splitext(fn)[0].upper())\n",
    "# preview(df, \"Yahoo (JSM)\")\n",
    "\n",
    "# # FH panel (prices)\n",
    "# p = os.path.expanduser(\"~/csc1171/data/raw/AMEX_NYSE_NASDAQ_stock_histories/fh_5yrs.csv\")\n",
    "# df = pd.read_csv(p)\n",
    "# df = clean_any(df, source=\"FH5\")\n",
    "# preview(df, \"FH5\")\n",
    "\n",
    "# # EDHEC (returns)\n",
    "# p = os.path.expanduser(\"~/csc1171/data/raw/EDHEC_Hedge_Fund_Returns/HF.csv\")\n",
    "# df = pd.read_csv(p)\n",
    "# df = clean_any(df, source=\"EDHEC\")\n",
    "# preview(df, \"EDHEC (returns)\")\n",
    "\n",
    "# FH panel (prices)\n",
    "p = os.path.expanduser(\"~/csc1171/data/raw/AMEX_NYSE_NASDAQ_stock_histories/fh_5yrs.csv\")\n",
    "df = clean_any(pd.read_csv(p), source=\"FH5\")\n",
    "print(\"\\n=== FH5 ===\", df.shape); print(df.dtypes); print(df.head(5))\n",
    "\n",
    "# EDHEC (returns)\n",
    "p = os.path.expanduser(\"~/csc1171/data/raw/EDHEC_Hedge_Fund_Returns/HF.csv\")\n",
    "df = clean_any(pd.read_csv(p), source=\"EDHEC\")\n",
    "print(\"\\n=== EDHEC ===\", df.shape); print(df.dtypes); print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxT9ptfWAjo8"
   },
   "outputs": [],
   "source": [
    "# align offical trading calanders\n",
    "\n",
    "# --- Minimal NYSE holiday calendar (covers core exchange closures) ------------\n",
    "class NYSEHolidayCalendar(AbstractHolidayCalendar):\n",
    "    rules = [\n",
    "        # New Year's Day\n",
    "        Holiday(\"NewYearsDay\", month=1, day=1, observance=nearest_workday),\n",
    "        # Martin Luther King Jr. Day (since 1998)\n",
    "        Holiday(\"MLK\", month=1, day=1, offset=pd.DateOffset(weekday=pd.offsets.WeekOfMonth(week=2, weekday=0)), start_date=\"1998-01-01\"),\n",
    "        # Washington's Birthday (Presidents' Day) – 3rd Monday in Feb\n",
    "        Holiday(\"PresidentsDay\", month=2, day=1, offset=pd.DateOffset(weekday=pd.offsets.WeekOfMonth(week=2, weekday=0))),\n",
    "        # Good Friday (NYSE closed)\n",
    "        GoodFriday,\n",
    "        # Memorial Day – last Monday in May\n",
    "        Holiday(\"MemorialDay\", month=5, day=31, offset=pd.DateOffset(weekday=pd.offsets.Week(weekday=0))),\n",
    "        # Juneteenth (since 2022), nearest workday\n",
    "        Holiday(\"Juneteenth\", month=6, day=19, observance=nearest_workday, start_date=\"2022-06-19\"),\n",
    "        # Independence Day\n",
    "        Holiday(\"IndependenceDay\", month=7, day=4, observance=nearest_workday),\n",
    "        # Labor Day – first Monday in September\n",
    "        Holiday(\"LaborDay\", month=9, day=1, offset=pd.DateOffset(weekday=pd.offsets.WeekOfMonth(week=0, weekday=0))),\n",
    "        # Thanksgiving – fourth Thursday in November\n",
    "        Holiday(\"Thanksgiving\", month=11, day=1, offset=pd.DateOffset(weekday=pd.offsets.WeekOfMonth(week=3, weekday=3))),\n",
    "        # Christmas Day\n",
    "        Holiday(\"Christmas\", month=12, day=25, observance=nearest_workday),\n",
    "    ]\n",
    "\n",
    "# Helper: build a sessions index for [start, end] inclusive\n",
    "# def trading_sessions(start: pd.Timestamp, end: pd.Timestamp, calendar: str = \"NYSE\", tz: str = \"UTC\") -> pd.DatetimeIndex:\n",
    "#     cal = NYSEHolidayCalendar() if calendar.upper() == \"NYSE\" else NYSEHolidayCalendar()\n",
    "#     cbd = CustomBusinessDay(calendar=cal)\n",
    "#     idx = pd.date_range(start=start.normalize(), end=end.normalize(), freq=cbd)\n",
    "#     # lock tz\n",
    "#     idx = pd.to_datetime(idx).tz_localize(\"UTC\").tz_convert(tz)\n",
    "#     return idx\n",
    "def trading_sessions(start: pd.Timestamp, end: pd.Timestamp, calendar: str = \"NYSE\", tz: str = \"UTC\") -> pd.DatetimeIndex:\n",
    "    cal = NYSEHolidayCalendar() if calendar.upper() == \"NYSE\" else NYSEHolidayCalendar()\n",
    "    cbd = CustomBusinessDay(calendar=cal)\n",
    "    idx = pd.date_range(start=start.normalize(), end=end.normalize(), freq=cbd)\n",
    "    # <-- the fix: only localize if naive, otherwise convert\n",
    "    if getattr(idx, \"tz\", None) is None:\n",
    "        idx = idx.tz_localize(\"UTC\")\n",
    "    else:\n",
    "        idx = idx.tz_convert(\"UTC\")\n",
    "    return idx.tz_convert(tz)\n",
    "\n",
    "# Proper OHLCV aggregation for same-day duplicates\n",
    "_OHLCV_AGG = {\n",
    "    \"open\": \"first\",\n",
    "    \"high\": \"max\",\n",
    "    \"low\": \"min\",\n",
    "    \"close\": \"last\",\n",
    "    \"adj_close\": \"last\",\n",
    "    \"volume\": \"sum\",\n",
    "}\n",
    "\n",
    "def _ensure_tz_utc(dt: pd.Series) -> pd.Series:\n",
    "    # Make tz-aware UTC without changing the wall time if naive (consistent with your pipeline)\n",
    "    if getattr(dt.dt, \"tz\", None) is not None:\n",
    "        return dt.dt.tz_convert(\"UTC\")\n",
    "    else:\n",
    "        return dt.dt.tz_localize(\"UTC\")\n",
    "\n",
    "def _dedupe_ohlcv(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    keys = [c for c in [\"date\", \"symbol\"] if c in df.columns]\n",
    "    if not keys:\n",
    "        return df\n",
    "    agg = {c: fn for c, fn in _OHLCV_AGG.items() if c in df.columns}\n",
    "    others = [c for c in df.columns if c not in set(list(agg.keys()) + keys)]\n",
    "    # Keep first non-null for non-price metadata\n",
    "    agg.update({c: \"first\" for c in others})\n",
    "    out = (df.groupby(keys, as_index=False).agg(agg)\n",
    "             .sort_values(keys)\n",
    "             .reset_index(drop=True))\n",
    "    return out\n",
    "\n",
    "def align_to_trading_calendar(\n",
    "    df: pd.DataFrame,\n",
    "    calendar: str = \"NYSE\",\n",
    "    tz: str = \"UTC\",\n",
    "    fill: dict | None = None,\n",
    "    limit_fill: int | None = None,\n",
    "    clip_to_observed_span: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    if \"date\" not in df.columns:\n",
    "        return df.copy()\n",
    "\n",
    "    # 1) lock to UTC internally\n",
    "    work = df.copy()\n",
    "    work[\"date\"] = _ensure_tz_utc(pd.to_datetime(work[\"date\"], errors=\"coerce\"))\n",
    "    work = work[work[\"date\"].notna()]\n",
    "    if \"symbol\" in work.columns:\n",
    "        work[\"symbol\"] = work[\"symbol\"].astype(str)\n",
    "\n",
    "    # collapse intraday times to daily session key (midnight UTC)\n",
    "    work[\"date\"] = work[\"date\"].dt.tz_convert(\"UTC\").dt.normalize()\n",
    "\n",
    "    # optional heads-up if dupes were dropped upstream\n",
    "    if work.duplicated(subset=[\"symbol\",\"date\"], keep=False).sum() == 0:\n",
    "        print(\"[WARN] No (symbol,date) duplicates pre-dedupe; upstream may have dropped intraday rows already.\")\n",
    "\n",
    "    # 2) dedupe exact same-day rows per symbol using OHLCV aggregation\n",
    "    work = _dedupe_ohlcv(work)\n",
    "\n",
    "    # 3) build calendar sessions\n",
    "    if clip_to_observed_span and \"symbol\" in work.columns:\n",
    "        spans = (work.groupby(\"symbol\")[\"date\"].agg([\"min\", \"max\"])\n",
    "                 .rename(columns={\"min\":\"start\",\"max\":\"end\"}))\n",
    "        frames = []\n",
    "        for sym, (start, end) in spans.iterrows():\n",
    "            sess = trading_sessions(start, end, calendar=calendar, tz=\"UTC\")\n",
    "            g = work[work[\"symbol\"] == sym].set_index(\"date\").sort_index()\n",
    "            g = g.reindex(sess)\n",
    "            g[\"symbol\"] = sym\n",
    "            frames.append(g)\n",
    "        aligned = (pd.concat(frames, axis=0)\n",
    "                     .reset_index()\n",
    "                     .rename(columns={\"index\":\"date\"}))\n",
    "    else:\n",
    "        start, end = work[\"date\"].min(), work[\"date\"].max()\n",
    "        sess = trading_sessions(start, end, calendar=calendar, tz=\"UTC\")\n",
    "        aligned = (work.set_index(\"date\").sort_index()\n",
    "                        .groupby(\"symbol\", group_keys=True)\n",
    "                        .apply(lambda g: g.reindex(sess))\n",
    "                        .reset_index(level=0))\n",
    "        aligned = aligned.reset_index().rename(columns={\"index\":\"date\"})\n",
    "\n",
    "    # 4) optional fills\n",
    "    if fill:\n",
    "        for col, how in fill.items():\n",
    "            if col not in aligned.columns:\n",
    "                continue\n",
    "            if how == \"ffill\":\n",
    "                aligned[col] = (aligned.groupby(\"symbol\", dropna=False)[col]\n",
    "                                .ffill(limit=limit_fill))\n",
    "            elif how == \"bfill\":\n",
    "                aligned[col] = (aligned.groupby(\"symbol\", dropna=False)[col]\n",
    "                                .bfill(limit=limit_fill))\n",
    "            elif how == \"fillna0\":\n",
    "                aligned[col] = aligned[col].fillna(0)\n",
    "\n",
    "    # 5) order, tz, dtypes\n",
    "    keep_order = [c for c in [\"date\",\"symbol\",\"exchange\",\"currency\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\"] if c in aligned.columns]\n",
    "    other_cols = [c for c in aligned.columns if c not in keep_order]\n",
    "    aligned = aligned[keep_order + other_cols]\n",
    "\n",
    "    aligned[\"date\"] = aligned[\"date\"].dt.tz_convert(tz)\n",
    "\n",
    "    sort_keys = [c for c in [\"symbol\",\"date\"] if c in aligned.columns]\n",
    "    aligned = aligned.sort_values(sort_keys).reset_index(drop=True)\n",
    "\n",
    "    for c in [\"open\",\"high\",\"low\",\"close\",\"adj_close\"]:\n",
    "        if c in aligned.columns:\n",
    "            aligned[c] = aligned[c].astype(\"float64\")\n",
    "    if \"volume\" in aligned.columns:\n",
    "        aligned[\"volume\"] = pd.array(aligned[\"volume\"], dtype=\"Int64\")\n",
    "\n",
    "    return aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V1SvHz1TAmRY"
   },
   "outputs": [],
   "source": [
    "# Testing (align offical trading calanders)\n",
    "\n",
    "# --- synthetic sample (weekend, holiday, duplicates) ---\n",
    "raw = pd.DataFrame({\n",
    "    \"date\": [\n",
    "        \"2024-07-03 16:00:00\",          # ABC dup 1\n",
    "        \"2024-07-03 10:00:00-04:00\",    # ABC dup 2 (same calendar day)\n",
    "        \"2024-07-04 16:00:00\",          # NYSE holiday (Independence Day)\n",
    "        \"2024-07-05 16:00:00\",          # trading day\n",
    "        \"2024-07-06 16:00:00\",          # Saturday\n",
    "        \"2024-07-05 09:30:00-04:00\",    # XYZ trading day\n",
    "    ],\n",
    "    \"symbol\": [\"ABC\",\"ABC\",\"ABC\",\"ABC\",\"ABC\",\"XYZ\"],\n",
    "    \"open\":   [10,  10.5, 11,   12,  12.5, 20],\n",
    "    \"high\":   [11,  11.5, 12,   13,  13.5, 21],\n",
    "    \"low\":    [ 9,   9.5, 10,   11,  11.5, 19],\n",
    "    \"close\":  [10.8,10.9, 11.8, 12.8,12.9, 20.5],\n",
    "    \"adj_close\":[10.8,10.9,11.8,12.8,12.9,20.5],\n",
    "    \"volume\": [100, 150, 200,  300,  400,  500],\n",
    "})\n",
    "\n",
    "print(\"=== RAW INPUT ===\")\n",
    "print(raw)\n",
    "\n",
    "# IMPORTANT: do NOT normalize 'raw[\"date\"]' here;\n",
    "# the function now normalizes internally before deduping.\n",
    "\n",
    "aligned = align_to_trading_calendar(raw, calendar=\"NYSE\", tz=\"UTC\")\n",
    "\n",
    "print(\"\\n=== AFTER ALIGNMENT (UTC, NYSE sessions only) ===\")\n",
    "print(aligned)\n",
    "\n",
    "print(\"\\nContains holiday 2024-07-04? ->\", any(\"2024-07-04\" in s for s in aligned[\"date\"].astype(str)))\n",
    "print(\"Contains Saturday 2024-07-06? ->\", any(\"2024-07-06\" in s for s in aligned[\"date\"].astype(str)))\n",
    "\n",
    "row_703 = aligned[(aligned[\"symbol\"]==\"ABC\") & (aligned[\"date\"].astype(str).str.startswith(\"2024-07-03\"))]\n",
    "if not row_703.empty:\n",
    "    r = row_703.iloc[0]\n",
    "    print(\"\\nABC 2024-07-03 -> open,high,low,close,volume:\", r[\"open\"], r[\"high\"], r[\"low\"], r[\"close\"], r[\"volume\"])\n",
    "else:\n",
    "    print(\"\\nABC 2024-07-03 row not found (unexpected).\")\n",
    "# Trading halts (same logic, with step-by-step prints) — FIXED\n",
    "def annotate_trading_halts(\n",
    "    df: pd.DataFrame,\n",
    "    price_cols=(\"open\",\"high\",\"low\",\"close\",\"adj_close\"),\n",
    "    symbol_col=\"symbol\",\n",
    "    date_col=\"date\",\n",
    "    market_open_mask: pd.Series | None = None,  # index=date (tz-aware), bool\n",
    "    span_mode: str = \"since_first\",             # \"since_first\" or \"first_to_last\"\n",
    "    verbose: bool = True,                       # <-- turn prints on/off\n",
    ") -> pd.DataFrame:\n",
    "    if verbose:\n",
    "        print(\"\\n---- annotate_trading_halts(): START ----\")\n",
    "        print(f\"Input rows: {len(df):,} | symbols: {df[symbol_col].nunique() if symbol_col in df else 'NA'}\")\n",
    "        if date_col in df:\n",
    "            print(f\"Date span: {df[date_col].min()}  ->  {df[date_col].max()}\")\n",
    "\n",
    "    out = df.copy()\n",
    "\n",
    "    # 1) Active trading landmarks (compute per symbol, then map to all rows)\n",
    "    any_price = out[list(price_cols)].notna().any(axis=1)\n",
    "    if verbose:\n",
    "        print(f\"Rows with ANY price present: {int(any_price.sum()):,}\")\n",
    "\n",
    "    sym_first = out.loc[any_price].groupby(symbol_col)[date_col].min()\n",
    "    sym_last  = out.loc[any_price].groupby(symbol_col)[date_col].max()\n",
    "    if verbose:\n",
    "        print(f\"Symbols with ≥1 traded row: {len(sym_first):,}\")\n",
    "\n",
    "    out[\"_first_trade\"] = out[symbol_col].map(sym_first)\n",
    "    out[\"_last_trade\"]  = out[symbol_col].map(sym_last)\n",
    "\n",
    "    if span_mode == \"first_to_last\":\n",
    "        if verbose: print('Span mode = \"first_to_last\"')\n",
    "        in_active_span = (\n",
    "            out[\"_first_trade\"].notna()\n",
    "            & out[\"_last_trade\"].notna()\n",
    "            & (out[date_col] >= out[\"_first_trade\"])\n",
    "            & (out[date_col] <= out[\"_last_trade\"])\n",
    "        )\n",
    "    else:\n",
    "        if verbose: print('Span mode = \"since_first\"')\n",
    "        in_active_span = out[\"_first_trade\"].notna() & (out[date_col] >= out[\"_first_trade\"])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"in_active_span = True rows: {int(in_active_span.sum()):,}\")\n",
    "\n",
    "    # 2) All prices missing on a trading session\n",
    "    no_prices = out[list(price_cols)].isna().all(axis=1)\n",
    "    if verbose:\n",
    "        print(f\"Rows where ALL price cols are NaN: {int(no_prices.sum()):,}\")\n",
    "\n",
    "    # 3) Optional market-open sanity (fixed + FutureWarning silenced)\n",
    "    if market_open_mask is not None:\n",
    "        mok = market_open_mask.copy()\n",
    "        mok.index = pd.to_datetime(mok.index).tz_convert(out[date_col].dt.tz)\n",
    "        mapped = mok.reindex(out[date_col].values)   # align by date values\n",
    "        mapped.index = out.index                     # align row index 1:1\n",
    "        open_flag = mapped.fillna(True).infer_objects(copy=False).astype(bool)\n",
    "        if verbose:\n",
    "            print(f\"market_open_mask provided. True rows: {int(open_flag.sum()):,}\")\n",
    "    else:\n",
    "        open_flag = pd.Series(True, index=out.index)\n",
    "        if verbose:\n",
    "            print(\"market_open_mask not provided → assuming all trading-session dates were open (True).\")\n",
    "\n",
    "    # Core flag\n",
    "    out[\"is_halt\"] = in_active_span & no_prices & open_flag\n",
    "    if verbose:\n",
    "        print(f\"HALT rows flagged (is_halt=True): {int(out['is_halt'].sum()):,}\")\n",
    "\n",
    "    # 4) Halt blocks\n",
    "    prev = out.groupby(symbol_col)[\"is_halt\"].shift(fill_value=False)\n",
    "    block_start = out[\"is_halt\"] & ~prev\n",
    "    out[\"halt_block_id\"] = block_start.groupby(out[symbol_col]).cumsum()\n",
    "    out.loc[~out[\"is_halt\"], \"halt_block_id\"] = pd.NA\n",
    "\n",
    "    # 5) Block lengths\n",
    "    out[\"halt_len\"] = (\n",
    "        out[out[\"is_halt\"]]\n",
    "        .groupby([symbol_col, \"halt_block_id\"])[date_col]\n",
    "        .transform(\"count\")\n",
    "    ).fillna(0).astype(\"Int64\")\n",
    "\n",
    "    # Cleanup\n",
    "    out = out.drop(columns=[\"_first_trade\",\"_last_trade\"])\n",
    "\n",
    "    if verbose:\n",
    "        # Small per-symbol summary (only those with halts)\n",
    "        sym_halts = (out.groupby(symbol_col)[\"is_halt\"].sum()).sort_values(ascending=False)\n",
    "        syms_with_halts = sym_halts[sym_halts > 0].index.tolist()\n",
    "        if syms_with_halts:\n",
    "            print(\"-- Halt summary (per symbol) --\")\n",
    "            for s in syms_with_halts[:8]:\n",
    "                sub = out[(out[symbol_col]==s) & (out[\"is_halt\"])]\n",
    "                blocks = sub.groupby(\"halt_block_id\")[date_col].agg([\"min\",\"max\",\"count\"])\n",
    "                total = int(sub[\"is_halt\"].sum())\n",
    "                print(f\"  {s}: {total} halt day(s), {len(blocks)} block(s)\")\n",
    "                for _, r in blocks.reset_index(drop=True).iterrows():\n",
    "                    print(f\"    {r['min']} -> {r['max']}  (len={int(r['count'])})\")\n",
    "        else:\n",
    "            print(\"No halts detected.\")\n",
    "        print(\"---- annotate_trading_halts(): END ----\\n\")\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0El9bxJ-Aoi0"
   },
   "outputs": [],
   "source": [
    "# --- Test: annotate_trading_halts on a small panel ---\n",
    "\n",
    "# Synthetic intraday-ish data across an NYSE holiday (Jul 4) + weekend,\n",
    "# with one *halt* day for ABC (all prices missing on a real session) while XYZ trades.\n",
    "\n",
    "raw = pd.DataFrame({\n",
    "    \"date\": [\n",
    "        \"2024-07-02 16:00:00\",      # both trade\n",
    "        \"2024-07-03 16:00:00\",      # both trade\n",
    "        \"2024-07-04 16:00:00\",      # NYSE holiday (should be removed by calendar)\n",
    "        \"2024-07-05 10:00:00\",      # XYZ trades; ABC halted (no rows for ABC -> will be NaN after reindex)\n",
    "        \"2024-07-06 16:00:00\",      # Saturday (removed)\n",
    "        \"2024-07-08 16:00:00\",      # both trade\n",
    "    ],\n",
    "    \"symbol\": [\"ABC\",\"ABC\",\"ABC\",\"XYZ\",\"ABC\",\"ABC\"],\n",
    "    \"open\":   [10,  10.5, 11,   20,  12.5, 13.0],\n",
    "    \"high\":   [11,  11.5, 12,   21,  13.5, 14.0],\n",
    "    \"low\":    [ 9,   9.5, 10,   19,  11.5, 12.0],\n",
    "    \"close\":  [10.8,10.9, 11.8, 20.5,12.9, 13.5],\n",
    "    \"adj_close\":[10.8,10.9,11.8,20.5,12.9,13.5],\n",
    "    \"volume\": [100, 150, 200,  500, 400,  300],\n",
    "})\n",
    "\n",
    "# Align to NYSE sessions (removes holiday/weekend; aggregates duplicates if any)\n",
    "aligned = align_to_trading_calendar(raw, calendar=\"NYSE\", tz=\"UTC\")\n",
    "\n",
    "# Build a panel-level \"market open\" mask based on any trading observed that date across symbols\n",
    "def build_market_open_mask(panel, date_col=\"date\", price_cols=(\"open\",\"high\",\"low\",\"close\",\"adj_close\")):\n",
    "    return panel.groupby(date_col)[list(price_cols)].apply(lambda x: x.notna().any().any())\n",
    "\n",
    "mopen = build_market_open_mask(aligned)\n",
    "\n",
    "# Apply halt annotation\n",
    "halted = annotate_trading_halts(aligned, market_open_mask=mopen)\n",
    "\n",
    "# Show only dates that are halts or a few surrounding rows for context\n",
    "view = halted.loc[\n",
    "    (halted[\"date\"].astype(str) >= \"2024-07-02\") & (halted[\"date\"].astype(str) <= \"2024-07-08\"),\n",
    "    [\"date\",\"symbol\",\"open\",\"high\",\"low\",\"close\",\"adj_close\",\"volume\",\"is_halt\",\"halt_block_id\",\"halt_len\"]\n",
    "].sort_values([\"date\",\"symbol\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== Aligned panel with halt annotations (focus window) ===\")\n",
    "print(view.to_string(index=False))\n",
    "\n",
    "# Quick checks\n",
    "print(\"\\nChecks:\")\n",
    "print(\"- 2024-07-04 present?        ->\", any(s.startswith(\"2024-07-04\") for s in halted[\"date\"].astype(str)))  # should be False (holiday)\n",
    "print(\"- 2024-07-06 present?        ->\", any(s.startswith(\"2024-07-06\") for s in halted[\"date\"].astype(str)))  # should be False (Saturday)\n",
    "print(\"- ABC halted on 2024-07-05?  ->\", bool(\n",
    "    halted[(halted[\"symbol\"]==\"ABC\") & (halted[\"date\"].astype(str).str.startswith(\"2024-07-05\"))][\"is_halt\"].any()\n",
    "))\n",
    "print(\"- XYZ traded on 2024-07-05?  ->\", bool(\n",
    "    halted[(halted[\"symbol\"]==\"XYZ\") & (halted[\"date\"].astype(str).str.startswith(\"2024-07-05\"))][[\"open\",\"close\"]].notna().any().any()\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjmdteREDTwt"
   },
   "outputs": [],
   "source": [
    "df_chi, chi2_stat, p_val = process_asset_file(\".csv\")\n",
    "print(df_chi)\n",
    "print(f\"Chi² statistic: {chi2_stat:.4f}, p-value: {p_val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
