{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful Notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    " ''' From Statistical Testing Notebook, we need this for mean-difference tests\n",
    "     (Fri -> Mon, ToM vs non-ToM, January vs Rest, H1 vs H2, Weekday pairs, etc.). '''\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def manual_t_from_summary(mean_a, mean_b, var_a, var_b, n_a, n_b, two_sided=True):\n",
    "    \"\"\"\n",
    "    Two-sample t-test from SUMMARY STATS (equal-variance classical form).\n",
    "    Use when you already have group means/variances/N (e.g., pre-aggregated by weekday).\n",
    "    Parameters are sample variances (ddof=1).\n",
    "    \"\"\"\n",
    "    # pooled standard error under equal-variance assumption\n",
    "    se = np.sqrt(var_a / n_a + var_b / n_b)\n",
    "    t_stat = (mean_a - mean_b) / se\n",
    "    df = n_a + n_b - 2\n",
    "    # two-sided p-value by default\n",
    "    if two_sided:\n",
    "        p = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df))\n",
    "    else:\n",
    "        p = 1 - stats.t.cdf(t_stat, df=df)\n",
    "    return t_stat, df, p\n",
    "\n",
    "def manual_t_from_samples(a, b, equal_var=True, two_sided=True):\n",
    "    \"\"\"\n",
    "    Two-sample t-test from RAW SAMPLES (arrays of returns).\n",
    "    Matches the structure of the original cell but operates on your empirical groups.\n",
    "    \"\"\"\n",
    "    n_a, n_b = len(a), len(b)\n",
    "    mean_a, mean_b = np.mean(a), np.mean(b)\n",
    "    var_a = np.var(a, ddof=1)\n",
    "    var_b = np.var(b, ddof=1)\n",
    "\n",
    "    if equal_var:\n",
    "        # classical pooled-variance t\n",
    "        se = np.sqrt(var_a / n_a + var_b / n_b)\n",
    "        df = n_a + n_b - 2\n",
    "    else:\n",
    "        # Welch's t (safer when variances differ)\n",
    "        se = np.sqrt(var_a / n_a + var_b / n_b)\n",
    "        df = (var_a / n_a + var_b / n_b) ** 2 / (\n",
    "            (var_a**2) / (n_a**2 * (n_a - 1)) + (var_b**2) / (n_b**2 * (n_b - 1))\n",
    "        )\n",
    "\n",
    "    t_stat = (mean_a - mean_b) / se\n",
    "    if two_sided:\n",
    "        p = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df))\n",
    "    else:\n",
    "        p = 1 - stats.t.cdf(t_stat, df=df)\n",
    "    return t_stat, df, p\n",
    "\n",
    "def scipy_t_from_samples(a, b, equal_var=True):\n",
    "    \"\"\"\n",
    "    SciPy wrapper equivalent of the above (useful for quick checks).\n",
    "    \"\"\"\n",
    "    t_stat, p = stats.ttest_ind(a, b, equal_var=equal_var)\n",
    "    # SciPy returns two-sided p; df is not returned, so we compute it if equal_var, else Welch-approx.\n",
    "    n_a, n_b = len(a), len(b)\n",
    "    if equal_var:\n",
    "        df = n_a + n_b - 2\n",
    "    else:\n",
    "        var_a = np.var(a, ddof=1); var_b = np.var(b, ddof=1)\n",
    "        df = (var_a / n_a + var_b / n_b) ** 2 / (\n",
    "            (var_a**2) / (n_a**2 * (n_a - 1)) + (var_b**2) / (n_b**2 * (n_b - 1))\n",
    "        )\n",
    "    return t_stat, df, p\n",
    "\n",
    "# ---- Example usage (REPLACE with your actual arrays) ----\n",
    "# fri_returns = ...   # numpy array\n",
    "# mon_returns = ...   # numpy array\n",
    "# t, df, p = manual_t_from_samples(fri_returns, mon_returns, equal_var=False)\n",
    "# print(f\"Fri−Mon: t={t:.3f}, df={df:.1f}, p={p:.3g}\")\n",
    "\n",
    "\n",
    "# ''' From Statistical Testing Notebook, we need this for global multi-group tests\n",
    "#     (Month-of-Year bars Jan…Dec → overall ANOVA; optionally Weekday 5-group test). '''\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "def anova_from_groups(group_dict, label_col=\"group\", value_col=\"value\"):\n",
    "    \"\"\"\n",
    "    One-way ANOVA using statsmodels, mirroring your original notebook pattern\n",
    "    but without loading any external demo file.\n",
    "    - group_dict: {'Jan': array_like, 'Feb': array_like, ..., 'Dec': array_like}\n",
    "                  (or {'Mon':..., 'Tue':..., ...} for weekday)\n",
    "    Returns (f_value, p_value, anova_table)\n",
    "    \"\"\"\n",
    "    # Build long DataFrame like your melt step\n",
    "    rows = []\n",
    "    for g, arr in group_dict.items():\n",
    "        arr = np.asarray(arr)\n",
    "        rows.append(pd.DataFrame({label_col: g, value_col: arr}))\n",
    "    df_long = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    # OLS with categorical factor\n",
    "    model = ols(f\"{value_col} ~ C({label_col})\", data=df_long).fit()\n",
    "    anova_tbl = sm.stats.anova_lm(model, typ=2)\n",
    "    f_value = float(anova_tbl.loc[f\"C({label_col})\", \"F\"])\n",
    "    p_value = float(anova_tbl.loc[f\"C({label_col})\", \"PR(>F)\"])\n",
    "    return f_value, p_value, anova_tbl\n",
    "\n",
    "# ---- Example usage (REPLACE with your month buckets) ----\n",
    "# month_groups = {\n",
    "#     'Jan': returns_jan, 'Feb': returns_feb, 'Mar': returns_mar, 'Apr': returns_apr,\n",
    "#     'May': returns_may, 'Jun': returns_jun, 'Jul': returns_jul, 'Aug': returns_aug,\n",
    "#     'Sep': returns_sep, 'Oct': returns_oct, 'Nov': returns_nov, 'Dec': returns_dec\n",
    "# }\n",
    "# F, p, table = anova_from_groups(month_groups, label_col=\"month\", value_col=\"r\")\n",
    "# print(f\"MoY ANOVA: F={F:.3f}, p={p:.3g}\")\n",
    "# display(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' From Linear Regression Lab, we need this for:\n",
    "#      (1) running OLS with a formula on our returns using calendar dummies/controls, and\n",
    "#      (2) basic diagnostics + a tidy coefficient table with 95% CI for plotting betas. '''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fit_ols(formula: str, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit OLS via statsmodels using a Patsy formula (e.g.,\n",
    "    'r ~ C(weekday) + ToM + C(month) + log_size + C(sector)').\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    model = smf.ols(formula=formula, data=data).fit()\n",
    "    return model\n",
    "\n",
    "def coef_table_with_ci(model, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Return a tidy coefficient table: term, estimate, std err, t, p, [CI_low, CI_high].\n",
    "    Useful for 'calendar-dummy regression betas ± CI' plots.\n",
    "    \"\"\"\n",
    "    params = model.params.rename(\"estimate\")\n",
    "    bse = model.bse.rename(\"std_err\")\n",
    "    tvals = model.tvalues.rename(\"t\")\n",
    "    pvals = model.pvalues.rename(\"p\")\n",
    "    ci = model.conf_int(alpha=alpha)\n",
    "    ci.columns = [\"ci_low\", \"ci_high\"]\n",
    "    out = pd.concat([params, bse, tvals, pvals, ci], axis=1).reset_index()\n",
    "    out = out.rename(columns={\"index\": \"term\"})\n",
    "    return out\n",
    "\n",
    "def plot_residuals_vs_fitted(model, title: str = None):\n",
    "    \"\"\"\n",
    "    Basic diagnostic: residuals vs fitted. Helps check mean-variance patterns\n",
    "    and outliers after calendar-dummy regressions.\n",
    "    \"\"\"\n",
    "    resid = model.resid\n",
    "    fitted = model.fittedvalues\n",
    "    plt.figure()\n",
    "    plt.scatter(fitted, resid, alpha=0.7)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(title or \"Residuals vs Fitted\")\n",
    "    plt.show()\n",
    "\n",
    "# ---- Example usage (REPLACE with your real DataFrame/columns) ----\n",
    "# df = ...  # your returns with engineered features (weekday, ToM, month, etc.)\n",
    "# model = fit_ols(\"r ~ C(weekday) + ToM + C(month) + C(quarter_end) + log_size + C(sector)\", data=df)\n",
    "# betas = coef_table_with_ci(model, alpha=0.05)\n",
    "# display(betas)\n",
    "# plot_residuals_vs_fitted(model, title=\"Residuals vs Fitted (calendar-dummy OLS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' From MLR-Practice Notebook, we need this for sector-controlled calendar regressions:\n",
    "#     - OLS with Patsy formula: r ~ calendar dummies + controls + C(sector) (fixed effects)\n",
    "#     - Variance Inflation Factor (VIF) to check multicollinearity among numeric controls\n",
    "#     - Durbin–Watson for autocorrelation diagnostic\n",
    "#     - Residuals vs Fitted plot (basic heteroskedasticity/outlier check)\n",
    "#     This supports: \"Size-/sector-controlled coefficient plot (calendar-dummy regression betas ± CI)\". '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "# -------- OLS wrapper (sector FE via C(sector)) --------\n",
    "def fit_ols(formula: str, data: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Fit OLS with a Patsy formula on your returns DataFrame.\n",
    "    Example formula:\n",
    "      r ~ C(weekday) + ToM + C(month) + log_size + C(sector)\n",
    "    \"\"\"\n",
    "    return smf.ols(formula=formula, data=data).fit()\n",
    "\n",
    "# -------- Tidy coefficient table (for betas ± CI plots) --------\n",
    "def coef_table_with_ci(model, alpha: float = 0.05) -> pd.DataFrame:\n",
    "    params = model.params.rename(\"estimate\")\n",
    "    bse = model.bse.rename(\"std_err\")\n",
    "    tvals = model.tvalues.rename(\"t\")\n",
    "    pvals = model.pvalues.rename(\"p\")\n",
    "    ci = model.conf_int(alpha=alpha)\n",
    "    ci.columns = [\"ci_low\", \"ci_high\"]\n",
    "    out = pd.concat([params, bse, tvals, pvals, ci], axis=1).reset_index()\n",
    "    return out.rename(columns={\"index\": \"term\"})\n",
    "\n",
    "# -------- Residuals vs Fitted diagnostic --------\n",
    "def plot_residuals_vs_fitted(model, title: str = \"Residuals vs Fitted\"):\n",
    "    resid = model.resid\n",
    "    fitted = model.fittedvalues\n",
    "    plt.figure()\n",
    "    plt.scatter(fitted, resid, alpha=0.7)\n",
    "    plt.axhline(0, linestyle=\"--\")\n",
    "    plt.xlabel(\"Fitted values\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# -------- VIF for numeric columns in a formula context --------\n",
    "def compute_vif_from_formula(formula: str, data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Computes VIF on the numeric design matrix implied by `formula`.\n",
    "    Categorical terms (e.g., C(sector), C(weekday)) expand to dummies; VIF is computed\n",
    "    on the full exogenous matrix excluding the intercept.\n",
    "    \"\"\"\n",
    "    # Build design matrices (y unused here, but y/X ensures same design handling as the model)\n",
    "    y, X = dmatrices(formula, data=data, return_type=\"dataframe\")\n",
    "    # Drop intercept if present\n",
    "    if \"Intercept\" in X.columns:\n",
    "        X_no_intercept = X.drop(columns=[\"Intercept\"])\n",
    "    else:\n",
    "        X_no_intercept = X.copy()\n",
    "    vif_rows = []\n",
    "    X_vals = X_no_intercept.values\n",
    "    for i, col in enumerate(X_no_intercept.columns):\n",
    "        vif_val = variance_inflation_factor(X_vals, i)\n",
    "        vif_rows.append({\"term\": col, \"VIF\": float(vif_val)})\n",
    "    return pd.DataFrame(vif_rows).sort_values(\"VIF\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# -------- Durbin–Watson convenience wrapper --------\n",
    "def durbin_watson_stat(model) -> float:\n",
    "    \"\"\"DW statistic ~2 means low first-order autocorrelation.\"\"\"\n",
    "    return float(durbin_watson(model.resid))\n",
    "\n",
    "# ---------------- Example usage (REPLACE with your real DataFrame/columns) ----------------\n",
    "# df = ...  # Your panel of returns with columns: r, weekday/month/ToM flags, sector, optional log_size, etc.\n",
    "\n",
    "# Example sector-controlled calendar regression:\n",
    "# formula = \"r ~ C(weekday) + ToM + C(month) + log_size + C(sector)\"\n",
    "# mdl = fit_ols(formula, df)\n",
    "# betas = coef_table_with_ci(mdl)\n",
    "# print(mdl.summary().tables[0])        # brief header\n",
    "# display(betas)                        # tidy table for plotting betas ± CI\n",
    "# print('Durbin–Watson:', durbin_watson_stat(mdl))\n",
    "# vif_tbl = compute_vif_from_formula(formula, df)\n",
    "# display(vif_tbl)\n",
    "# plot_residuals_vs_fitted(mdl, title=\"Residuals vs Fitted (calendar + sector FE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' From Logistic-EDA ideas (repurposed for our calendar-effects QC):\n",
    "    - Group-size bar charts for calendar buckets (e.g., Weekday, ToM vs non-ToM, H1 vs H2, Quarter-end vs other)\n",
    "    - Correlation heatmap for numeric controls (pairs naturally with our VIF table)\n",
    "  Why: show sample sizes (assumptions/context for t/ANOVA) and inspect collinearity among controls before OLS.\n",
    "  Uses only numpy/pandas/matplotlib (no seaborn). Ready to drop into our project notebook. '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Group-size tables & bars ----------\n",
    "\n",
    "def group_size_table(df: pd.DataFrame, group_col: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a small table with counts and percentages by group.\n",
    "    Useful to place next to mean-difference bars so readers see n per bucket.\n",
    "    \"\"\"\n",
    "    vc = df[group_col].value_counts(dropna=False)\n",
    "    tbl = pd.DataFrame({\n",
    "        group_col: vc.index.astype(object),\n",
    "        \"n\": vc.values\n",
    "    })\n",
    "    tbl[\"pct\"] = (tbl[\"n\"] / tbl[\"n\"].sum()) * 100.0\n",
    "    return tbl.sort_values(group_col).reset_index(drop=True)\n",
    "\n",
    "def plot_group_size_bar(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    title: str = None,\n",
    "    order: list | None = None,\n",
    "    annotate: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Bar chart of sample counts per group_col.\n",
    "    - order: optional list specifying the display order (e.g., [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\"])\n",
    "    - annotate: add 'n (pct%)' labels above bars\n",
    "    \"\"\"\n",
    "    tbl = group_size_table(df, group_col)\n",
    "    if order is not None:\n",
    "        # Ensure all categories present; if not, we still show with zero counts\n",
    "        all_vals = pd.Index(order, name=group_col)\n",
    "        tbl = tbl.set_index(group_col).reindex(all_vals, fill_value=0).reset_index()\n",
    "    x = tbl[group_col].astype(str).tolist()\n",
    "    y = tbl[\"n\"].to_numpy()\n",
    "    pct = tbl[\"pct\"].to_numpy()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.bar(x, y)\n",
    "    ax.set_xlabel(group_col)\n",
    "    ax.set_ylabel(\"Sample count (n)\")\n",
    "    ax.set_title(title or f\"Group sizes: {group_col}\")\n",
    "    ax.set_ylim(0, max(y) * 1.15 if len(y) else 1)\n",
    "\n",
    "    if annotate:\n",
    "        for rect, n_i, p_i in zip(bars, y, pct):\n",
    "            ax.text(\n",
    "                rect.get_x() + rect.get_width() / 2.0,\n",
    "                rect.get_height(),\n",
    "                f\"{int(n_i)} ({p_i:.1f}%)\",\n",
    "                ha=\"center\", va=\"bottom\", rotation=0\n",
    "            )\n",
    "    plt.show()\n",
    "    return tbl  # so you can display the table beside the plot\n",
    "\n",
    "# ---------- Correlation heatmap for numeric controls ----------\n",
    "\n",
    "def correlation_df(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    method: str = \"pearson\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute correlation matrix among numeric controls.\n",
    "    - numeric_cols: explicit list; if None, auto-detect numeric columns.\n",
    "    - method: 'pearson' (default), 'spearman', or 'kendall'\n",
    "    \"\"\"\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    corr = df[numeric_cols].corr(method=method)\n",
    "    return corr\n",
    "\n",
    "def plot_correlation_heatmap(\n",
    "    corr: pd.DataFrame,\n",
    "    title: str = \"Correlation heatmap (controls)\",\n",
    "    annotate: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot a heatmap (matplotlib only) from a correlation DataFrame.\n",
    "    - annotate: write correlation values in each cell\n",
    "    \"\"\"\n",
    "    labels = corr.columns.tolist()\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(corr.to_numpy(), aspect=\"auto\", origin=\"upper\")\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(labels)\n",
    "    ax.set_title(title)\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "    cbar.ax.set_ylabel(\"corr\", rotation=90)\n",
    "\n",
    "    if annotate:\n",
    "        mat = corr.to_numpy()\n",
    "        nrows, ncols = mat.shape\n",
    "        for i in range(nrows):\n",
    "            for j in range(ncols):\n",
    "                ax.text(j, i, f\"{mat[i, j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ---------------- Example usage (REPLACE with your real DataFrame/columns) ----------------\n",
    "# df = ...  # your cleaned returns DataFrame with calendar flags/controls\n",
    "#\n",
    "# # 1) Group-size bars (QC next to mean-difference charts):\n",
    "# plot_group_size_bar(df, \"weekday\", order=[\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\"])\n",
    "# plot_group_size_bar(df, \"ToM\", title=\"Group sizes: Turn-of-the-Month (ToM)\")\n",
    "# plot_group_size_bar(df, \"half\", order=[\"H1\",\"H2\"], title=\"Group sizes: H1 vs H2\")\n",
    "# plot_group_size_bar(df, \"quarter_end\", title=\"Group sizes: Quarter-end vs other\")\n",
    "#\n",
    "# # 2) Correlation heatmap for numeric controls (pair with our VIF table):\n",
    "# num_cols = [\"log_size\", \"volatility_20d\", \"liquidity\", \"beta\"]  # example\n",
    "# corr = correlation_df(df, numeric_cols=num_cols, method=\"pearson\")\n",
    "# plot_correlation_heatmap(corr, title=\"Controls correlation (Pearson)\")\n",
    "#\n",
    "# # VIF remains in our existing helper:\n",
    "# # from earlier cell:\n",
    "# # vif_tbl = compute_vif_from_formula(\"r ~ C(weekday) + ToM + C(month) + log_size + C(sector)\", df)\n",
    "# # display(vif_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' From Paired-Tests Notebook, we need this for:\n",
    "#     - Overnight vs Intraday comparisons on the SAME dates (paired samples)\n",
    "#     - Optional H1 vs H2 if computed as per-year pairs (e.g., mean(H1_year_i) vs mean(H2_year_i))\n",
    "#   Outputs t, df, p and 95% CI for the mean difference; also a grouped version (e.g., by weekday).\n",
    "# '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def paired_t_from_samples(a, b, alpha: float = 0.05, two_sided: bool = True):\n",
    "    \"\"\"\n",
    "    Paired t-test on two same-length arrays a, b (pair matched row-wise).\n",
    "    Returns dict with: n, mean_a, mean_b, mean_diff, se_diff, t, df, p, ci_low, ci_high.\n",
    "    Drops pairs with any NaN.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    mask = np.isfinite(a) & np.isfinite(b)\n",
    "    a = a[mask]; b = b[mask]\n",
    "    n = a.size\n",
    "    if n < 2:\n",
    "        return {\n",
    "            \"n\": n, \"mean_a\": np.nan, \"mean_b\": np.nan, \"mean_diff\": np.nan,\n",
    "            \"se_diff\": np.nan, \"t\": np.nan, \"df\": np.nan, \"p\": np.nan,\n",
    "            \"ci_low\": np.nan, \"ci_high\": np.nan\n",
    "        }\n",
    "    d = a - b\n",
    "    mean_a = float(np.mean(a))\n",
    "    mean_b = float(np.mean(b))\n",
    "    mean_diff = float(np.mean(d))\n",
    "    sd_diff = float(np.std(d, ddof=1))\n",
    "    se_diff = sd_diff / np.sqrt(n)\n",
    "    df = n - 1\n",
    "    t_stat = mean_diff / se_diff if se_diff > 0 else np.inf\n",
    "    if two_sided:\n",
    "        p = 2 * (1 - stats.t.cdf(np.abs(t_stat), df=df))\n",
    "        tcrit = stats.t.ppf(1 - alpha/2, df=df)\n",
    "    else:\n",
    "        p = 1 - stats.t.cdf(t_stat, df=df)\n",
    "        tcrit = stats.t.ppf(1 - alpha, df=df)\n",
    "    half_width = tcrit * se_diff\n",
    "    ci_low = mean_diff - half_width\n",
    "    ci_high = mean_diff + half_width\n",
    "    return {\n",
    "        \"n\": int(n),\n",
    "        \"mean_a\": mean_a,\n",
    "        \"mean_b\": mean_b,\n",
    "        \"mean_diff\": mean_diff,\n",
    "        \"se_diff\": se_diff,\n",
    "        \"t\": float(t_stat),\n",
    "        \"df\": float(df),\n",
    "        \"p\": float(p),\n",
    "        \"ci_low\": float(ci_low),\n",
    "        \"ci_high\": float(ci_high),\n",
    "    }\n",
    "\n",
    "def paired_tests_by_group(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    a_col: str,\n",
    "    b_col: str,\n",
    "    order: list | None = None,\n",
    "    alpha: float = 0.05,\n",
    "    two_sided: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run paired t-tests within each group (e.g., by 'weekday').\n",
    "    Assumes rows are date-aligned so (a_col, b_col) are valid pairs within each group slice.\n",
    "    Returns tidy DataFrame: [group, n, mean_a, mean_b, mean_diff, ci_low, ci_high, t, df, p].\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    # Determine group order\n",
    "    groups = order if order is not None else (\n",
    "        df[group_col].dropna().unique().tolist()\n",
    "    )\n",
    "    for g in groups:\n",
    "        sub = df[df[group_col] == g]\n",
    "        res = paired_t_from_samples(sub[a_col].values, sub[b_col].values, alpha=alpha, two_sided=two_sided)\n",
    "        row = {\"group\": g}; row.update(res)\n",
    "        results.append(row)\n",
    "    out = pd.DataFrame(results)\n",
    "    # If some groups in order had no rows, fill with NaNs already produced by the function (n=0)\n",
    "    return out\n",
    "\n",
    "# ---------------- Example usage (REPLACE with your real columns) ----------------\n",
    "# df = ...  # cleaned returns with columns: date, weekday, r_overnight, r_intraday, ...\n",
    "#\n",
    "# # Overall paired test (all dates):\n",
    "# overall = paired_t_from_samples(df[\"r_overnight\"], df[\"r_intraday\"], alpha=0.05, two_sided=True)\n",
    "# print(\"Overnight vs Intraday (overall):\", overall)\n",
    "#\n",
    "# # By-weekday paired tests (to annotate 'Overnight vs Intraday by weekday' bars):\n",
    "# wd_order = [\"Mon\",\"Tue\",\"Wed\",\"Thu\",\"Fri\"]\n",
    "# by_wd = paired_tests_by_group(df, group_col=\"weekday\",\n",
    "#                               a_col=\"r_overnight\", b_col=\"r_intraday\",\n",
    "#                               order=wd_order, alpha=0.05, two_sided=True)\n",
    "# display(by_wd)\n",
    "#\n",
    "# # If doing H1 vs H2 paired by year (per-year means), first build per-year pairs\n",
    "# # year_means = df.groupby([\"year\",\"half\"], as_index=False)[\"r\"].mean().pivot(index=\"year\", columns=\"half\", values=\"r\")\n",
    "# # h1 = year_means[\"H1\"].values; h2 = year_means[\"H2\"].values\n",
    "# # print(paired_t_from_samples(h2, h1, alpha=0.05, two_sided=True))  # (H2 - H1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' From Discretization Notes (3-4-5 rule), adapted for calendar-effects cross-sections:\n",
    "#     - three_four_five_bins(x): bin a numeric control (e.g., log_size, volatility) into natural intervals\n",
    "#     - bucket_diff_bar(...): per-bucket mean difference (group A vs group B) with 95% CI bars\n",
    "#   Use cases:\n",
    "#     * ToM premium by SIZE bucket (bars with CI, t/p optional)\n",
    "#     * Weekday mean by VOLATILITY bucket (5 bars × 5 buckets) or heatmap\n",
    "#   Plotting: matplotlib only.\n",
    "# '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _rule_edges(n_msd: int, lo: float, hi: float) -> list[float]:\n",
    "    \"\"\"Return bin edges per 3-4-5 rule for integer n_msd in {1..10}.\"\"\"\n",
    "    width = (hi - lo)\n",
    "    if n_msd in (3, 6, 9):\n",
    "        k = 3; gaps = [i*width/k for i in range(k+1)]\n",
    "        return [lo + g for g in gaps]\n",
    "    if n_msd == 7:\n",
    "        # 2-3-2 split\n",
    "        g = width / 7.0\n",
    "        return [lo, lo+2*g, lo+5*g, hi]\n",
    "    if n_msd in (2, 4, 8):\n",
    "        k = 4; gaps = [i*width/k for i in range(k+1)]\n",
    "        return [lo + g for g in gaps]\n",
    "    if n_msd in (1, 5, 10):\n",
    "        k = 5; gaps = [i*width/k for i in range(k+1)]\n",
    "        return [lo + g for g in gaps]\n",
    "    # Fallback: single bin\n",
    "    return [lo, hi]\n",
    "\n",
    "def three_four_five_bins(x: pd.Series, trim_pct: float = 0.0, label_prefix: str = \"\") -> pd.Categorical:\n",
    "    \"\"\"\n",
    "    Discretize numeric Series x using the 3-4-5 rule.\n",
    "    - trim_pct: e.g., 0.05 to drop bottom/top 5% before setting the outer edges (default 0 for cleaned data)\n",
    "    - returns a pandas.Categorical with (a, b] style labels and stores edges in .attrs['edges']\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(x, errors=\"coerce\")\n",
    "    s_valid = s.dropna()\n",
    "    if trim_pct > 0:\n",
    "        lo_q = s_valid.quantile(trim_pct)\n",
    "        hi_q = s_valid.quantile(1 - trim_pct)\n",
    "        s_clip = s_valid[(s_valid >= lo_q) & (s_valid <= hi_q)]\n",
    "    else:\n",
    "        s_clip = s_valid\n",
    "\n",
    "    if s_clip.empty:\n",
    "        return pd.Categorical([np.nan]*len(s), categories=[])\n",
    "\n",
    "    # Most significant digit scale\n",
    "    max_abs = float(np.max(np.abs([s_clip.min(), s_clip.max()])))\n",
    "    j = int(math.floor(math.log10(max_abs))) if max_abs > 0 else 0\n",
    "    scale = 10.0**j\n",
    "\n",
    "    lo = math.floor(s_clip.min() / scale) * scale\n",
    "    hi = math.ceil(s_clip.max() / scale) * scale\n",
    "    n_msd = int(round((hi - lo) / scale))\n",
    "    n_msd = max(1, min(10, n_msd))  # clamp\n",
    "\n",
    "    edges = _rule_edges(n_msd, lo, hi)\n",
    "    # Ensure strictly increasing edges\n",
    "    edges = [edges[0]] + [edges[i] if edges[i] > edges[i-1] else edges[i-1] + np.finfo(float).eps\n",
    "                          for i in range(1, len(edges))]\n",
    "\n",
    "    cats = pd.cut(s, bins=edges, right=True, include_lowest=False)\n",
    "    if label_prefix:\n",
    "        cats = cats.rename_categories(lambda lab: f\"{label_prefix} {lab}\")\n",
    "    # keep edges for reuse\n",
    "    cats = cats.set_attrs({\"edges\": edges})\n",
    "    return cats\n",
    "\n",
    "def _mean_ci(a: np.ndarray, alpha: float = 0.05):\n",
    "    a = a[np.isfinite(a)]\n",
    "    n = a.size\n",
    "    m = float(np.mean(a)) if n else np.nan\n",
    "    if n <= 1:\n",
    "        return m, (np.nan, np.nan), n\n",
    "    s = float(np.std(a, ddof=1))\n",
    "    tcrit = stats.t.ppf(1 - alpha/2, df=n-1)\n",
    "    half = tcrit * s / np.sqrt(n)\n",
    "    return m, (m - half, m + half), n\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def bucket_diff_bar(\n",
    "    df: pd.DataFrame,\n",
    "    bucket_col: str,\n",
    "    value_col: str,\n",
    "    group_col: str,\n",
    "    group_a, group_b,\n",
    "    alpha: float = 0.05,\n",
    "    title: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    For each bucket, compute mean(value | group==A) - mean(value | group==B) with 95% CI (Welch).\n",
    "    Displays a bar chart with CI whiskers; returns a tidy results DataFrame.\n",
    "    \"\"\"\n",
    "    buckets = df[bucket_col].cat.categories if isinstance(df[bucket_col].dtype, pd.CategoricalDtype) else sorted(df[bucket_col].dropna().unique())\n",
    "    rows = []\n",
    "    diffs, ci_l, ci_u, labels = [], [], [], []\n",
    "\n",
    "    for b in buckets:\n",
    "        sub = df[df[bucket_col] == b]\n",
    "        a = sub.loc[sub[group_col] == group_a, value_col].to_numpy(dtype=float)\n",
    "        bvals = sub.loc[sub[group_col] == group_b, value_col].to_numpy(dtype=float)\n",
    "\n",
    "        # Welch stats\n",
    "        na, nb = len(a), len(bvals)\n",
    "        ma, mb = np.mean(a) if na else np.nan, np.mean(bvals) if nb else np.nan\n",
    "        va, vb = np.var(a, ddof=1) if na > 1 else np.nan, np.var(bvals, ddof=1) if nb > 1 else np.nan\n",
    "        se = np.sqrt(va/na + vb/nb) if na and nb and na>1 and nb>1 else np.nan\n",
    "        t = (ma - mb) / se if se and np.isfinite(se) else np.nan\n",
    "        df_w = (va/na + vb/nb)**2 / ((va**2)/(na**2*(na-1)) + (vb**2)/(nb**2*(nb-1))) if na>1 and nb>1 else np.nan\n",
    "        p = 2*(1 - stats.t.cdf(np.abs(t), df=df_w)) if np.isfinite(t) and np.isfinite(df_w) else np.nan\n",
    "\n",
    "        # CI for difference\n",
    "        if na>1 and nb>1 and np.isfinite(df_w):\n",
    "            tcrit = stats.t.ppf(1 - alpha/2, df=df_w)\n",
    "            half = tcrit * se\n",
    "            ci = ( (ma-mb) - half, (ma-mb) + half )\n",
    "        else:\n",
    "            ci = (np.nan, np.nan)\n",
    "\n",
    "        rows.append({\n",
    "            \"bucket\": str(b),\n",
    "            \"n_A\": int(na), \"mean_A\": float(ma),\n",
    "            \"n_B\": int(nb), \"mean_B\": float(mb),\n",
    "            \"diff_A_minus_B\": float(ma - mb),\n",
    "            \"ci_low\": float(ci[0]), \"ci_high\": float(ci[1]),\n",
    "            \"t\": float(t) if np.isfinite(t) else np.nan,\n",
    "            \"df\": float(df_w) if np.isfinite(df_w) else np.nan,\n",
    "            \"p\": float(p) if np.isfinite(p) else np.nan\n",
    "        })\n",
    "\n",
    "        labels.append(str(b))\n",
    "        diffs.append(ma - mb)\n",
    "        ci_l.append((ma - mb) - ci[0] if np.isfinite(ci[0]) else 0.0)\n",
    "        ci_u.append(ci[1] - (ma - mb) if np.isfinite(ci[1]) else 0.0)\n",
    "\n",
    "    res = pd.DataFrame(rows)\n",
    "\n",
    "    # Plot\n",
    "    x = np.arange(len(labels))\n",
    "    fig, ax = plt.subplots()\n",
    "    bars = ax.bar(x, diffs, yerr=[ci_l, ci_u], capsize=4)\n",
    "    ax.set_xticks(x); ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(f\"Mean({value_col} | {group_col}={group_a}) - Mean({value_col} | {group_col}={group_b})\")\n",
    "    ax.set_title(title or f\"Difference by {bucket_col}\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "    return res\n",
    "\n",
    "# ---------------- Example usage (REPLACE with your real DataFrame/columns) ----------------\n",
    "# df = ...  # cleaned returns with columns like: r, ToM (bool or {1,0}), weekday, log_size, volatility_20d, ...\n",
    "#\n",
    "# # A) Create SIZE buckets via 3-4-5 rule (no trimming since data are cleaned):\n",
    "# df[\"size_bucket\"] = three_four_five_bins(df[\"log_size\"], trim_pct=0.0, label_prefix=\"Size\")\n",
    "#\n",
    "# # B) ToM premium by SIZE bucket (A=ToM, B=non-ToM):\n",
    "# tom_by_size = bucket_diff_bar(df, bucket_col=\"size_bucket\", value_col=\"r\",\n",
    "#                               group_col=\"ToM\", group_a=1, group_b=0,\n",
    "#                               title=\"ToM premium by size bucket (Δ ±95% CI)\")\n",
    "# # C) Weekday mean by VOLATILITY bucket can be built by running bucketed means per weekday, or a heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' VIF visual: compact bar chart to accompany compute_vif_from_formula(...)\n",
    "#     - Input: the DataFrame returned by compute_vif_from_formula(formula, df)\n",
    "#     - Purpose: quickly spot multicollinearity (e.g., VIF > 5 or > 10) in our calendar-dummy OLS\n",
    "#     - Plotting: matplotlib only (no seaborn)\n",
    "# '''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_vif_bar(vif_df: pd.DataFrame, threshold: float = 5.0, top_n: int | None = None, title: str | None = None):\n",
    "    \"\"\"\n",
    "    Plot VIF values as a horizontal bar chart.\n",
    "    Parameters\n",
    "    ----------\n",
    "    vif_df : DataFrame with columns ['term','VIF'] as returned by compute_vif_from_formula(...)\n",
    "    threshold : float, draw a vertical reference line at this VIF (e.g., 5 or 10)\n",
    "    top_n : show only the top_n terms by VIF (None = all)\n",
    "    title : optional title\n",
    "    \"\"\"\n",
    "    df_plot = vif_df.copy()\n",
    "    if top_n is not None:\n",
    "        df_plot = df_plot.sort_values(\"VIF\", ascending=False).head(top_n)\n",
    "    else:\n",
    "        df_plot = df_plot.sort_values(\"VIF\", ascending=True)\n",
    "\n",
    "    terms = df_plot[\"term\"].tolist()\n",
    "    vals = df_plot[\"VIF\"].to_numpy()\n",
    "\n",
    "    y = np.arange(len(terms))\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.barh(y, vals)\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(terms)\n",
    "    ax.set_xlabel(\"VIF\")\n",
    "    ax.set_title(title or \"Variance Inflation Factor (VIF)\")\n",
    "    ax.axvline(threshold, linestyle=\"--\")\n",
    "    # optional annotate numeric values\n",
    "    for yi, v in zip(y, vals):\n",
    "        ax.text(v, yi, f\" {v:.2f}\", va=\"center\", ha=\"left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    return df_plot\n",
    "\n",
    "# ---------------- Example usage (hooks into your existing helper) ----------------\n",
    "# formula = \"r ~ C(weekday) + ToM + C(month) + log_size + C(sector)\"\n",
    "# vif_tbl = compute_vif_from_formula(formula, df)   # from your MLR-Practice helper\n",
    "# plot_vif_bar(vif_tbl, threshold=5.0, title=\"VIF (calendar-dummy OLS)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
