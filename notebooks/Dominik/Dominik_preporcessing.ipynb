{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "So take top 10 companies by sector instead of all 8800 thousand\n",
    "(DONE)\n",
    "\n",
    ":\n",
    "(pre-processing) do on all data before 2020 Do all reproducing from lecturers after cleaning check for correlations (Bar chart all this):\n",
    "            Simple linear regression (DONE)\n",
    "            Multi linear regressions (DONE)\n",
    "            logistic Regression (Sergei Did this so copy sergei code) (DONE)\n",
    "            Missing values see if there is any missing values still (DONE)\n",
    "            Outlier detection (3 std deviation) (DONE)\n",
    "            Data Transformation (We already did data normalisation), \n",
    "                                    we need to do discretisation (DONE)\n",
    "            Feature Enginnering we need to remove inflation adjsuted clsoign prices (already done)\n",
    "\n",
    "When you split the data make sure you split from 2008 to 2020 is training data, then 2020 - now is training data\n",
    "top 10 comapnies per sector right now\n",
    "(DONE)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Energy,XOM,CVX,COP,WMB,EPD,MPC,ET,MPLX,PSX,VLO\n",
    "Materials,LIN,SCCO,SHW,ECL,CRH,FCX,APD,CTVA,AU,VMC\n",
    "Industrial,GE,CAT,RTX,GEV,BA,UNP,ETN,DE,HON,LMT\n",
    "Consumer discretionary,AMZN,TSLA,BABA,HD,TM,MCD,PDD,TJX,BKNG,LOW\n",
    "Consumer staples,WMT,COST,PG,KO,PM,PEP,UL,BTI,BUD,MO\n",
    "Health care,LLy,JNJ,ABBV,UNH,MRK,ABT,ISRG,AMGN,DHR,GILD\n",
    "Financials,JPM,V,BAC,GS MS,WFC,C,AXP,RY\n",
    "Information technology,AAPL,MSFT,GOOG,AVGO,TSMC,ORCL,PLTR,AMD,CSCO,MU\n",
    "Communication,GOOG,META,NFLX,TMUS,DIS,APP,T,VZ,SPOT,CMCSA\n",
    "Utilities,CEG,SO,DUK,AEP,VST,SRE,D,XEL,EXC,ETR\n",
    "Real estate,PLD,AMT,EQIX,SPG,DLR,O,PSA,CBRE,CCI,VTR\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning pipeline\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov 20 17:48:05 2025\n",
    "\n",
    "@author: Enrico\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from pandas_datareader.data import DataReader\n",
    "\n",
    "\n",
    "def build_market_info():\n",
    "    raw = [\n",
    "        (\"BITCOIN\", \"Digital Currencies\"),\n",
    "        (\"GOLD\", \"Commodities\"),\n",
    "        (\"SILVER\", \"Commodities\"),\n",
    "        (\"OIL\", \"Commodities\"),\n",
    "        (\"VIX\", \"Volatility Index\"),\n",
    "        (\"FTSE100\", \"Equities\"),\n",
    "        (\"DAX\", \"Equities\"),\n",
    "        (\"CAC40\", \"Equities\"),\n",
    "        (\"NIKKEI\", \"Equities\"),\n",
    "        (\"SP500\", \"Equities\"),\n",
    "        (\"NASDAQ\", \"Equities\"),\n",
    "        (\"REIT_USA\", \"Real Estate Investment Trusts (REIT)\"),\n",
    "        (\"REIT_EUROPE\", \"Real Estate Investment Trusts (REIT)\"),\n",
    "        (\"REIT_ASIA\", \"Real Estate Investment Trusts (REIT)\")\n",
    "    ]\n",
    "    df = pd.DataFrame(raw, columns=[\"symbol (acronym)\", \"market_group\"])\n",
    "    df[\"Currency\"] = \"USD\"\n",
    "    df.loc[df[\"symbol (acronym)\"] == \"DAX\", \"Currency\"] = \"EUR\"\n",
    "    df.loc[df[\"symbol (acronym)\"] == \"CAC40\", \"Currency\"] = \"EUR\"\n",
    "    return df\n",
    "\n",
    "market_info = build_market_info()\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"Normalize column names: lowercase, strip spaces, replace non-alphanum with underscores.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r'\\s+', '_', regex=True)\n",
    "                  .str.replace(r'[^\\w]', '', regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def _dedupe_ohlcv(df):\n",
    "    \"\"\"\n",
    "    Remove duplicate dates in OHLCV data, keeping the last occurrence.\n",
    "    Assumes 'date' column exists.\n",
    "    \"\"\"\n",
    "    if 'date' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have a 'date' column\")\n",
    "    df = df.sort_values('date')  # ensure chronological order\n",
    "    df = df.drop_duplicates(subset='date', keep='last')\n",
    "    df = df.set_index('date')\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_exchange_rate(currency, start_date, end_date):\n",
    "    \"\"\"Get daily exchange rate to USD.\"\"\"\n",
    "    if currency.upper() == 'USD':\n",
    "        df = pd.DataFrame({\n",
    "            'Date': pd.date_range(start=start_date, end=end_date),\n",
    "            'Rate_to_USD': 1.0\n",
    "        })\n",
    "        return df\n",
    "    fx_map = {\n",
    "        'EUR': ('EURUSD=X', 1),\n",
    "        'GBP': ('GBPUSD=X', 1),\n",
    "        'AUD': ('AUDUSD=X', 1),\n",
    "        'NZD': ('NZDUSD=X', 1),\n",
    "        'CAD': ('USDCAD=X', -1),\n",
    "        'JPY': ('USDJPY=X', -1),\n",
    "        'CHF': ('USDCHF=X', -1),\n",
    "        'CNY': ('USDCNY=X', -1),\n",
    "        'HKD': ('USDHKD=X', -1),\n",
    "        'SEK': ('USDSEK=X', -1),\n",
    "        'NOK': ('USDNOK=X', -1),\n",
    "        'SGD': ('USDSGD=X', -1)\n",
    "    }\n",
    "\n",
    "    cur = currency.upper()\n",
    "    if cur in fx_map:\n",
    "        symbol, invert = fx_map[cur]\n",
    "    else:\n",
    "        symbol, invert = (f\"{cur}USD=X\", 1)  # fallback\n",
    "\n",
    "    data = yf.download(symbol, start=start_date, end=end_date)[['Close']].reset_index()\n",
    "    data = data.rename(columns={'Close': 'Rate_to_USD'})\n",
    "    if invert == -1:\n",
    "        data['Rate_to_USD'] = 1 / data['Rate_to_USD']\n",
    "    return data[['Date','Rate_to_USD']]\n",
    "\n",
    "def convert_to_usd(df, currency, exchange_rates):\n",
    "    \"\"\"\n",
    "    Merge df with exchange rates and compute Value_USD.\n",
    "    Ensures no MultiIndex issues.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Reset index if it's a MultiIndex\n",
    "    if isinstance(df.index, pd.MultiIndex):\n",
    "        df = df.reset_index()\n",
    "\n",
    "    # Ensure 'Date' column exists for merge\n",
    "    if 'Date' not in df.columns:\n",
    "        df['Date'] = df.index\n",
    "\n",
    "    df['Date'] = pd.to_datetime(df['Date']).dt.tz_localize(None)\n",
    "    exchange_rates['Date'] = pd.to_datetime(exchange_rates['Date']).dt.tz_localize(None)\n",
    "\n",
    "    merged = pd.merge(df, exchange_rates, on='Date', how='left')\n",
    "\n",
    "    # Use 'close' if exists, otherwise fallback to first numeric column\n",
    "    if 'close' in merged.columns:\n",
    "        merged['Value_USD'] = merged['close'] * merged['Rate_to_USD']\n",
    "    else:\n",
    "        first_numeric = merged.select_dtypes(include='number').columns[0]\n",
    "        merged['Value_USD'] = merged[first_numeric] * merged['Rate_to_USD']\n",
    "\n",
    "    merged = merged.set_index('Date')\n",
    "    return merged\n",
    "\n",
    "\n",
    "def get_inflation_index(start_date, end_date):\n",
    "    \"\"\"Fetch daily interpolated US CPI from FRED.\"\"\"\n",
    "    try:\n",
    "        cpi = DataReader('CPIAUCNS', 'fred', start_date - pd.DateOffset(days=31), end_date)\n",
    "    except Exception as e:\n",
    "        print(\"FRED CPI fetch failed:\", e)\n",
    "        return pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date), 'CPI_USD': np.nan})\n",
    "\n",
    "    cpi = cpi.reset_index().rename(columns={'DATE':'Date','CPIAUCNS':'CPI_USD'})\n",
    "    cpi['Date'] = pd.to_datetime(cpi['Date'])\n",
    "\n",
    "    daily_index = pd.DataFrame({'Date': pd.date_range(start=start_date, end=end_date)})\n",
    "    daily_index = pd.merge_asof(daily_index.sort_values('Date'),\n",
    "                                cpi.sort_values('Date'),\n",
    "                                on='Date',\n",
    "                                direction='backward')\n",
    "    daily_index['CPI_USD'] = daily_index['CPI_USD'].ffill().bfill().interpolate(method='linear')\n",
    "    return daily_index\n",
    "\n",
    "def adjust_for_inflation(df, inflation_index):\n",
    "    df = df.copy()\n",
    "    df['Date'] = df.index\n",
    "    inflation_index['Date'] = pd.to_datetime(inflation_index['Date']).dt.tz_localize(None)\n",
    "    merged = df.merge(inflation_index, on='Date', how='left')\n",
    "    latest_cpi = inflation_index['CPI_USD'].iloc[-1]\n",
    "    merged['real_close'] = merged['Value_USD'] * (latest_cpi / merged['CPI_USD'])\n",
    "    merged = merged.set_index('Date')\n",
    "    return merged\n",
    "\n",
    "def euclidean_vector(a, b):\n",
    "    a = np.array(a, dtype=float)\n",
    "    b = np.array(b, dtype=float)\n",
    "    if a.shape != b.shape:\n",
    "        raise ValueError(\"Vectors must be same length\")\n",
    "    return float(np.sqrt(np.sum((a - b) ** 2)))\n",
    "\n",
    "def chi_square_test(observed, expected):\n",
    "    observed = np.array(observed, dtype=float)\n",
    "    expected = np.array(expected, dtype=float)\n",
    "    if observed.shape != expected.shape:\n",
    "        raise ValueError(\"Length mismatch\")\n",
    "    if np.any(expected == 0):\n",
    "        raise ValueError(\"Zero expected freq not allowed\")\n",
    "    chi_sq = np.sum((observed - expected) ** 2 / expected)\n",
    "    return chi_sq\n",
    "\n",
    "def process_asset_file(filepath, asset_name, market_info):\n",
    "    \"\"\"\n",
    "    Process a single asset CSV file:\n",
    "    - Clean columns and normalize names\n",
    "    - Ensure date column exists\n",
    "    - Deduplicate OHLCV data\n",
    "    - Convert to USD if necessary\n",
    "    - Adjust for inflation\n",
    "    \"\"\"\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(filepath, on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "    # Normalize columns\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    # Find date-like column\n",
    "    date_col = next((c for c in df.columns if \"date\" in c or \"time\" in c), None)\n",
    "    if date_col is None:\n",
    "        raise ValueError(f\"No date-like column found in {filepath}\")\n",
    "\n",
    "    # Rename to 'date'\n",
    "    if date_col != \"date\":\n",
    "        df = df.rename(columns={date_col: \"date\"})\n",
    "\n",
    "    # Convert date to datetime\n",
    "    df['date'] = pd.to_datetime(df['date'], errors=\"coerce\")\n",
    "    df = df.dropna(subset=['date'])\n",
    "    df = df.sort_values('date')\n",
    "\n",
    "    # Deduplicate OHLCV\n",
    "    df = _dedupe_ohlcv(df)\n",
    "\n",
    "    # Skip if empty\n",
    "    if df.empty:\n",
    "        print(f\"Skipping {asset_name}: no valid dates after cleaning\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Get currency\n",
    "    row = market_info.loc[market_info['symbol (acronym)'] == asset_name]\n",
    "    currency = row['Currency'].iloc[0] if len(row) else 'USD'\n",
    "\n",
    "    # Convert to USD if needed\n",
    "    if currency != \"USD\":\n",
    "        fx = get_exchange_rate(currency, df.index.min(), df.index.max())\n",
    "        df = convert_to_usd(df, currency, fx)\n",
    "    else:\n",
    "        df['Value_USD'] = df.get('close', np.nan)\n",
    "\n",
    "    # Inflation adjustment\n",
    "    inflation_index = get_inflation_index(df.index.min(), df.index.max())\n",
    "    df = adjust_for_inflation(df, inflation_index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def looks_like_returns_matrix(df):\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        try: df.index = pd.to_datetime(df.index)\n",
    "        except: return False\n",
    "    if not df.apply(lambda s: pd.api.types.is_numeric_dtype(s)).all():\n",
    "        return False\n",
    "    if not df.columns.str.contains(r\"^\\d\").any():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_edhec_returns(df, source):\n",
    "    df = df.copy()\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.sort_index()\n",
    "    df = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def clean_names_and_dtypes(df, source, symbol_hint=None):\n",
    "    df = normalize_columns(df)\n",
    "    if \"date\" in df.columns:\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\")\n",
    "        df = df.dropna(subset=[\"date\"])\n",
    "    return df\n",
    "\n",
    "def clean_any(df, source, symbol_hint=None):\n",
    "    if looks_like_returns_matrix(df.rename(columns=str.lower)):\n",
    "        return clean_edhec_returns(df, source)\n",
    "    return clean_names_and_dtypes(df, source, symbol_hint)\n",
    "\n",
    "def calendar_align(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # force datetime index\n",
    "    df.index = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    df = df[~df.index.isna()]\n",
    "\n",
    "    # localize/convert to UTC\n",
    "    try:\n",
    "        df.index = df.index.tz_localize(\"UTC\")\n",
    "    except TypeError:\n",
    "        df.index = df.index.tz_convert(\"UTC\")\n",
    "\n",
    "    return df\n",
    "\n",
    "def process_folder(path, market_info):\n",
    "    results = {}\n",
    "    for file in os.listdir(path):\n",
    "        if not file.lower().endswith(\".csv\"): continue\n",
    "        symbol = os.path.splitext(file)[0].upper()\n",
    "        df = process_asset_file(os.path.join(path, file), symbol, market_info)\n",
    "        df = calendar_align(df)\n",
    "        results[symbol] = df\n",
    "    return results\n",
    "\n",
    "def summarize_data(df, symbol=None):\n",
    "    summary = pd.DataFrame({\n",
    "        \"first_date\": [df.index.min()],\n",
    "        \"last_date\": [df.index.max()],\n",
    "        \"num_rows\": [len(df)],\n",
    "        \"num_missing\": [df.isna().sum().sum()],\n",
    "        \"columns\": [\", \".join(df.columns)]\n",
    "    })\n",
    "    if symbol:\n",
    "        print(f\"Summary for {symbol}:\")\n",
    "    print(summary)\n",
    "    return summary\n",
    "\n",
    "def plot_time_series(df, column=\"close\", symbol=None):\n",
    "    # Plot original vs real_close\n",
    "    if \"close\" in df.columns and \"real_close\" in df.columns:\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.plot(df.index, df[\"close\"], label=\"Original Close / AdjClose\", alpha=0.8)\n",
    "        plt.plot(df.index, df[\"real_close\"], label=\"Inflation Adjusted Close\", alpha=0.8)\n",
    "        plt.title(f\"{symbol} Close vs Inflation-Adjusted Close\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Price\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"{symbol}: missing 'close' or 'real_close', cannot plot comparison\")\n",
    "\n",
    "def process_folder_using_dictionary(path, market_info, dictionary_csv):\n",
    "\n",
    "    # Read dictionary\n",
    "    dict_df = pd.read_csv(dictionary_csv, header=None)\n",
    "\n",
    "    # Extract all tickers (columns 1..end)\n",
    "    all_symbols = set()\n",
    "\n",
    "    for _, row in dict_df.iterrows():\n",
    "        # skip the first entry (category)\n",
    "        symbols = row.dropna().tolist()[1:]\n",
    "        symbols = [str(s).strip().upper() for s in symbols]\n",
    "        all_symbols.update(symbols)\n",
    "\n",
    "    # Convert symbols → filenames\n",
    "    allowed_files = {f\"{symbol}.csv\" for symbol in all_symbols}\n",
    "\n",
    "    print(\"Allowed files:\", allowed_files)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for file in os.listdir(path):\n",
    "        if file not in allowed_files:\n",
    "            continue  # skip files not in dictionary\n",
    "\n",
    "        symbol = os.path.splitext(file)[0].upper()\n",
    "        full_path = os.path.join(path, file)\n",
    "\n",
    "        print(f\"Processing {symbol} ...\")\n",
    "\n",
    "        df = process_asset_file(full_path, symbol, market_info)\n",
    "        if df.empty:\n",
    "            print(f\"Skipping {symbol}: empty after cleaning (not saving).\")\n",
    "            continue\n",
    "        df = calendar_align(df)\n",
    "        results[symbol] = df\n",
    "\n",
    "    return results\n",
    "\n",
    "folder_path = os.path.expanduser(\"~/csc1171/data/raw/SP500_ETF_FX_Crypto_Daily\")\n",
    "\n",
    "dictionary_path = os.path.expanduser(\"~/csc1171/notebooks/Dominik/company_dictionary.csv\")\n",
    "\n",
    "\n",
    "cleaned_data = process_folder_using_dictionary(\n",
    "    folder_path,\n",
    "    market_info,\n",
    "    dictionary_path\n",
    ")\n",
    "\n",
    "\n",
    "for symbol, df in cleaned_data.items():\n",
    "    print(symbol, df.head())\n",
    "\n",
    "for symbol, df in cleaned_data.items():\n",
    "    summarize_data(df, symbol)\n",
    "    plot_time_series(df, \"real_close\", symbol)\n",
    "\n",
    "\n",
    "output_folder = os.path.expanduser(\"~/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)  # create folder if it doesn't exist\n",
    "\n",
    "for symbol, df in cleaned_data.items():\n",
    "\n",
    "    full_path = os.path.join(output_folder, f\"{symbol}_cleaned_full.csv\")\n",
    "    df.to_csv(full_path)\n",
    "    \n",
    "    df_2008_2020 = df.loc[(df.index >= \"2008-01-01\") & (df.index <= \"2020-12-31\")]\n",
    "    path_2008_2020 = os.path.join(output_folder, f\"{symbol}_cleaned_2008_2020.csv\")\n",
    "    df_2008_2020.to_csv(path_2008_2020)\n",
    "\n",
    "    df_2020_onward = df.loc[df.index >= \"2020-01-01\"]\n",
    "    path_2020_onward = os.path.join(output_folder, f\"{symbol}_cleaned_2020_onward.csv\")\n",
    "    df_2020_onward.to_csv(path_2020_onward)\n",
    "\n",
    "    print(f\"Saved 3 versions for {symbol}\")\n",
    "\n",
    "\n",
    "for symbol, df in cleaned_data.items():\n",
    "    # Save each cleaned DataFrame as CSV\n",
    "    output_path = os.path.join(output_folder, f\"{symbol}_cleaned.csv\")\n",
    "    df.to_csv(output_path)\n",
    "    print(f\"Saved cleaned data for {symbol} to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Simple Linear Regression: pure secular drift in real terms\n",
    "import glob, pandas as pd, numpy as np, statsmodels.api as sm\n",
    "\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "y_col = \"real_close\"\n",
    "results = []\n",
    "\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    if y_col not in df.columns:\n",
    "        continue\n",
    "    dfi = df[[y_col]].dropna().copy()\n",
    "    if dfi.empty:\n",
    "        continue\n",
    "\n",
    "    dfi[\"t\"] = (dfi.index - dfi.index.min()).days\n",
    "    X = sm.add_constant(dfi[\"t\"])\n",
    "    y = dfi[y_col].astype(float)\n",
    "\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    ticker = path.split(\"/\")[-1].replace(\"_cleaned.csv\", \"\")\n",
    "\n",
    "    results.append({\n",
    "        \"ticker\": ticker,\n",
    "        \"beta_intercept\": res.params.get(\"const\", np.nan),\n",
    "        \"beta_slope\": res.params.get(\"t\", np.nan),\n",
    "        \"p_value_slope\": res.pvalues.get(\"t\", np.nan),\n",
    "        \"R2\": res.rsquared,\n",
    "        \"Adj_R2\": res.rsquared_adj,\n",
    "        \"AIC\": res.aic,\n",
    "        \"BIC\": res.bic,\n",
    "        \"n\": int(res.nobs)\n",
    "    })\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "reg_table = pd.DataFrame(results).sort_values(\"p_value_slope\")\n",
    "reg_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MLR: drift net of same-day price levels/volume.\n",
    "\n",
    "'''\n",
    "How much did each stock’s price linearly trend upward or downward between 2008–2020?\n",
    "'''\n",
    "\n",
    "\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "y_col = \"real_close\"\n",
    "\n",
    "candidate_predictors = [\"volume\"]\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    if y_col not in df.columns:\n",
    "        continue\n",
    "    # build X from whatever predictors exist in this file\n",
    "    preds_use = [c for c in candidate_predictors if c in df.columns and c != y_col]\n",
    "    if len(preds_use) < 1:\n",
    "        continue\n",
    "    \n",
    "    dfi = df[[y_col] + preds_use].dropna().copy()\n",
    "    dfi[\"t\"] = (dfi.index - dfi.index.min()).days\n",
    "    preds_use = preds_use + [\"t\"]\n",
    "\n",
    "    if dfi.empty:\n",
    "        continue\n",
    "    \n",
    "    X = sm.add_constant(dfi[preds_use].astype(float))\n",
    "    y = dfi[y_col].astype(float)\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    coef_tbl = pd.DataFrame({\n",
    "        \"term\": res.params.index,\n",
    "        \"estimate\": res.params.values,\n",
    "        \"std_error\": res.bse.values,\n",
    "        \"t_value\": res.tvalues.values,\n",
    "        \"p_value\": res.pvalues.values\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "    ticker = os.path.basename(path).replace(\"_cleaned_2008_2020.csv\", \"\")\n",
    "\n",
    "    results.append({\n",
    "        \"ticker\": ticker,\n",
    "        \"beta_intercept\": res.params.get(\"const\", np.nan),\n",
    "        \"beta_slope\": res.params.get(\"t\", np.nan),\n",
    "        \"p_value_slope\": res.pvalues.get(\"t\", np.nan),\n",
    "        \"R2\": res.rsquared,\n",
    "        \"Adj_R2\": res.rsquared_adj,\n",
    "        \"AIC\": res.aic,\n",
    "        \"BIC\": res.bic,\n",
    "        \"n\": int(res.nobs)\n",
    "    })\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "reg_table = pd.DataFrame(results).sort_values(\"p_value_slope\")\n",
    "reg_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# logistic Regression (Sergei Did this so copy sergei code)\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError\n",
    "\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def make_calendar_features(df):\n",
    "    \"\"\"Create the calendar columns used in your snippet.\"\"\"\n",
    "    idx = df.index\n",
    "    out = pd.DataFrame(index=idx)\n",
    "    out[\"DayOfYear\"]   = idx.dayofyear.astype(int)\n",
    "    out[\"IsMonthStart\"]= idx.is_month_start.astype(int)\n",
    "    out[\"IsJanuary\"]   = (idx.month == 1).astype(int)\n",
    "    out[\"IsWeekend\"]   = (idx.weekday >= 5).astype(int)  # you can ignore this if not needed\n",
    "    return out\n",
    "\n",
    "def safe_pct_change(s):\n",
    "    r = s.pct_change()\n",
    "    return r.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "results = []\n",
    "\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    if \"real_close\" not in df.columns:\n",
    "        continue  # nothing to model\n",
    "\n",
    "    # --- target: daily return sign ---\n",
    "    df[\"Return\"] = safe_pct_change(df[\"real_close\"])\n",
    "    feats = make_calendar_features(df)\n",
    "\n",
    "    # keep exactly your chosen predictors\n",
    "    X_logit = feats[[\"DayOfYear\", \"IsMonthStart\", \"IsJanuary\"]].copy()\n",
    "    y = (df[\"Return\"] > 0).astype(int)\n",
    "\n",
    "    # align / drop NA\n",
    "    dfi = pd.concat([X_logit, y.rename(\"Return_Positive\")], axis=1).dropna()\n",
    "    if dfi.empty or dfi[\"Return_Positive\"].nunique() < 2:\n",
    "        # need both classes to fit logit\n",
    "        continue\n",
    "\n",
    "    X = sm.add_constant(dfi[[\"DayOfYear\", \"IsMonthStart\", \"IsJanuary\"]])\n",
    "    yb = dfi[\"Return_Positive\"]\n",
    "\n",
    "    try:\n",
    "        # Fit Logit (same as your code)\n",
    "        logit_model = sm.Logit(yb, X).fit(disp=False)\n",
    "\n",
    "        params = logit_model.params\n",
    "        pvals  = logit_model.pvalues\n",
    "\n",
    "        # predictions for a quick accuracy metric (optional)\n",
    "        phat = logit_model.predict(X)\n",
    "        pred = (phat >= 0.5).astype(int)\n",
    "        acc = (pred == yb).mean()\n",
    "\n",
    "        ticker = os.path.basename(path).replace(\"_cleaned_2008_2020.csv\", \"\")\n",
    "\n",
    "        results.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"coef_const\":        params.get(\"const\", np.nan),\n",
    "            \"coef_DayOfYear\":    params.get(\"DayOfYear\", np.nan),\n",
    "            \"coef_IsMonthStart\": params.get(\"IsMonthStart\", np.nan),\n",
    "            \"coef_IsJanuary\":    params.get(\"IsJanuary\", np.nan),\n",
    "            \"p_const\":           pvals.get(\"const\", np.nan),\n",
    "            \"p_DayOfYear\":       pvals.get(\"DayOfYear\", np.nan),\n",
    "            \"p_IsMonthStart\":    pvals.get(\"IsMonthStart\", np.nan),\n",
    "            \"p_IsJanuary\":       pvals.get(\"IsJanuary\", np.nan),\n",
    "            \"pseudoR2_McFadden\": logit_model.prsquared,\n",
    "            \"AIC\": logit_model.aic,\n",
    "            \"BIC\": logit_model.bic,\n",
    "            \"n\": int(logit_model.nobs),\n",
    "            \"accuracy@0.5\": float(acc)\n",
    "        })\n",
    "\n",
    "    except PerfectSeparationError:\n",
    "        print(f\"SKIP {os.path.basename(path)}: PerfectSeparationError\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        print(f\"SKIP {os.path.basename(path)}: {type(e).__name__}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "logit_table = pd.DataFrame(results)\n",
    "if logit_table.empty:\n",
    "    print(\"No models were fitted (results is empty) — likely because all rows were skipped.\")\n",
    "else:\n",
    "    logit_table = logit_table.sort_values(\"p_DayOfYear\")\n",
    "logit_table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values see if there is any missing values still\n",
    "\n",
    "# === Missingness check across all cleaned 2008–2020 files (lecture-style) ===\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    # keep it naive as in your earlier cells\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "per_file_rows = []\n",
    "per_file_missing = []     # long form: (ticker, column, n_missing, pct_missing)\n",
    "per_file_anyrow = []      # rows with ANY NaN\n",
    "key_cols = [\"real_close\", \"close\", \"adj_close\", \"open\", \"high\", \"low\", \"volume\", \"Value_USD\", \"CPI_USD\"]\n",
    "\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    ticker = os.path.basename(path).replace(\"_cleaned_2008_2020.csv\", \"\")\n",
    "\n",
    "    n = len(df)\n",
    "    per_file_rows.append({\"ticker\": ticker, \"n_rows\": n})\n",
    "\n",
    "    # count missing by column (restrict to columns present)\n",
    "    cols_here = [c for c in key_cols if c in df.columns]\n",
    "    miss_counts = df[cols_here].isna().sum().to_dict()\n",
    "\n",
    "    # store long-form missing counts and percents\n",
    "    for c in cols_here:\n",
    "        n_miss = int(miss_counts[c])\n",
    "        pct = (n_miss / n * 100.0) if n > 0 else np.nan\n",
    "        per_file_missing.append({\n",
    "            \"ticker\": ticker, \"column\": c,\n",
    "            \"n_missing\": n_miss, \"pct_missing\": pct\n",
    "        })\n",
    "\n",
    "    # rows with ANY NaN (across all columns, not just price columns)\n",
    "    any_na_count = int(df.isna().any(axis=1).sum())\n",
    "    any_na_pct = (any_na_count / n * 100.0) if n > 0 else np.nan\n",
    "    per_file_anyrow.append({\n",
    "        \"ticker\": ticker,\n",
    "        \"rows_with_any_na\": any_na_count,\n",
    "        \"pct_rows_with_any_na\": any_na_pct\n",
    "    })\n",
    "\n",
    "# === Summaries ===\n",
    "rows_tbl   = pd.DataFrame(per_file_rows).sort_values(\"ticker\").reset_index(drop=True)\n",
    "missing_tbl= pd.DataFrame(per_file_missing).sort_values([\"ticker\",\"column\"]).reset_index(drop=True)\n",
    "anyrow_tbl = pd.DataFrame(per_file_anyrow).sort_values(\"ticker\").reset_index(drop=True)\n",
    "\n",
    "print(\"=== Rows per file ===\")\n",
    "print(rows_tbl.head(10))\n",
    "print(f\"... total files: {len(rows_tbl)}\")\n",
    "\n",
    "print(\"\\n=== Missingness by file & column (first 20 rows) ===\")\n",
    "print(missing_tbl.head(20))\n",
    "\n",
    "print(\"\\n=== Rows containing ANY NaN per file (top 20) ===\")\n",
    "print(anyrow_tbl.head(20))\n",
    "\n",
    "# === Wide summary: one row per ticker, key columns as % missing (like describe in lectures) ===\n",
    "wide = (missing_tbl\n",
    "        .pivot(index=\"ticker\", columns=\"column\", values=\"pct_missing\")\n",
    "        .fillna(0.0)\n",
    "        .reset_index())\n",
    "\n",
    "print(\"\\n=== % missing by ticker (wide table, first 10) ===\")\n",
    "print(wide.head(10))\n",
    "\n",
    "# === Quick spotlight on the main target: real_close ===\n",
    "if \"real_close\" in missing_tbl[\"column\"].unique():\n",
    "    miss_real = (missing_tbl[missing_tbl[\"column\"]==\"real_close\"]\n",
    "                 .sort_values(\"pct_missing\", ascending=False)\n",
    "                 .reset_index(drop=True))\n",
    "    print(\"\\n=== Tickers with most missing in real_close ===\")\n",
    "    print(miss_real.head(20))\n",
    "\n",
    "# === Optional CSV exports for your report/appendix (comment out if not needed) ===\n",
    "OUT_DIR = os.path.join(CLEAN_DIR, \"_missingness_checks\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "rows_tbl.to_csv(os.path.join(OUT_DIR, \"rows_per_file.csv\"), index=False)\n",
    "missing_tbl.to_csv(os.path.join(OUT_DIR, \"missing_by_file_and_column.csv\"), index=False)\n",
    "anyrow_tbl.to_csv(os.path.join(OUT_DIR, \"rows_with_any_na.csv\"), index=False)\n",
    "wide.to_csv(os.path.join(OUT_DIR, \"pct_missing_wide.csv\"), index=False)\n",
    "\n",
    "print(f\"\\nSaved summaries to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Outlier detection (3 std deviation)\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def safe_pct_change(s):\n",
    "    r = s.pct_change()\n",
    "    return r.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def flag_outliers_3sigma(series):\n",
    "    \"\"\"Return a DataFrame with mean, std, z, and boolean mask |z|>3.\"\"\"\n",
    "    s = pd.to_numeric(series, errors=\"coerce\")\n",
    "    mu = s.mean()\n",
    "    sd = s.std(ddof=1)  # sample std (lecture default)\n",
    "    z = (s - mu) / sd if sd not in (0, np.nan, None) and not np.isclose(sd, 0.0) else pd.Series(np.nan, index=s.index)\n",
    "    mask = z.abs() > 3\n",
    "    return mu, sd, z, mask\n",
    "\n",
    "per_ticker_summary = []   # one row per ticker per variable\n",
    "all_outlier_rows = []     # concatenated outlier rows for inspection\n",
    "\n",
    "# choose variables to check (you can add/remove to match exactly your lecture scope)\n",
    "CHECK_VARS = [\"Return\", \"real_close\", \"volume\"]\n",
    "\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    ticker = os.path.basename(path).replace(\"_cleaned_2008_2020.csv\", \"\")\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # derive daily return from real_close (as in previous cells)\n",
    "    if \"real_close\" in df.columns:\n",
    "        df[\"Return\"] = safe_pct_change(df[\"real_close\"])\n",
    "\n",
    "    out_rows_this_ticker = []\n",
    "\n",
    "    for var in CHECK_VARS:\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "\n",
    "        mu, sd, z, mask = flag_outliers_3sigma(df[var])\n",
    "\n",
    "        # per-ticker summary (counts & percents)\n",
    "        n = df[var].dropna().shape[0]\n",
    "        n_out = int(mask.sum()) if mask.notna().any() else 0\n",
    "        pct_out = (n_out / n * 100.0) if n > 0 else np.nan\n",
    "\n",
    "        per_ticker_summary.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"variable\": var,\n",
    "            \"mean\": float(mu) if pd.notna(mu) else np.nan,\n",
    "            \"std\": float(sd) if pd.notna(sd) else np.nan,\n",
    "            \"n_obs\": int(n),\n",
    "            \"n_outliers_3sigma\": n_out,\n",
    "            \"pct_outliers_3sigma\": pct_out\n",
    "        })\n",
    "\n",
    "        # collect outlier rows (with the variable, its z-score, and other context)\n",
    "        if n_out > 0:\n",
    "            tmp = df.loc[mask].copy()\n",
    "            tmp[\"__variable\"] = var\n",
    "            tmp[\"__zscore\"] = z[mask]\n",
    "            tmp[\"__ticker\"] = ticker\n",
    "            # keep only a compact set of columns + the variable flagged\n",
    "            keep_cols = [c for c in [\"real_close\", \"close\", \"adj_close\", \"open\", \"high\", \"low\", \"volume\", var] if c in tmp.columns]\n",
    "            keep_cols = list(dict.fromkeys(keep_cols))  # de-duplicate while preserving order\n",
    "            tmp = tmp[keep_cols + [\"__variable\", \"__zscore\", \"__ticker\"]]\n",
    "\n",
    "            out_rows_this_ticker.append(tmp)\n",
    "\n",
    "    # print per-ticker outliers (first 10) instead of saving\n",
    "    if out_rows_this_ticker:\n",
    "        out_df = pd.concat(out_rows_this_ticker).sort_index()\n",
    "        print(f\"\\n=== {ticker}: first 10 outliers across variables ===\")\n",
    "        print(out_df.head(10))\n",
    "        # still keep for overall combined preview\n",
    "        all_outlier_rows.append(out_df.assign(__file=ticker))\n",
    "\n",
    "\n",
    "# write summaries\n",
    "summary_df = pd.DataFrame(per_ticker_summary).sort_values([\"variable\", \"ticker\"]).reset_index(drop=True)\n",
    "print(\"\\n=== Outlier summary by ticker & variable (top 30 rows) ===\")\n",
    "print(summary_df.head(30))\n",
    "\n",
    "\n",
    "if all_outlier_rows:\n",
    "    combined_outliers = pd.concat(all_outlier_rows).sort_index()\n",
    "    print(\"\\n=== Combined outliers (first 50 rows) ===\")\n",
    "    print(combined_outliers.head(50))\n",
    "    print(f\"\\nTotal outlier rows: {len(combined_outliers)}\")\n",
    "else:\n",
    "    print(\"No outliers detected at ±3σ for the checked variables (or all series have near-zero std).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Data Transformation (We already did data normalisation), we need to do discretisation\n",
    "\n",
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# controls\n",
    "CLEAN_DIR = \"/home/ug/orlovsd2/csc1171/data/clean/SP500_ETF_FX_Crypto_Daily\"\n",
    "files = glob.glob(f\"{CLEAN_DIR}/*_cleaned_2008_2020.csv\")\n",
    "\n",
    "TARGET_VARS = [\"Return\", \"real_close\", \"volume\"]\n",
    "\n",
    "BIN_SIZE    = 50                # number of observations per bin (lecture used 5 on a 150 sample)\n",
    "pd.set_option(\"display.max_rows\", 1000)   # increase if needed\n",
    "pd.set_option(\"display.width\", 200)\n",
    "PRINT_TICKERS    = -1    # how many tickers to print (set -1 for all)\n",
    "PRINT_BINS       = -1     # how many bins to show per ticker (set -1 for all)\n",
    "ROWS_PER_TICKER  = 20     # how many outlier/data rows to show per ticker if you print \n",
    "ATTACH_COLUMN = True            # add a discretised column back to each df (not saved; just demo)\n",
    "\n",
    "def load_clean_csv(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=[\"Date\"]); df = df.set_index(\"Date\")\n",
    "    except ValueError:\n",
    "        df = pd.read_csv(path, index_col=0, parse_dates=[0])\n",
    "    df.index = pd.to_datetime(df.index).tz_localize(None)\n",
    "    return df\n",
    "\n",
    "def safe_pct_change(s):\n",
    "    r = s.pct_change()\n",
    "    return r.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def discretise_by_fixed_bin_size(s, bin_size):\n",
    "    \"\"\"\n",
    "    Lecture-style discretisation:\n",
    "      1) sort values\n",
    "      2) split into contiguous bins of equal size\n",
    "      3) compute Bin Mean, Boundaries, Median\n",
    "      4) map each original index to its bin id\n",
    "    Returns:\n",
    "      bin_table: DataFrame with one row per bin (mean/bounds/median/count)\n",
    "      labels:    Series aligned to original index with integer bin ids [0..B-1]\n",
    "    \"\"\"\n",
    "    s = pd.to_numeric(s, errors=\"coerce\").dropna()\n",
    "    if s.empty:\n",
    "        return pd.DataFrame(), pd.Series(index=s.index, dtype=\"Int64\")\n",
    "\n",
    "    # sort values (like b = np.sort(b) in the lecture)\n",
    "    sorted_vals = s.sort_values()\n",
    "    n = len(sorted_vals)\n",
    "    n_bins = max(1, n // bin_size)  # floor\n",
    "    if n_bins == 1:\n",
    "        # everything in one bin; still return lecture-style outputs\n",
    "        v = sorted_vals.values\n",
    "        row = {\n",
    "            \"bin_id\": 0,\n",
    "            \"lower_bound\": float(v[0]),\n",
    "            \"upper_bound\": float(v[-1]),\n",
    "            \"bin_mean\": float(np.mean(v)),\n",
    "            \"bin_median\": float(np.median(v)),\n",
    "            \"count\": int(len(v))\n",
    "        }\n",
    "        bin_table = pd.DataFrame([row])\n",
    "        labels = pd.Series(0, index=s.index, dtype=int)\n",
    "        return bin_table, labels\n",
    "\n",
    "    # chunk indices\n",
    "    edges = list(range(0, n, bin_size))\n",
    "    if edges[-1] != n:  # ensure last edge at n\n",
    "        edges.append(n)\n",
    "\n",
    "    rows = []\n",
    "    # map original index to bin via the *rank order* after sorting\n",
    "    # build a series of bin ids for sorted index first\n",
    "    label_sorted = pd.Series(index=sorted_vals.index, dtype=int)\n",
    "\n",
    "    for b in range(len(edges) - 1):\n",
    "        i0, i1 = edges[b], edges[b+1]\n",
    "        block = sorted_vals.iloc[i0:i1]\n",
    "        v = block.values\n",
    "\n",
    "        # lecture stats\n",
    "        lower = float(v[0])\n",
    "        upper = float(v[-1])\n",
    "        mean  = float(np.mean(v))\n",
    "        med   = float(np.median(v))\n",
    "\n",
    "        rows.append({\n",
    "            \"bin_id\": b,\n",
    "            \"lower_bound\": lower,\n",
    "            \"upper_bound\": upper,\n",
    "            \"bin_mean\": mean,\n",
    "            \"bin_median\": med,\n",
    "            \"count\": int(len(v))\n",
    "        })\n",
    "\n",
    "        # assign label b to these rows\n",
    "        label_sorted.loc[block.index] = b\n",
    "\n",
    "    bin_table = pd.DataFrame(rows)\n",
    "\n",
    "    # reorder labels back to original time index\n",
    "    labels = label_sorted.reindex(s.index).astype(int)\n",
    "\n",
    "    return bin_table, labels\n",
    "\n",
    "preview_left = PRINT_TICKERS\n",
    "for path in files:\n",
    "    df = load_clean_csv(path)\n",
    "    ticker = os.path.basename(path).replace(\"_cleaned_2008_2020.csv\", \"\")\n",
    "    if df.empty:\n",
    "        continue\n",
    "\n",
    "    # derive Return for this discretisation step (matches earlier cells)\n",
    "    if \"real_close\" in df.columns and \"Return\" not in df.columns:\n",
    "        df[\"Return\"] = safe_pct_change(df[\"real_close\"])\n",
    "\n",
    "    for var in TARGET_VARS:\n",
    "        if var not in df.columns:\n",
    "            continue\n",
    "\n",
    "        bin_table, labels = discretise_by_fixed_bin_size(df[var], BIN_SIZE)\n",
    "\n",
    "        # attach discretised labels back (if requested)\n",
    "        if ATTACH_COLUMN and not labels.empty:\n",
    "            df[f\"{var}_bin\"] = labels.astype(\"Int64\") \n",
    "\n",
    "\n",
    "        if PRINT_TICKERS == -1 or preview_left > 0:\n",
    "            print(f\"\\n=== {ticker} • {var} • BIN_SIZE={BIN_SIZE} ===\")\n",
    "            print(\"Bin Mean / Median / Boundaries:\")\n",
    "            if PRINT_BINS == -1:\n",
    "                print(bin_table)             # print ALL bins\n",
    "            else:\n",
    "                print(bin_table.head(PRINT_BINS))\n",
    "            # optional: show more label rows\n",
    "            print(\"\\nDiscretised labels sample (date → bin_id):\")\n",
    "            print(df[[var, f\"{var}_bin\"]].dropna().head(ROWS_PER_TICKER))\n",
    "            if PRINT_TICKERS != -1:\n",
    "                preview_left -= 1\n",
    "\n",
    "\n",
    "# Note:\n",
    "# If you want equal-*width* bins instead (not equal-frequency), swap logic to:\n",
    "# pd.cut(s, bins=n_bins, right=True, include_lowest=True) and compute mean/median per category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
