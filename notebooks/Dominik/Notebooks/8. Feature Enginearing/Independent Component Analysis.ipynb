{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TdomuIx9qqGF"},"source":["#**Independent Component Analysis**\n","\n","\n","Imagine you are a spy at an embassy coctail party and various VIP and diplomats are at this party. Now you have 2 major groups at the party. The first is the Americans and the second is the Russians.  Now if you were trying to record these conversations with 1 microphone you would get extremely distorted sound quality.Now you may think I can just put 2 microphones in the room. Yes this is true you could do this, but you will still get levels of noise from each conversation. You will also not know where to put the microphones exactly. You can see in Fig 1 below that each source is mixed together and the outputs in $s_1$ and $ S_2$ and mixed by some matrix A. So this is where Independent Component Analysis (ICA) comes in. It helps filter conflicting sources into their original source.  \n","\n","![image.png](https://www.computing.dcu.ie/~amccarren/mcm_images/coctail_party.png)\n","Fig 1\n","\n","\n","ICA (also known as Blind Signal Seperation) allows us to identify & separate a mixtures of sources with little prior information. There are many applications and these include the following:\n","\n","* Audio Processing\n","* Medical data\n","* Finance\n","* Array processing (beamforming)\n","* Coding\n","\n","and most applications where Factor Analysis and PCA is currently used.\n","\n","While PCA seeks directions that represents data best, ICA seeks such directions that are most independent from each other.\n","\n","In this step We are going to rely on notes written by  [Klein Carsten](https://github.com/akcarsten/Independent_Component_Analysis). I however use Sklearn FastICA instead of Kleins method, as Kliens version is quite long. But, I would suggest you implement and compare the results.\n","\n","\n","As we saw from fig 1 ICA assumes we generate the observed data from an underlying unknown independent processes $x$ with mixing signals $A$.\n","\n",">>$x=As$\n","\n","Now lets get started with the coding for a simple process. We will need the following libraries.\n"]},{"cell_type":"code","metadata":{"id":"SU5Ps_NmrCSm","executionInfo":{"status":"ok","timestamp":1761933479789,"user_tz":0,"elapsed":2911,"user":{"displayName":"Maryam Basereh","userId":"17916650843126787012"}}},"source":["import pandas as pd\n","import numpy as np\n","from scipy import signal\n","import matplotlib.pyplot as plt\n","\n","# Enable plots inside the Jupyter NotebookLet the\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xYLWECxxrE8M"},"source":["\n","\n","\n","**Create toy signals**\n","We will start by creating some independent signals that will be mixed by matrix A. The independent sources signals are (1) a sine wave, (2) a saw tooth signal and (3) a random noise vector. After calculating their dot product with A we get three linear combinations of these source signals."]},{"cell_type":"code","metadata":{"id":"kRqyvT5UtSJa"},"source":["# Set a seed for the random number generator for reproducibility\n","np.random.seed(23)\n","\n","# Number of samples\n","ns = np.linspace(0, 200, 1000)\n","\n","# Source matrix\n","S = np.array([np.sin(ns * 1),\n","              signal.sawtooth(ns * 1.9),\n","              np.random.random(len(ns))]).T\n","\n","\n","\n","# Mixing matrix\n","A = np.array([[0.5, 1, 0.2],\n","              [1, 0.5, 0.4],\n","              [0.5, 0.8, 1]])\n","\n","\n","# Mixed signal matrix\n","X = S.dot(A).T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EUsYjOK24hCv"},"source":["Now we will just plot all the original signals and their mixed versions."]},{"cell_type":"code","metadata":{"id":"G5b8K_qUtVXk"},"source":["# Plot sources & signals\n","fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n","ax.plot(ns, S, lw=5)\n","ax.set_xticks([])\n","ax.set_yticks([-1, 1])\n","ax.set_xlim(ns[0], ns[200])\n","ax.tick_params(labelsize=12)\n","ax.set_title('Independent sources', fontsize=25)\n","\n","fig, ax = plt.subplots(3, 1, figsize=[18, 5], sharex=True)\n","ax[0].plot(ns, X[0], lw=5)\n","ax[0].set_title('Mixed signals', fontsize=25)\n","ax[0].tick_params(labelsize=12)\n","\n","ax[1].plot(ns, X[1], lw=5)\n","ax[1].tick_params(labelsize=12)\n","ax[1].set_xlim(ns[0], ns[-1])\n","\n","ax[2].plot(ns, X[2], lw=5)\n","ax[2].tick_params(labelsize=12)\n","ax[2].set_xlim(ns[0], ns[-1])\n","ax[2].set_xlabel('Sample number', fontsize=20)\n","ax[2].set_xlim(ns[0], ns[200])\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JLDQ366M2jvA"},"source":["The code above generates 3 plots from the orginal 3 signals. Each is mixed with the other 2 signals with a mixing matrix A."]},{"cell_type":"markdown","metadata":{"id":"MsiME9Nn3DyG"},"source":["**Retrieving the components**\n","The above equations implies that if we invert A and multiply it with the observed signals x we will retrieve our sources:\n","\n",">> $W=A^{-1}$\n","\n",">> $s=xW$\n","\n","This means that what our ICA algorithm needs to estimate is W. So the challenge for ICA is to find the matrix W which will always be unknown.\n","\n","**ICA Assumptions**\n","\n","The following assumptions exist when applying ICA:\n","* The mixed signals are linear combinations of any number of source signals\n","* The source signals are independent of each other. In otherwords there isn't and individual who is taking part in both conversations.\n","* The independent components are non-Gausian.  [Klein](https://github.com/akcarsten/Independent_Component_Analysis) gives the following explaination :\n","\n","\"The joint density distribution of two independent non-Gaussian signals will be uniform on a square. Mixing these two signals with an orthogonal matrix will result in two signals that are now not independent anymore and have a uniform distribution on a parallelogram. Which means that if we are at the minimum or maximum value of one of our mixed signals we know the value of the other signal. Therefore they are not independent anymore. Doing the same with two Gaussian signals will result in something else. The joint distribution of the source signals is completely symmetric and so is the joint distribution of the mixed signals. Therefore it does not contain any information about the mixing matrix, the inverse of which we want to calculate. It follows that in this case the ICA algorithm will fail\".\n","\n","Figure 2 tries to explain this in so far as if you mix 2 guasian sorces you will not have any directions to work from.\n","\n","![image.png](https://www.computing.dcu.ie/~amccarren/mcm_images/ICA_gausianVsnongausian.png)\n","\n","I prefer to use the logic that if you add 2 gausian distributions together then you will get another guasian distribution but the new signal as no information as to the orginal paramters of the source signal.\n","\n","So you can go to Kleins repository to read further on this if you want. The key to solving the this problem is maximizing the non-guassianity of $s=xW$. There are numerous  [measures of Non-Guassianity](http://fourier.eng.hmc.edu/e161/lectures/ica/node4.html) but for the purposes of this step we are going to use the 'logcosh' which is the default in python. Next we will go through some of the preprocessing requirements for an ICA.\n","\n","**Preprocessing**\n","\n","In ICA we generally use 2 preprocessing steps as it is not a good idea to insert your data directly into Pythons fastIca.\n","\n","\n","* Centering is the first preprocessing step and is basically the same that we used for PCA, where we take away the mean for each observation:\n","> $\\hat{x_i}=x_i-\\bar{x}$\n","\n","**Whitening**\n","\n","The second pre-processing step that we need to do is to whiten our signals X. The goal here is to linearly transform X so that potential correlations between the signals are removed and their variances equal unity. As a result the covariance matrix of the whitened signals will be equal to the identity matrix. Now in FastICA we will use the default parameters. I have found it quite difficult to change my contrast function to a user specified one using scikit learns library. I expect this is why Klein developed his own version of FastICA.\n","\n","\n","So we will now implement an ICA with the data outlined above  using scikit learns version of FastICA. I would also like you to implement Kleins approach and compare your results.\n","\n"]},{"cell_type":"code","metadata":{"id":"Y9112NneJQAN"},"source":["import numpy as np\n","# Set a seed for the random number generator for reproducibility\n","np.random.seed(23)\n","\n","# Number of samples\n","ns = np.linspace(0, 200, 1000)\n","\n","# Source matrix\n","S = np.array([np.sin(ns * 1),\n","              signal.sawtooth(ns * 1.9),\n","              np.random.random(len(ns))]).T\n","S /= S.std(axis=0)\n","# Plot sources & signals\n","meanPoint = S.mean(axis = 0)\n","S=S-meanPoint\n","#S /= S.std(axis=0)\n","\n","\n","\n","#print(S)\n","# Mixing matrix\n","A = np.array([[0.5, 1, 0.2],\n","              [1, 0.5, 0.4],\n","              [0.5, 0.8, 1]])\n","\n","# Mixed signal matrix\n","X = np.dot(S, A.T)\n","print(X.shape)\n","\n","\n","from sklearn.decomposition import FastICA\n","ica = FastICA(n_components=3,fun='logcosh',whiten=True)\n","#,fun='exp','logcosh'\n","S_ = ica.fit_transform(X)  # Reconstruct signals\n","print(S_.shape)\n","A_ = ica.mixing_  # Get estimated mixing matrix\n","\n","print(A_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpXtwvMk6RU-"},"source":["You will see from the code above that we have used the \"logcosh\" contrast function. This function is specified to minimise the guassinity of the sources.\n","We then plot the results and compare them to the original sources."]},{"cell_type":"code","metadata":{"id":"uJLM14yQM391"},"source":["# Plot sources & signals\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n","ax.plot(S, lw=5)\n","ax.tick_params(labelsize=12)\n","ax.set_xticks([])\n","ax.set_yticks([-1, 1])\n","ax.set_title('Source signals', fontsize=25)\n","ax.set_xlim(0, 100)\n","\n","\n","fig, ax = plt.subplots(1, 1, figsize=[18, 5])\n","ax.plot(S_, '--', label='Recovered signals', lw=5)\n","ax.set_xlabel('Sample number', fontsize=20)\n","ax.set_title('Recovered signals', fontsize=25)\n","ax.set_xlim(0, 100)\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UdQpX-rq61bn"},"source":["The output shows us that we have found quite similar  sources to the original ones. One thing to note is FASTica from scikit learn does not recreate the sources in their orginal scale, however, Klein rescales the results. When I implemented Kleins approach I got better results. I suspect this is caused by the contrast function that I used above.\n","\n","\n","Can you now implement Kleins approach and compare the results."]}]}