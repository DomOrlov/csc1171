{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0eLsYhep-Ov4"},"source":["#**MCA Example**\n","\n","In the example below we are using a toy dataset known as the [balloons dataset](http://vxy10.github.io/2016/06/10/intro-MCA/) which was taken from [UCI datasets](https://archive.ics.uci.edu/ml/datasets.html). This dataset follows that the most common format for Categorigical variables, and so hopefully you will grasp the idea fairly quickly."]},{"cell_type":"markdown","metadata":{"id":"C6-F_WO6m3cz"},"source":["Install a package named \"mca\". There is a package called \"prince\" but I found that a lot of it was depracted, plus I found \"mca\" easier to use."]},{"cell_type":"code","metadata":{"id":"pdAtJ9Iu4-SI"},"source":["#!pip install  prince\n","!pip install  mca"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K60Hr2eTnN6n"},"source":["Now lets import the relevant libraries."]},{"cell_type":"code","metadata":{"id":"z88euLb_33oE"},"source":["import mca\n","import pandas as pd\n","import numpy as np\n","\n","np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\n","pd.set_option('display.precision', 5)\n","pd.set_option('display.max_columns', 25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dp4yH9XsneI"},"source":["Reading the dataset from UCI."]},{"cell_type":"code","metadata":{"id":"O0LyNTnADIDF"},"source":["import pandas as pd\n","X = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/balloons/adult+stretch.data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vc48pldVntfg"},"source":["This example is going to read the data straight into pandas from the UCI repository. We are also going to use the [LabelBinarizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) to covert the muti-class labels to binary variables. If you would like to know more about all this go to [Machine Learning Mastery](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelBinarizer.html) for a discussion as to how and why we do this."]},{"cell_type":"code","metadata":{"id":"dAEzDZIVDSVz"},"source":["from sklearn.preprocessing import LabelBinarizer\n","X.columns = ['Color', 'Size', 'Action', 'Age', 'Inflated']\n","X.head()\n","\n","lb = LabelBinarizer()\n","X['Color']=lb.fit_transform(X['Color'])\n","X['Size']=lb.fit_transform(X['Size'])\n","X['Action']=lb.fit_transform(X['Action'])\n","X['Age']=lb.fit_transform(X['Age'])\n","X['Inflated']=lb.fit_transform(X['Inflated'])\n","print(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QVW0Ksw2BSpP"},"source":["MCA calculations are implemented via MCA object. The default condition applies Benz√©cri correction for eigen values, therefore, benzecri flag has to be set to false. Below are the list of attributes for the MCA function, - .L Eigen values - .F Factor scores for columns. (components are linear combination of columns) - .G Factor scores for rows. (components are linear combination of rows) - .expl_var Explained variance. - .fs_r Projections onto the factor space, can also be computed by applying fs_r_sup on each of the row elements. - .cos_r Cosine distance between $i^{th}$ vector and $j^{th}$ factor (or row eigen vector) - .cont_r Contribution of individual categorical variable to the factor.\n","\n","In the code below you will see how we have inserted the dataset into the MCA object. We have selected \"benzecri=False\" as it when left true it tends to \"chop\" the dataset down a bit to much.\n","\n","We also print out  the factor scores for each row, the contribution of each row to the each factor and the eigen values for each factor."]},{"cell_type":"code","metadata":{"id":"OojGBZNFEzqq"},"source":["import mca\n","print(X.columns)\n","#cols=['Color', 'Size', 'Action','Age', 'Inflated']\n","mca1 = mca.MCA(X,cols=['Color','Size', 'Action','Age','Inflated'],benzecri=False)\n","#mca1 = mca.MCA(X,benzecri=False)\n","#print(mca1.L)\n","print(\"Factor scores for each row \")\n","print(print(mca1.fs_r(N=5)))\n","print(\"contribution from each row to the components\")\n","print(mca1.cont_r(N=5))\n","print(\" Eigen values :\",mca1.L,\" Total Accumulated variance:\", mca1.L.sum())\n","#mca_fit.total_inertia_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RVTl-1mBqDEz"},"source":["In the next peice of code we are organizing the data into a convient table."]},{"cell_type":"code","metadata":{"id":"8xwPu0-zd953"},"source":["fs, cos, cont = 'Factor score','Squared cosines', 'Contributions x 1000'\n","table3 = pd.DataFrame(columns=X.index, index=pd.MultiIndex\n","                      .from_product([[fs, cos, cont], range(1, 3)]))\n","\n","table3.loc[fs,    :] = mca1.fs_r(N=2).T\n","table3.loc[cos,   :] = mca1.cos_r(N=2).T\n","table3.loc[cont,  :] = mca1.cont_r(N=2).T * 1000\n","\n","np.round(table3.astype(float), 2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1Bb6WnUecK2"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","points = table3.loc[fs].values\n","labels = table3.columns.values\n","print(points)\n","plt.figure()\n","plt.margins(0.1)\n","plt.axhline(0, color='gray')\n","plt.axvline(0, color='blue')\n","plt.xlabel('Factor 1')\n","plt.ylabel('Factor 2')\n","plt.scatter(*points, s=120, marker='o', c='r', alpha=1, linewidths=0)\n","for label, x, y in zip(labels, *points):\n","    plt.annotate(label, xy=(x, y), xytext=(x + .03, y + .03))\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_OEIXGQ4vDC3"},"source":["#**Conclusions**\n","\n","We can see from our analysis that the first 4 components contribute to the lions share of the variance. This means we could potenially drop 5th component and cosequently reduce our dataset.\n","\n","We have also shown how you can create new factors that ensure that correlation within categorical variables can be avoided."]}]}