{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"MhTrOao00DF8"},"source":["#**Measuring MultiCollinearity**\n","\n","In Topic M1.3.12 we have shown how VIF can be used to measure multicollinearity in a feature datasets. VIF is calculated by regressing an independent variable against all the other variables and using the following formula:\n","\n",">>$X_1=\\alpha + X_2+...+X_n$\n","\n","We then get the $R^2$ from each variables regression model and then calculate the VIF using the following equation:\n","\n",">>$ V.I.F. = 1 / (1 - R^2). $\n","\n","If your VIF factor is >10 then you really need to  drop variables from your model, but if it is between 5-10 then you need to consider it.\n","\n","Lets look at the Boston housing data fro Scikit Learn."]},{"cell_type":"code","metadata":{"id":"cDnQ9YJPz6qo"},"source":["from sklearn.datasets import fetch_california_housing\n","\n","import numpy as np\n","import pandas as pd\n","\n","#Visualization Libraries\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","#To plot the graph embedded in the notebook\n","%matplotlib inline\n","boston = fetch_california_housing()\n","#print(boston.DESCR)\n","bos = pd.DataFrame(boston.data, columns = boston.feature_names)\n","bos['PRICE'] = boston.target\n","print(bos.describe())\n","print(bos.columns)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSOSB-lZR9em"},"source":["So we have 8 independent variables and we will now examine the correlation matrix. From this matrix you will already see correlations >0.7. This tells us that we are likely to have issues."]},{"cell_type":"code","metadata":{"id":"JOkfG7kpPlwA"},"source":["bos_1 = pd.DataFrame(boston.data, columns = boston.feature_names)\n","\n","correlation_matrix = bos_1.corr().round(2)\n","sns.heatmap(data=correlation_matrix, annot=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWArfbJUTAxk"},"source":["When you run your model in python it gives 2 small warnings at the bottom of the summary. Don't worry about [1] but\n","\\[2\\] tells us that the inverted $X^tX$ matrix is close to non-invertible. This is telling us that there is possible multcollinearity in our data. The condition number is the ratio of the largest eigenvalue to the smallest eigenvalue in the $X^TX$(design matrix) matrix. Now this eigenvalue ratio may also be high becuase of scaling differences in our design matrix. So we will have to calculate the VIF's for each variable before we can decide what to do next."]},{"cell_type":"code","metadata":{"id":"wdNHocL-1NvO"},"source":["import statsmodels.api as sm\n","import numpy as np\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.metrics import mean_squared_error\n","\n","X=bos[boston.feature_names]\n","\n","y=bos['PRICE']\n","X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size = 0.2, random_state=5)\n","\n","\n","X = sm.add_constant(X_train_1)\n","\n","model = sm.OLS(np.log(y_train_1),X)\n","results = model.fit()\n","y_pred=results.predict(X)\n","\n","rms = np.sqrt(mean_squared_error(y_train_1, y_pred))\n","#\n","\n","X_test = sm.add_constant(X_test_1)\n","y_test_pred=results.predict(X_test)\n","rms_test = np.sqrt(mean_squared_error(y_test_1, y_test_pred))\n","\n","print(\"training root mean Square error is: \",rms)\n","print(\"test root mean Square error is: \",rms_test)\n","print(results.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TrtL-QCWq9R7"},"source":["import matplotlib.pyplot as plt\n","\n","fig, ax = plt.subplots()\n","ax.plot(y_test_pred, np.log(y_test_1), 'o', label=\"Test\")\n","ax.plot(y_pred, np.log(y_train_1), 'o', label=\"Train\")\n","\n","\n","ax.legend(loc=\"best\");"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jT6R_hjs2Pof"},"source":["from statsmodels.stats.outliers_influence import variance_inflation_factor\n","from patsy import dmatrices\n","from statsmodels.api import add_constant\n","import pandas as pd\n","\n","\n","\n","X = add_constant(X_train_1)\n","\n","vif = [variance_inflation_factor(X.to_numpy(), i) for i in range(X.to_numpy().shape[1])]\n","print(vif[1:])\n","\n","print(\"VIF > 5:\",X.columns[np.where(np.asarray(vif[1:])>5)])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r3NBjT3yTmLs"},"source":["Now the code above is used to find the variables that are affected by multicollinearity. They are 'HouseAge', 'AveRooms', 'AveOccup', and 'Latitude' variables. Now if we remove these variable you will see the condition number is lower but still high, however the VIFs are all fine. This high condition number is purely a scaling issue as the Multicollinearity is now gone. The literature tells that a condition number above 20 is high. However, this can be caused by variables comming from differing scales as well as multicollinearity.\n","\n","I would now like you to experiment with the variables in this model and see what happens when you reduce them further. What happens your predictions? Are you concerned about the low values of the Y variable? And can you explain why we have put a log value around Y?"]}]}