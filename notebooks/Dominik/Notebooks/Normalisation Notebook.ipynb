{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uXKx3bFHicK8"},"source":["#**Data Normalization in Data Mining**\n","Normalization is used to scale the data of an attribute so that it falls in a smaller range, such as -1.0 to 1.0 or 0.0 to 1.0. It helps certain machine learning agorithms avoid bias in their predictions. Many machine learning algorithms require the input attributes to be scaled as the cost function used to optimize the weights/parameters will be influenced by values from differing scales. **It is not a prerequisite for all algorithms though, and should only be used when required. Currently, there is a fashion in the machine learning world to do it as a mater of course. Any transformation approach will reduce the amount of information arising from the variable in question**.\n","\n","Situations where you may want to standarise are:\n","\n","* The variables are measuring different physical quantities\n","\n","* Numeric values are on vastly different scales of magnitude\n","\n","* There is no evidence that variables with high variation should be considered more important.\n","\n","Situations where you will not:\n","\n","* Where variables are the same physical quantity and are roughly the same magnitude\n","* you should not standarise varibles that do not change between samples. It may be worthwhile excluding them\n","\n","* if you have such physically related variables, your measurement noise may be roughly the same for all variables, but the signal intensity varies much more. I.e. variables with low values have higher relative noise. Standardizing would blow up the noise. In other words, you may have to decide whether you want relative or absolute noise to be standardized.\n"]},{"cell_type":"markdown","metadata":{"id":"mqChFAZqa6ZZ"},"source":["#**Min-Max Normalization**\n","Mix-Max normalization uses the max and min values of series in order to convert the series to a series of values between 0 and 1.\n","\n","A Min-Max scaling is typically done using the following equation:\n","\n",">> $X^{'}=\\frac{X−X_{min}}{X_{max}−X_{min}}$\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3ZDf233Zka-0"},"source":["The code below demonstrates Min-Max noormalisation using both pandas and numpy.\n"]},{"cell_type":"code","metadata":{"id":"NPcH6qM2gRvD"},"source":["import pandas as pd\n","\n","s1 = pd.Series([1, 2, 3, 4, 5, 6], index=(range(6)))\n","s2 = pd.Series([10, 9, 8, 7, 6, 5], index=(range(6)))\n","df = pd.DataFrame(s1, columns=['s1'])\n","df['s2'] = s2\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D2LSzkKAgZD3"},"source":["from mlxtend.preprocessing import minmax_scaling\n","\n","minmax_scaling(df, columns=['s1', 's2'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XYwhAtiqhE9t"},"source":["import numpy as np\n","\n","X = np.array([[1, 10], [2, 9], [3, 8],\n","              [4, 7], [5, 6], [6, 5]])\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3oj6hzJEhMkV"},"source":["from mlxtend.preprocessing import minmax_scaling\n","\n","minmax_scaling(X, columns=[0, 1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u4343iZNk43u"},"source":["# **Z-Score Normalisation**\n","\n","Z-score normalization converts a variable to a standard normal distribution, using the following formula:\n","\n",">> $X^{'}=\\frac{X−\\bar{X}}{ \\sigma}$\n","\n","where $\\bar{X}$ is the series average and $\\sigma$ is the sample standard deviation.\n","The one benifit to this technique is outliers will have less impact that the other 2 techniques. It will also center the attribute so the interpretation of the estimated weights/parameter estimates will change in your analysis.\n","\n","</br>\n","The following code shows how we can do this.\n"]},{"cell_type":"code","metadata":{"id":"GHnQg_M-nGKH"},"source":["from scipy import stats\n","import numpy as np\n","\n","b = np.array([ 0.7972,  0.0767,  0.4383,  0.7866,  0.8091,  0.1954,\n","                   0.6307, 0.6599,  0.1065,  0.0508])\n","np.round(stats.zscore(b),2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0-bSf4ipI5t"},"source":["# **Normalization by Decimal Scaling**\n","\n","With Normalization by decimal scaling we are basically taking the largest number in our dataset divided by 10 to the power of j such that this number is less than 1. It can be described as follows:\n","\n",">> $X^{'}=\\frac{X}{ 10^j}$ where max $X^{'}<1$"]},{"cell_type":"code","metadata":{"id":"XWPi7jnypHhA"},"source":["\n","import math\n","import numpy as np\n","import pandas as pd\n","X=pd.Series([11,2.4, -100])\n","j = round(math.log10(max(np.abs(X))))+1\n","print(j)\n","X_new=X.apply(lambda num:num/(10**j))\n","print(X_new)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uoy0gJJ-tlBQ"},"source":["# **Review**\n","Scaling or standardization should only be done unless it's absolutely necessary. If the algorithm you use requires it then yes go ahead. So for example if you are using a neural network then you will have to do it. However, if you are using a Linear Regression model then there is absolutely no need to do it. In fact it can casue quite a lot of confusion when you are trying to interpret parameter estimates from interaction variables, (Preacher, 2003).\n","\n","\n","Can you point out a drawback to where these scaling process have a major weakness? Which techniques will be less effected by outliers?"]}]}