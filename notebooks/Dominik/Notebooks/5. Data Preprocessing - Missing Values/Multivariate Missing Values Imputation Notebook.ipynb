{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wrRkbtZqpjNK"},"source":["# **Multivariate Feature Imputation**\n","\n","Multivariate imputation effectively uses complete variables to predict missing values. Skikit-learn describe it as follows:\n","\n","\"IterativeImputer models each feature with missing values as a function of other features, and uses that estimate for imputation. It does so in an iterated round-robin fashion: at each step, a feature column is designated as output y and the other feature columns are treated as inputs X. A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. The results of the final imputation round are returned.\"\n","\n","The estimates of the The MICE package in R uses Multivariate Imputation by chained equations. The skikit-learn iterative imputer is inspired by MICE with the exception that it only produces one set of results.\n","\n","If you go to the following [link](https://scikit-learn.org/stable/auto_examples/impute/plot_missing_values.html#sphx-glr-auto-examples-impute-plot-missing-values-py) you will see a rather complicated approach to estimating missing values. The objective is to build a number of methods and  the results are then combined/aggregated. This is very similar to a technique you will learn about called random forest regression.  I like to think of this approach as getting a bunch of different experts into a room and they all put their opinions on a piece of paper and this is then aggregated to get the overall group opinion.\n"]},{"cell_type":"code","metadata":{"id":"cI-nSqF0rZik"},"source":["import numpy as np\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","\n","y=np.array([[780,750,690,710,680,730,690,720,740,900,950,975,995,1000,1010,1020],\n","    [5.1,4.5,np.nan,3.3,3.6,9.3,6.7,2.8,5.4,np.nan,7.8,np.nan,np.nan,10.1,6.7,np.nan],\n","    [78000,75000,100000,71000,68000,70000,69000,72000,74000,69000,102000,101000,79000,114000,101000,95000],\n","    [0.5,0.55,0.1,0.6,0.7,0.45,0.56,0.73,0.45,0.67,0.43,0.23,0.78,0.42,0.36,0.23]])\n","#y=np.reshape(y, (4, 16))\n","y=y.transpose()\n","#random_state=0\n","imp = IterativeImputer(max_iter=100, sample_posterior=True,tol=0.000001)\n","y=imp.fit_transform(y)\n","print(y)\n","\n","#imp.fit(y)\n","#y_res=imp.transform(y)\n","\n","\n","# the model learns that the second feature is double the first\n","#print(y_res)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQiaF-Yuwy8I"},"source":["Re-run this analysis a number of times you will notice you are getting different results each time. This is because the the random seed is not fixed and the algorithm is using differing combinations of variables and data to solve the problem."]},{"cell_type":"markdown","metadata":{"id":"hc0MYIdnyWi3"},"source":["Adjust the tolerence and go to this [link](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer) and experiment with the parameters. You will have to do this in real life problems. Also re-run the linear regression analysis and see what it shows up.\n","\n","How would you stabilise the results?"]}]}