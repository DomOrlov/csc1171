{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dausxVR9kfBe"},"source":["We are now going to complete an example of how you would implement the steps of a PCA. There are routines in Python or R that would allow you to do it automatically, but working it out from first principles is a really good excercise.\n","\n","Lets get started.\n","\n"]},{"cell_type":"code","metadata":{"id":"dUcHBaHaChwq"},"source":["import numpy as np\n","a=np.array([[2.5,2.4],[0.5,0.7],[2.2,2.9],[1.9,2.2],[3.1,3.0],[2.3,2.7],[2,1.6],[1,1.1],[1.5,1.6],[1.1,0.9]])\n","ahat=(a - np.mean(a, axis=0))\n","ahat\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sLI3KE3uBgEX"},"source":["We have now centered our data around the mean of each column."]},{"cell_type":"code","metadata":{"id":"M691SHVR6Yae"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","x=ahat[:,0]\n","y=ahat[:,1]\n","fig = plt.figure()\n","ax = fig.add_subplot(1, 1, 1)\n","ax.scatter(x, y)\n","plt.show()\n","print(\"correlation matrix is \\n\",np.corrcoef([x,y]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4jKJFwS8BwAE"},"source":["The data is highly correlated is higly correlated with a $\\rho=0.92$."]},{"cell_type":"code","metadata":{"id":"iXNzoRkh7BP2"},"source":["from numpy import linalg as LA\n","cov=(np.matmul(ahat.transpose(),ahat))/(len(ahat)-1)\n","print(\"covariance is:\", cov)\n","vals, vecs = np.linalg.eig(cov)\n","# sort these based on the eigenvalues\n","vecs = vecs[:,np.argsort(-vals)]\n","vals = vals[np.argsort(-vals)]\n","print(\"eigenvalues are : \",vals)\n","print(\"eigenvectors are : \",vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6ZtpIxOChMb"},"source":["Now we calcaulate the covariance matrix(cov) and the eigenvectors(vecs) and eigenvalues(vals) for the dataset. Notice how I sorted the vecs and values by using \"np.argsort(-vals)\" function in numpy. We need to have these values in the correct order"]},{"cell_type":"code","metadata":{"id":"Y_rasn2R8KvV"},"source":["\n","anew=np.matmul(vecs.transpose(),ahat.transpose())\n","print(anew.transpose())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qVKFCm2sCz5i"},"source":["Finally we create our new principal components as follows:\n","  \n","  >>$PC=B^T \\hat{X^T}$\n","\n","the first column of this new matrix corresponds to PC 1 and explains 96.01\\% of the variation:\n",">> $\\frac{1.28}{(1.2840+ 0.0491)}*100=96.01\\%$"]},{"cell_type":"code","metadata":{"id":"1Nvw-ipd-l78"},"source":["x=anew.transpose()[:,0]\n","y=anew.transpose()[:,1]\n","fig = plt.figure()\n","ax = fig.add_subplot(1, 1, 1)\n","ax.scatter(x, y)\n","plt.xlabel('Principal Component 1 values')\n","plt.ylabel('Principal Component 2 values')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dyLOHeFPEYz9"},"source":["Finally, we can see the plot of the new components and it shows us that most of the variation is in PC1. We could now with loss of to much information use PC1 to replace X and Y. Thus we have achieved 2 things with this operation. The first to create a new set of features that are not correlated and the second is to create a feature that is nearly as powerful as the original 2 features.\n","\n","**Review**\n","\n","Increase the number of dimensions and see if you can get the above code to calculate new principal components. Compare this apporach to the one outlined in MOOC 2."]}]}