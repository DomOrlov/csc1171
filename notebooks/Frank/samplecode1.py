# -*- coding: utf-8 -*-
"""Samplecode1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PVkn_zWVRJThLI4UltuMvnuwmfFIZBlM
"""

with open('all_symbols.txt', 'r') as f:
    symbols = [line.strip() for line in f if line.strip()]
    for symbol in symbols:
        print(symbol)

"""### American Airlines Group Inc Summary"""

import pandas as pd

df = pd.read_csv('AAL.csv')

display(df.head())

df['year'] = pd.to_datetime(df['date']).dt.year

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(df['year'], df['close'])
plt.xlabel('Year')
plt.ylabel('Closing Price')
plt.title('Closing Price Over Time')
plt.grid(True)
plt.show()



import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['volume'])
plt.xlabel('Date')
plt.ylabel('Volume')
plt.title('Trading Volume Over Time')
plt.grid(True)
plt.show()



import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
plt.plot(df['date'], df['adjclose'])
plt.xlabel('Date')
plt.ylabel('Adjusted Closing Price')
plt.title('Adjusted Closing Price Over Time')
plt.grid(True)
plt.show()

correlation_matrix = df.corr(numeric_only=True)
display(correlation_matrix)

"""# Anomalies"""







"""# Task
Load the "all_symbols.txt" file, parse the data to extract the stock symbol and company name, filter for companies on AMEX, NYSE, and NASDAQ, select the middle 10 companies from the filtered data, and display the symbol and name for the selected companies.

## Load data

### Subtask:
Load the `all_symbols.txt` file into a pandas DataFrame.

**Reasoning**:
The subtask is to load the `all_symbols.txt` file into a pandas DataFrame and display the head.
"""

df_symbols = pd.read_csv('all_symbols.txt', header=None, sep=' ')
display(df_symbols.head())

"""## Data cleaning and parsing

### Subtask:
Parse the data to separate the stock symbol and company name. Filter for companies on AMEX, NYSE, and NASDAQ.

**Reasoning**:
The error persists because `str.split` with `expand=True` can still produce varying numbers of columns if there are leading spaces or multiple spaces in the original string. A more robust approach is to use a regular expression to split the string at the first space and ensure exactly two columns are created.
"""

df_symbols[['Symbol', 'Company Info']] = df_symbols[0].str.extract(r'(\S+)\s+(.*)').fillna('')

def extract_exchange(company_info):
    if 'AMEX' in company_info:
        return 'AMEX'
    elif 'NYSE' in company_info:
        return 'NYSE'
    elif 'NASDAQ' in company_info:
        return 'NASDAQ'
    else:
        return None

df_symbols['Exchange'] = df_symbols['Company Info'].apply(extract_exchange)

filtered_df_symbols = df_symbols[df_symbols['Exchange'].isin(['AMEX', 'NYSE', 'NASDAQ'])].copy()
display(filtered_df_symbols.head())

"""**Reasoning**:
The filtered DataFrame is empty, indicating that the exchange extraction or filtering logic might be incorrect based on the actual content of the 'Company Info' column. I need to inspect the 'Company Info' column to understand its structure and adjust the extraction logic accordingly.

**Reasoning**:
The 'Company Info' column appears to be empty after the split. This suggests that the regular expression might not be correctly capturing the company information or that there's an issue with the initial data loading or splitting process. I will re-examine the original data loading and splitting to ensure the 'Company Info' is correctly populated before attempting to extract the exchange.

**Reasoning**:
The error indicates that the split is still producing a variable number of columns. I will try a different splitting approach that is more robust to inconsistent spacing or lack of spaces. I will iterate through each row, find the first space, and split the string manually.
"""

symbols_list = []
company_info_list = []
for index, row in df_symbols.iterrows():
    line = row[0]
    first_space_index = line.find(' ')
    if first_space_index != -1:
        symbols_list.append(line[:first_space_index])
        company_info_list.append(line[first_space_index + 1:])
    else:
        symbols_list.append(line)
        company_info_list.append('')

df_symbols['Symbol'] = symbols_list
df_symbols['Company Info'] = company_info_list

def extract_exchange(company_info):
    if 'AMEX' in company_info:
        return 'AMEX'
    elif 'NYSE' in company_info:
        return 'NYSE'
    elif 'NASDAQ' in company_info:
        return 'NASDAQ'
    else:
        return None

df_symbols['Exchange'] = df_symbols['Company Info'].apply(extract_exchange)

filtered_df_symbols = df_symbols[df_symbols['Exchange'].isin(['AMEX', 'NYSE', 'NASDAQ'])].copy()
display(filtered_df_symbols.head())

"""**Reasoning**:
The filtered DataFrame is still empty. This suggests the exchange extraction logic based on checking for 'AMEX', 'NYSE', or 'NASDAQ' within the 'Company Info' string is not working as expected. I need to investigate the content of the 'Company Info' column again to understand how the exchange is actually represented, if at all, after the manual split.


"""

display(df_symbols['Company Info'].head(20))

"""**Reasoning**:
The 'Company Info' column is still empty, indicating that the manual split is not correctly capturing the company information. It seems the initial loading of the data with `pd.read_csv` might be interpreting the entire line as a single field even without specifying a separator, or the space is not the correct delimiter. I will try loading the data without specifying a separator and then use a more flexible method to split the string based on the first occurrence of a space followed by any characters.


"""

df_symbols = pd.read_csv('all_symbols.txt', header=None)

def split_symbol_company(row):
    line = row[0]
    first_space_index = line.find(' ')
    if first_space_index != -1:
        return line[:first_space_index], line[first_space_index + 1:]
    else:
        return line, ''

df_symbols[['Symbol', 'Company Info']] = df_symbols.apply(split_symbol_company, axis=1, result_type='expand')

def extract_exchange(company_info):
    if 'AMEX' in company_info:
        return 'AMEX'
    elif 'NYSE' in company_info:
        return 'NYSE'
    elif 'NASDAQ' in company_info:
        return 'NASDAQ'
    else:
        return None

df_symbols['Exchange'] = df_symbols['Company Info'].apply(extract_exchange)

filtered_df_symbols = df_symbols[df_symbols['Exchange'].isin(['AMEX', 'NYSE', 'NASDAQ'])].copy()
display(filtered_df_symbols.head())

"""## Data cleaning and parsing

### Subtask:
Parse the data to separate the stock symbol and company name. Filter for companies on AMEX, NYSE, and NASDAQ.

**Reasoning**:
The previous attempts to split the symbol and company information based on a single space failed because the data seems to use multiple spaces as a separator. I will try to use a regular expression that looks for one or more whitespace characters to split the string. I will then apply the function to extract the exchange and filter the DataFrame.

**Reasoning**:
The previous command failed because the `re` module was not imported. I will import the `re` module and rerun the code to perform the splitting, extraction, and filtering.
"""

import re

df_symbols = pd.read_csv('all_symbols.txt', header=None)

def split_symbol_company(row):
    line = row[0]
    # Use regex to find one or more whitespace characters as a separator
    match = re.search(r'\s+', line)
    if match:
        # Split based on the first occurrence of one or more spaces
        symbol = line[:match.start()].strip()
        company_info = line[match.end():].strip()
        return symbol, company_info
    else:
        # No space found, assume the whole line is the symbol
        return line.strip(), ''

df_symbols[['Symbol', 'Company Info']] = df_symbols.apply(split_symbol_company, axis=1, result_type='expand')

def extract_exchange(company_info):
    if 'AMEX' in company_info:
        return 'AMEX'
    elif 'NYSE' in company_info:
        return 'NYSE'
    elif 'NASDAQ' in company_info:
        return 'NASDAQ'
    else:
        return None

df_symbols['Exchange'] = df_symbols['Company Info'].apply(extract_exchange)

filtered_df_symbols = df_symbols[df_symbols['Exchange'].isin(['AMEX', 'NYSE', 'NASDAQ'])].copy()
display(filtered_df_symbols.head())

display(df_symbols.head())

"""**Reasoning**:
The filtered DataFrame is still empty after manually reading and splitting the lines. This strongly suggests that the company information, including the exchange name, is not consistently formatted after the symbol, or perhaps the exchange information is not always present immediately after the symbol in a way that the current splitting logic can capture. To understand the data format better and identify the issue, I will display the first 20 entries of the 'Company Info' column from the `df_symbols` DataFrame generated by the manual splitting process. This will show me what kind of data is actually ending up in the 'Company Info' column.


"""

display(df_symbols['Company Info'].head(20))

"""## Select subset

### Subtask:
Extract the middle 10 companies from the filtered data.

**Reasoning**:
The filtered dataframe is still empty after attempting to split by the first space. This suggests that splitting by a single space might not be the correct approach for all lines, or there might be other issues with the data format. I need to re-examine the raw data format to understand the structure and identify a reliable way to separate the symbol from the company information. Since I cannot directly inspect the file contents in the kernel, I will try to display the raw lines from the file as a string to see the actual format.
"""

with open('all_symbols.txt', 'r') as f:
    first_20_lines = [next(f) for _ in range(20)]
display("".join(first_20_lines))

"""## Summary:

### Data Analysis Key Findings

*   The "all\_symbols.txt" file was successfully loaded into a pandas DataFrame.
*   Multiple attempts were made to parse the data to separate the stock symbol from the company name and extract the exchange information (AMEX, NYSE, NASDAQ).
*   Parsing attempts using `str.split`, `str.extract` with regex, and manual iteration/apply based on space delimiters consistently failed to populate the 'Company Info' column in the DataFrame.
*   Inspection of the raw input file revealed that it only contained stock symbols, one per line, with no company information or exchange details.
*   Due to the lack of company and exchange information in the source file, the filtering task based on AMEX, NYSE, and NASDAQ exchanges could not be performed.
*   Consequently, the step to select the middle 10 companies from the filtered data also failed as the filtered DataFrame was empty.

### Insights or Next Steps

*   The provided "all\_symbols.txt" file is insufficient for the task as it lacks the company name and exchange information required for filtering.
*   To complete the task, a data source containing stock symbols, company names, and their respective exchanges is needed.

# api access

**Note:** Make sure you have obtained a free API key from Alpha Vantage. You can store it securely in Colab's Secrets feature (under the "🔑" icon in the left panel) and access it in your code.

# Task
Show the anomalies in the stock trade data in the file "/content/AAL.csv".

## Calculate key metrics

### Subtask:
Calculate daily price change and percentage change, and analyze trading volume.

**Reasoning**:
Calculate daily price change, percentage change, and rolling statistics for trading volume as requested in the subtask instructions.
"""

df['price_change'] = df['close'] - df['open']
df['percentage_price_change'] = (df['price_change'] / df['open']) * 100
df['volume_rolling_mean'] = df['volume'].rolling(window=10).mean()
df['volume_rolling_std'] = df['volume'].rolling(window=10).std()
display(df.head(15))

"""## Identify potential anomalies

### Subtask:
Define criteria for identifying anomalies (e.g., large percentage price changes, unusually high volume).

**Reasoning**:
Calculate the mean and standard deviation for percentage price change and volume, and define anomaly thresholds based on these statistics.
"""

price_change_mean = df['percentage_price_change'].mean()
price_change_std = df['percentage_price_change'].std()
volume_mean = df['volume'].mean()
volume_std = df['volume'].std()

# Define anomaly thresholds (e.g., 3 standard deviations from the mean)
price_anomaly_threshold = price_change_mean + 3 * price_change_std
volume_anomaly_threshold = volume_mean + 3 * volume_std

print(f"Percentage Price Change Mean: {price_change_mean:.2f}")
print(f"Percentage Price Change Std Dev: {price_change_std:.2f}")
print(f"Volume Mean: {volume_mean:.2f}")
print(f"Volume Std Dev: {volume_std:.2f}")
print(f"Price Anomaly Threshold (3 Std Dev): {price_anomaly_threshold:.2f}")
print(f"Volume Anomaly Threshold (3 Std Dev): {volume_anomaly_threshold:.2f}")

"""## Filter and display anomalies

### Subtask:
Filter the DataFrame to show the rows that meet the anomaly criteria and display the results.

**Reasoning**:
Filter the DataFrame based on the defined anomaly thresholds for price change and volume, and then display the combined anomalies.
"""

price_anomalies = df[abs(df['percentage_price_change']) > price_anomaly_threshold].copy()
volume_anomalies = df[df['volume'] > volume_anomaly_threshold].copy()

anomalies = pd.concat([price_anomalies, volume_anomalies]).drop_duplicates().sort_values(by='date')

print("Potential Anomalies:")
display(anomalies.head())
print(f"\nTotal number of anomalies: {len(anomalies)}")

"""## Visualize potential anomalies

### Subtask:
Visualize the identified potential anomalies on a plot to see them in context.

**Reasoning**:
I need to visualize the closing price over time, highlighting the identified anomalies. I will create a scatter plot of the original closing prices and overlay the anomaly points with a distinct marker and color.
"""

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

plt.figure(figsize=(12, 6))

# Plot the original closing price data
plt.scatter(df['date'], df['close'], label='Closing Price', alpha=0.6, s=10)

# Overlay the anomaly points
plt.scatter(anomalies['date'], anomalies['close'], color='red', marker='X', label='Potential Anomaly', s=50)

plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('Closing Price Over Time with Potential Anomalies')
plt.legend()
plt.grid(False)

# Format the x-axis to show dates clearly
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
plt.gcf().autofmt_xdate() # Auto-rotate date labels

plt.show()

"""## Summary:

### Data Analysis Key Findings

*   The mean percentage price change was approximately -0.11% with a standard deviation of about 3.91%.
*   The average trading volume was around 9,435,318 shares, with a standard deviation of approximately 16,432,681 shares.
*   Based on a threshold of 3 standard deviations from the mean, a price anomaly was defined as a percentage price change exceeding approximately 11.63%.
*   A volume anomaly was defined as a trading volume exceeding approximately 58,733,361 shares.
*   Based on these criteria, 122 potential anomalies were identified in the dataset.

### Insights or Next Steps

*   Investigate the specific events or news that might have caused the identified price and volume anomalies.
*   Consider using other anomaly detection techniques, such as time series analysis or machine learning models, for a more comprehensive analysis.

# Paper

# Task
Compare stock trading anomalies for the years 2020 and 2006 using the `df` DataFrame.

## Filter data for 2020

### Subtask:
Create a new DataFrame containing only the data for the year 2020.

**Reasoning**:
Filter the DataFrame to include only data from 2020 and display the head of the new DataFrame to verify the filtering.
"""

df_2020 = df[df['year'] == 2020].copy()
display(df_2020.head())

"""**Reasoning**:
The subtask of creating a DataFrame for the year 2020 has been completed. I will now create a new DataFrame for the year 2006 by filtering the original DataFrame and then display the head of the new DataFrame.


"""

df_2006 = df[df['year'] == 2006].copy()
display(df_2006.head())

"""**Reasoning**:
Calculate the mean and standard deviation of the 'percentage_price_change' and 'volume' columns for the 2020 data to determine anomaly thresholds for this year.


"""

price_change_mean_2020 = df_2020['percentage_price_change'].mean()
price_change_std_2020 = df_2020['percentage_price_change'].std()
volume_mean_2020 = df_2020['volume'].mean()
volume_std_2020 = df_2020['volume'].std()

print(f"2020 Percentage Price Change Mean: {price_change_mean_2020:.2f}")
print(f"2020 Percentage Price Change Std Dev: {price_change_std_2020:.2f}")
print(f"2020 Volume Mean: {volume_mean_2020:.2f}")
print(f"2020 Volume Std Dev: {volume_std_2020:.2f}")

"""**Reasoning**:
Calculate the mean and standard deviation of the 'percentage_price_change' and 'volume' columns for the 2006 data to determine anomaly thresholds for this year.


"""

price_change_mean_2006 = df_2006['percentage_price_change'].mean()
price_change_std_2006 = df_2006['percentage_price_change'].std()
volume_mean_2006 = df_2006['volume'].mean()
volume_std_2006 = df_2006['volume'].std()

print(f"2006 Percentage Price Change Mean: {price_change_mean_2006:.2f}")
print(f"2006 Percentage Price Change Std Dev: {price_change_std_2006:.2f}")
print(f"2006 Volume Mean: {volume_mean_2006:.2f}")
print(f"2006 Volume Std Dev: {volume_std_2006:.2f}")

"""**Reasoning**:
Define anomaly thresholds for 2020 and 2006 based on 3 standard deviations from the mean for both percentage price change and volume, then print these thresholds for comparison.


"""

price_anomaly_threshold_2020 = price_change_mean_2020 + 3 * price_change_std_2020
volume_anomaly_threshold_2020 = volume_mean_2020 + 3 * volume_std_2020

price_anomaly_threshold_2006 = price_change_mean_2006 + 3 * price_change_std_2006
volume_anomaly_threshold_2006 = volume_mean_2006 + 3 * volume_std_2006

print(f"\n2020 Price Anomaly Threshold (3 Std Dev): {price_anomaly_threshold_2020:.2f}")
print(f"2020 Volume Anomaly Threshold (3 Std Dev): {volume_anomaly_threshold_2020:.2f}")
print(f"2006 Price Anomaly Threshold (3 Std Dev): {price_anomaly_threshold_2006:.2f}")
print(f"2006 Volume Anomaly Threshold (3 Std Dev): {volume_anomaly_threshold_2006:.2f}")

"""**Reasoning**:
Filter the 2020 and 2006 DataFrames to identify anomalies based on the calculated thresholds for each year, combine these anomalies into separate DataFrames for each year, and display the head of each anomalies DataFrame along with the total number of anomalies for each year.


"""

price_anomalies_2020 = df_2020[abs(df_2020['percentage_price_change']) > price_anomaly_threshold_2020].copy()
volume_anomalies_2020 = df_2020[df_2020['volume'] > volume_anomaly_threshold_2020].copy()
anomalies_2020 = pd.concat([price_anomalies_2020, volume_anomalies_2020]).drop_duplicates().sort_values(by='date')

print("Potential Anomalies in 2020:")
display(anomalies_2020.head())
print(f"\nTotal number of anomalies in 2020: {len(anomalies_2020)}")

price_anomalies_2006 = df_2006[abs(df_2006['percentage_price_change']) > price_anomaly_threshold_2006].copy()
volume_anomalies_2006 = df_2006[df_2006['volume'] > volume_anomaly_threshold_2006].copy()
anomalies_2006 = pd.concat([price_anomalies_2006, volume_anomalies_2006]).drop_duplicates().sort_values(by='date')

print("\nPotential Anomalies in 2006:")
display(anomalies_2006.head())
print(f"\nTotal number of anomalies in 2006: {len(anomalies_2006)}")

"""**Reasoning**:
The anomaly detection for both 2020 and 2006 has been performed based on the defined criteria. The next step is to visualize these anomalies on separate plots for each year to compare them.


"""

import matplotlib.pyplot as plt

# Visualize anomalies for 2020
plt.figure(figsize=(12, 6))
plt.scatter(df_2020['date'], df_2020['close'], label='2020 Closing Price', alpha=0.6, s=10)
plt.scatter(anomalies_2020['date'], anomalies_2020['close'], color='red', marker='X', label='2020 Potential Anomaly', s=50)
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('2020 Closing Price Over Time with Potential Anomalies')
plt.legend()
plt.grid(False)
plt.show()

# Visualize anomalies for 2006
plt.figure(figsize=(12, 6))
plt.scatter(df_2006['date'], df_2006['close'], label='2006 Closing Price', alpha=0.6, s=10)
plt.scatter(anomalies_2006['date'], anomalies_2006['close'], color='red', marker='X', label='2006 Potential Anomaly', s=50)
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('2006 Closing Price Over Time with Potential Anomalies')
plt.legend()
plt.grid(False)
plt.show()

"""## Summarize comparison

### Subtask:
Summarize the findings from the anomaly comparison between 2020 and 2006.

## Summary:

### Data Analysis Key Findings

*   The mean percentage price change in 2020 was approximately 0.05%, with a standard deviation of 2.54%. In contrast, 2006 had a mean percentage price change of about 0.06% and a standard deviation of 0.89%.
*   The mean volume in 2020 was approximately 5.09 million, with a standard deviation of 3.41 million. For 2006, the mean volume was about 2.91 million, with a standard deviation of 1.19 million.
*   The price anomaly threshold (3 standard deviations above the mean) in 2020 was 7.68%, significantly higher than the 3.72% threshold in 2006.
*   The volume anomaly threshold (3 standard deviations above the mean) in 2020 was 15.31 million, also notably higher than the 6.47 million threshold in 2006.
*   A total of 8 potential anomalies were identified in the stock trading data for 2020.
*   A total of 14 potential anomalies were identified in the stock trading data for 2006.

### Insights or Next Steps

*   Investigate the specific dates identified as anomalies in both 2020 and 2006 to understand the market events or news that may have contributed to these unusual price or volume movements.
*   Explore alternative anomaly detection methods, such as time series analysis or machine learning algorithms, to compare their effectiveness and identify potential anomalies not captured by the 3-standard-deviation rule.
"""



"""# New section

Calculate Metrics: We calculated the daily percentage change in price and used
the trading volume.

Determine Normal Variation: We calculated the mean and standard deviation for both the percentage price change and the trading volume. The standard deviation gives us a measure of how spread out the data is from the average.


Set Anomaly Thresholds: We defined an "anomaly" as a data point (a day's trading) where either the absolute percentage price change or the trading volume was more than 3 standard deviations away from its respective mean. This is a common statistical technique to identify values that are significantly different from the typical pattern.

Filter Data: We then filtered the dataset to select only those rows (trading days) that met these criteria, marking them as potential anomalies.


Essentially, we flagged days where the price moved drastically or the trading volume was exceptionally high compared to the average behavior of the stock, considering the typical variation (standard deviation).


"""

display(anomalies_2020['date'])

display(anomalies_2006['date'])

"""# F-test"""

from scipy.stats import zscore
import numpy as np

# Calculate Z-scores for percentage price change and volume
df['price_change_zscore'] = np.abs(zscore(df['percentage_price_change'].dropna()))
df['volume_zscore'] = np.abs(zscore(df['volume'].dropna()))

# Define a Z-score threshold for anomalies (e.g., Z-score > 3)
zscore_threshold = 3

# Filter for anomalies based on Z-score
price_anomalies_zscore = df[df['price_change_zscore'] > zscore_threshold].copy()
volume_anomalies_zscore = df[df['volume_zscore'] > zscore_threshold].copy()

# Combine price and volume anomalies based on Z-score
anomalies_zscore = pd.concat([price_anomalies_zscore, volume_anomalies_zscore]).drop_duplicates().sort_values(by='date')

print(f"Potential Anomalies (Z-score > {zscore_threshold}):")
display(anomalies_zscore.head())
print(f"\nTotal number of anomalies identified by Z-score: {len(anomalies_zscore)}")

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

plt.figure(figsize=(12, 6))

# Plot the original closing price data
plt.scatter(df['date'], df['close'], label='Closing Price', alpha=0.6, s=10)

# Overlay the anomaly points identified by Z-score
plt.scatter(anomalies_zscore['date'], anomalies_zscore['close'], color='red', marker='X', label=f'Potential Anomaly (Z-score > {zscore_threshold})', s=50)

plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title('Closing Price Over Time with Potential Anomalies (Z-score Method)')
plt.legend()
plt.grid(False)

# Format the x-axis to show dates clearly
plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
plt.gcf().autofmt_xdate() # Auto-rotate date labels

plt.show()

# Merge the two anomaly DataFrames to find common and unique anomalies
# Using an outer merge to keep all anomalies from both methods
comparison_df = pd.merge(anomalies_zscore, anomalies, on='date', how='outer', indicator=True)

# Filter for anomalies unique to Z-score method
zscore_only_anomalies = comparison_df[comparison_df['_merge'] == 'left_only']

# Filter for anomalies unique to 3-standard-deviation method
std_only_anomalies = comparison_df[comparison_df['_merge'] == 'right_only']

# Filter for anomalies found by both methods
common_anomalies = comparison_df[comparison_df['_merge'] == 'both']

print(f"Total anomalies found by Z-score method: {len(anomalies_zscore)}")
print(f"Total anomalies found by 3-standard-deviation method: {len(anomalies)}")
print(f"\nAnomalies found by both methods: {len(common_anomalies)}")
print(f"Anomalies found only by Z-score method: {len(zscore_only_anomalies)}")
print(f"Anomalies found only by 3-standard-deviation method: {len(std_only_anomalies)}")

print("\nDates of anomalies found by both methods:")
display(common_anomalies['date'].head())

print("\nDates of anomalies found only by Z-score method:")
display(zscore_only_anomalies['date'].head())

print("\nDates of anomalies found only by 3-standard-deviation method:")
display(std_only_anomalies['date'].head())

"""# both methods
# add new technique
"""



"""##APPLYING SPLIT AND DIVIDENTS"""

import yfinance as yf
import pandas as pd

# Define the ticker symbol for American Airlines Group Inc.
ticker_symbol = "AAL"

# Create a Ticker object
aal_ticker = yf.Ticker(ticker_symbol)

# Get dividend history
dividends = aal_ticker.dividends

# Get stock split history
splits = aal_ticker.splits

print(f"Dividend History for {ticker_symbol}:")
display(dividends)

print(f"\nStock Split History for {ticker_symbol}:")
display(splits)

import yfinance as yf

# Define the ticker symbol for American Airlines Group Inc. (assuming AAL based on previous context)
ticker_symbol = "AAL"

# Create a Ticker object
aal_ticker = yf.Ticker(ticker_symbol)

# Get company information
company_info = aal_ticker.info

# Display a brief history (business summary) if available
if 'longBusinessSummary' in company_info:
    print(f"Brief History/Business Summary for {ticker_symbol}:")
    print(company_info['longBusinessSummary'])
else:
    print(f"Business summary not available for {ticker_symbol} via yfinance.")

# You can explore other information available in the company_info dictionary
# display(company_info)

# Assuming company_info was fetched in a previous cell (e.g., AKVCpSvS8n_Z)
if 'sector' in company_info and 'industry' in company_info:
    print(f"Sector for AAL: {company_info['sector']}")
    print(f"Industry for AAL: {company_info['industry']}")
elif 'sector' in company_info:
    print(f"Sector for AAL: {company_info['sector']}")
    print("Industry information not available.")
elif 'industry' in company_info:
    print(f"Industry for AAL: {company_info['industry']}")
    print("Sector information not available.")
else:
    print("Sector and Industry information not available for AAL.")

import yfinance as yf
import matplotlib.pyplot as plt

# Define the ticker symbol for American Airlines Group Inc.
ticker_symbol = "AAL"

# Create a Ticker object
aal_ticker = yf.Ticker(ticker_symbol)

# Get historical market data for the maximum available period
historical_data = aal_ticker.history(period="max")

# Plot the closing price
plt.figure(figsize=(12, 6))
plt.plot(historical_data.index, historical_data['Close'])
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.title(f'{ticker_symbol} Closing Price Over Time (Full History)')
plt.grid(True)
plt.show()



"""# Task
Find the exchange closure dates for NYSE, NASDAQ, and AMEX and compare them with the stock trading anomalies found in the AAL data.

## Identify data sources

### Subtask:
Research and identify potential data sources or APIs that provide historical data on exchange closures for NYSE, NASDAQ, and AMEX.

## Access data

### Subtask:
Access exchange closure data using the `trading_calendars` library.

**Reasoning**:
Import the `get_calendar` function from the `trading_calendars` library and retrieve the calendar objects for NYSE, NASDAQ, and AMEX as requested in the subtask. Then, display the types of the retrieved calendar objects to confirm they were loaded correctly.

**Reasoning**:
The previous command failed because the `trading_calendars` library is not installed. I need to install the library using pip.

**Reasoning**:
The installation of `trading_calendars` failed. This might be due to dependencies or compatibility issues. I will try installing a specific version that is known to be stable or compatible. I will try installing version 1.1.0.

## Identify data sources

### Subtask:
Research and identify potential data sources or APIs that provide historical data on exchange closures for NYSE, NASDAQ, and AMEX.

**Reasoning**:
I need to research and identify potential data sources or APIs that provide historical data on exchange closures for NYSE, NASDAQ, and AMEX. Since I cannot perform live web searches, I will rely on my internal knowledge and attempt to access known Python libraries that might contain this information. I will try to use the `pandas_market_calendars` library, as it is specifically designed for this purpose.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install pandas_market_calendars

"""**Reasoning**:
The `pandas_market_calendars` library was successfully installed. I can now use it to access the historical exchange closure dates for NYSE, NASDAQ, and AMEX. I will get the calendars for each of these exchanges and display the first few closure dates for each to confirm that the library is working and providing the expected data.


"""

import pandas_market_calendars as mcal

# Get the NYSE calendar
nyse = mcal.get_calendar('XNYS')
# Get the NASDAQ calendar
nasdaq = mcal.get_calendar('XNAS')
# Get the AMEX calendar
amex = mcal.get_calendar('XASE')

# Get the holidays for a date range (e.g., from 2005 to 2021, covering the AAL data)
start_date = '2005-01-01'
end_date = '2021-12-31'

nyse_holidays = nyse.holidays(start_date=start_date, end_date=end_date)
nasdaq_holidays = nasdaq.holidays(start_date=start_date, end_date=end_date)
amex_holidays = amex.holidays(start_date=start_date, end_date=end_date)

print("NYSE Holidays:")
display(nyse_holidays.head())

print("\nNASDAQ Holidays:")
display(nasdaq_holidays.head())

print("\nAMEX Holidays:")
display(amex_holidays.head())

"""**Reasoning**:
The previous command failed because 'XNAS' and 'XASE' are not recognized calendar names in the installed version of `pandas_market_calendars`. The error message lists the available calendars. I need to use the correct calendar names for NASDAQ and AMEX. Based on the error output, 'NASDAQ' and 'AMEX' appear to be the correct names. I will try getting the calendars again using these names.


"""

import pandas_market_calendars as mcal

# Get the NYSE calendar
nyse = mcal.get_calendar('XNYS')
# Get the NASDAQ calendar
nasdaq = mcal.get_calendar('NASDAQ')
# Get the AMEX calendar
amex = mcal.get_calendar('AMEX')

# Get the holidays for a date range (e.g., from 2005 to 2021, covering the AAL data)
start_date = '2005-01-01'
end_date = '2021-12-31'

nyse_holidays = nyse.holidays(start_date=start_date, end_date=end_date)
nasdaq_holidays = nasdaq.holidays(start_date=start_date, end_date=end_date)
amex_holidays = amex.holidays(start_date=start_date, end_date=end_date)

print("NYSE Holidays:")
display(nyse_holidays.head())

print("\nNASDAQ Holidays:")
display(nasdaq_holidays.head())

print("\nAMEX Holidays:")
display(amex_holidays.head())

"""**Reasoning**:
The previous command failed because 'AMEX' is not a recognized calendar name. The error output shows that 'XNYS' is available, but 'NASDAQ' and 'AMEX' are not. Looking at the list of available calendars in the error message, 'NASDAQ' and 'AMEX' are not present. However, there is a calendar named 'XNYS' (for NYSE), and typically NASDAQ and AMEX follow the NYSE holiday schedule. I will proceed by getting the NYSE calendar and assuming it applies to NASDAQ and AMEX as well for the purpose of identifying potential data sources. I will get the holidays for the relevant date range and display the head.


"""

import pandas_market_calendars as mcal

# Get the NYSE calendar (assuming it covers NASDAQ and AMEX holidays)
nyse = mcal.get_calendar('XNYS')

# Get the holidays for a date range (e.g., from 2005 to 2021, covering the AAL data)
start_date = '2005-01-01'
end_date = '2021-12-31'

exchange_holidays = nyse.holidays(start_date=start_date, end_date=end_date)

print("Exchange Holidays (using NYSE calendar):")
display(exchange_holidays.head())

"""**Reasoning**:
The previous command failed because the `holidays()` method in the installed version of `pandas_market_calendars` does not accept `start_date` and `end_date` as keyword arguments directly. It appears the method expects a date range to be passed differently or the documentation I was referencing is for a different version. To identify the correct way to get holidays for a date range, I need to inspect the available methods and their signatures for the `nyse` calendar object. I will use the `help()` function or check the object's attributes to find the correct method for retrieving holidays within a specified date range.


"""

help(nyse.holidays)

"""**Reasoning**:
The `help(nyse.holidays)` output shows that the `holidays()` method returns a `CustomBusinessDay` object and does not take `start_date` or `end_date` arguments. To get holidays within a date range, I need to use a different method or approach provided by the library. Based on the library's typical usage, the `schedule()` method is used to get the trading schedule, including holidays, for a date range. I will use the `schedule()` method to get the trading days and then identify the holidays from the resulting schedule.


"""

# Get the trading schedule for the date range
schedule = nyse.schedule(start_date=start_date, end_date=end_date)

# Holidays are the days within the date range that are NOT in the schedule
all_days = pd.date_range(start=start_date, end=end_date, freq='D')
exchange_holidays = all_days.difference(schedule.index)

print("Exchange Holidays (using NYSE calendar and schedule):")
display(exchange_holidays.to_list()[:5]) # Display first 5 holidays

"""## Compare with anomalies

### Subtask:
Compare with anomalies

**Reasoning**:
Convert the 'date' column in the anomalies DataFrame to datetime objects, convert the exchange_holidays list to a DatetimeIndex, check for overlapping dates between the anomaly dates and holiday dates, and print the result.
"""

anomalies['date'] = pd.to_datetime(anomalies['date'])
exchange_holidays_dt = pd.DatetimeIndex(exchange_holidays)

anomalies_on_holidays = anomalies[anomalies['date'].isin(exchange_holidays_dt)]

if not anomalies_on_holidays.empty:
    print("Potential anomalies occurred on exchange closure dates:")
    display(anomalies_on_holidays['date'])
else:
    print("No potential anomalies occurred on exchange closure dates.")

"""**Reasoning**:
The previous command failed because the `anomalies` DataFrame was not defined in the current session. This DataFrame was created in a previous code block that was not executed. I need to recreate the `anomalies` DataFrame by filtering the original `df` DataFrame based on the previously calculated anomaly thresholds before attempting to compare the dates with exchange holidays.


"""

# Recalculate anomaly thresholds (assuming df is available from previous steps)
price_change_mean = df['percentage_price_change'].mean()
price_change_std = df['percentage_price_change'].std()
volume_mean = df['volume'].mean()
volume_std = df['volume'].std()
price_anomaly_threshold = price_change_mean + 3 * price_change_std
volume_anomaly_threshold = volume_mean + 3 * volume_std

# Recreate the anomalies DataFrame
price_anomalies = df[abs(df['percentage_price_change']) > price_anomaly_threshold].copy()
volume_anomalies = df[df['volume'] > volume_anomaly_threshold].copy()
anomalies = pd.concat([price_anomalies, volume_anomalies]).drop_duplicates().sort_values(by='date')

# Now proceed with comparing anomaly dates with exchange holidays
anomalies['date'] = pd.to_datetime(anomalies['date'])
exchange_holidays_dt = pd.DatetimeIndex(exchange_holidays)

anomalies_on_holidays = anomalies[anomalies['date'].isin(exchange_holidays_dt)]

if not anomalies_on_holidays.empty:
    print("Potential anomalies occurred on exchange closure dates:")
    display(anomalies_on_holidays['date'])
else:
    print("No potential anomalies occurred on exchange closure dates.")

"""**Reasoning**:
The code failed because the 'percentage_price_change' column does not exist in the `df` DataFrame in the current state. This column was likely created in a previous code block that was not executed before this one. I need to recalculate the 'price_change' and 'percentage_price_change' columns before attempting to calculate their mean and standard deviation and filter for anomalies.


"""

# Recalculate price change and percentage price change
df['price_change'] = df['close'] - df['open']
df['percentage_price_change'] = (df['price_change'] / df['open']) * 100

# Recalculate anomaly thresholds
price_change_mean = df['percentage_price_change'].mean()
price_change_std = df['percentage_price_change'].std()
volume_mean = df['volume'].mean()
volume_std = df['volume'].std()
price_anomaly_threshold = price_change_mean + 3 * price_change_std
volume_anomaly_threshold = volume_mean + 3 * volume_std

# Recreate the anomalies DataFrame
price_anomalies = df[abs(df['percentage_price_change']) > price_anomaly_threshold].copy()
volume_anomalies = df[df['volume'] > volume_anomaly_threshold].copy()
anomalies = pd.concat([price_anomalies, volume_anomalies]).drop_duplicates().sort_values(by='date')

# Now proceed with comparing anomaly dates with exchange holidays
anomalies['date'] = pd.to_datetime(anomalies['date'])
exchange_holidays_dt = pd.DatetimeIndex(exchange_holidays)

anomalies_on_holidays = anomalies[anomalies['date'].isin(exchange_holidays_dt)]

if not anomalies_on_holidays.empty:
    print("Potential anomalies occurred on exchange closure dates:")
    display(anomalies_on_holidays['date'])
else:
    print("No potential anomalies occurred on exchange closure dates.")

"""## Summarize findings

### Subtask:
Summarize the findings from the comparison of stock trading anomalies and exchange closure dates.

## Summary:

### Data Analysis Key Findings

*   The `pandas_market_calendars` library was successfully used to obtain historical exchange closure dates for NYSE ('XNYS').
*   Specific calendars for NASDAQ and AMEX were not directly available under their standard names in the installed version of the library.
*   No potential stock trading anomalies for AAL were found to occur on the identified exchange closure dates between 2005 and 2021.

### Insights or Next Steps

*   Investigate alternative methods or libraries to obtain more comprehensive historical exchange closure data for NASDAQ and AMEX.
*   Extend the analysis to include a wider range of stocks to determine if the lack of anomalies on exchange closure dates is consistent across different equities.
"""



import pandas_market_calendars as mcal
import pandas as pd

# Get the NASDAQ calendar
nasdaq = mcal.get_calendar('NASDAQ')

# Get the holidays for 2024
start_date_2024 = '2006-01-01'
end_date_2024 = '2024-12-31'

# Get the trading schedule for the date range
schedule_2024 = nasdaq.schedule(start_date=start_date_2024, end_date=end_date_2024)

# Holidays are the days within the date range that are NOT in the schedule
all_days_2024 = pd.date_range(start=start_date_2024, end=end_date_2024, freq='D')
nasdaq_holidays_2024 = all_days_2024.difference(schedule_2024.index)

print("NASDAQ Exchange Closure Dates in 2024:")
for holiday in nasdaq_holidays_2024:
    print(holiday.date())

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd

# Ensure 'date' column in df is datetime objects
df['date'] = pd.to_datetime(df['date'])

# Create a figure and axes for the plot
plt.figure(figsize=(14, 7))
ax = plt.gca()

# Plot the AAL closing price
ax.plot(df['date'], df['close'], label='AAL Closing Price', alpha=0.6)

# Highlight NASDAQ closure dates for the period covered by the AAL data
# Get the date range from the AAL data
start_date_aal = df['date'].min()
end_date_aal = df['date'].max()

# Get the NASDAQ calendar
nasdaq = mcal.get_calendar('NASDAQ')

# Get the trading schedule for the AAL data date range
schedule_aal_period = nasdaq.schedule(start_date=start_date_aal, end_date=end_date_aal)

# Holidays are the days within the date range that are NOT in the schedule
all_days_aal_period = pd.date_range(start=start_date_aal, end=end_date_aal, freq='D')
nasdaq_holidays_aal_period = all_days_aal_period.difference(schedule_aal_period.index)

# Convert holidays to a list of datetime objects for easier plotting
holiday_dates_to_plot = [holiday for holiday in nasdaq_holidays_aal_period if holiday in df['date'].values]


# Plot vertical lines for each NASDAQ closure date within the AAL data range
for holiday in holiday_dates_to_plot:
    ax.axvline(holiday, color='red', linestyle='--', alpha=0.5)


# Add a legend entry for the NASDAQ closures (only one entry is needed)
ax.plot([], [], color='red', linestyle='--', label='NASDAQ Closure')


# Set labels and title
ax.set_xlabel('Date')
ax.set_ylabel('Closing Price')
plt.title('AAL Closing Price and NASDAQ Closure Dates (Period of AAL Data)')
ax.legend()
ax.grid(True)

# Format the x-axis to show dates clearly
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
ax.xaxis.set_major_locator(mdates.AutoDateLocator())
plt.gcf().autofmt_xdate() # Auto-rotate date labels

plt.show()